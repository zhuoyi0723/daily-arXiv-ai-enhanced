<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 164]
- [cs.CL](#cs.CL) [Total: 85]
- [cs.AI](#cs.AI) [Total: 46]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Randomized-MLP Regularization Improves Domain Adaptation and Interpretability in DINOv2](https://arxiv.org/abs/2511.05509)
*Joel Valdivia Ortega,Lorenz Lamm,Franziska Eckardt,Benedikt Schworm,Marion Jasnin,Tingying Peng*

Main category: cs.CV

TL;DR: 本文提出了Randomized-MLP (RMLP)正则化方法，通过对比学习提高ViT模型在医学图像上的语义对齐和可解释性，在保持性能的同时增强注意力图的可解释性。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers如DINOv2在多个领域表现优异，但往往重复利用低信息的patch tokens，降低了注意力和特征图的可解释性。在医学成像领域，域漂移问题会同时影响性能和透明度，这对临床应用至关重要。

Method: 提出Randomized-MLP (RMLP)正则化方法，这是一种基于对比学习的技术，鼓励模型学习更多语义对齐的表征。在微调DINOv2时应用RMLP，处理医学图像和自然图像两种模态。

Result: 实验证明RMLP能够改善或维持下游任务的性能，同时产生更具可解释性的注意力图。该方法有效地提升了ViT模型在医学成像中的表现和透明度。

Conclusion: RMLP正则化为增强ViT模型的可解释性提供了有效方案，不仅提升了医学图像分析的实用性，也为理解对比学习在Transformer中的作用提供了理论见解，推动了可解释AI在医疗领域的发展。

Abstract: Vision Transformers (ViTs), such as DINOv2, achieve strong performance across
domains but often repurpose low-informative patch tokens in ways that reduce
the interpretability of attention and feature maps. This challenge is
especially evident in medical imaging, where domain shifts can degrade both
performance and transparency. In this paper, we introduce Randomized-MLP (RMLP)
regularization, a contrastive learning-based method that encourages more
semantically aligned representations. We use RMLPs when fine-tuning DINOv2 to
both medical and natural image modalities, showing that it improves or
maintains downstream performance while producing more interpretable attention
maps. We also provide a mathematical analysis of RMLPs, offering insights into
its role in enhancing ViT-based models and advancing our understanding of
contrastive learning.

</details>


### [2] [Token Is All You Need: Cognitive Planning through Sparse Intent Alignment](https://arxiv.org/abs/2511.05540)
*Shiyao Sang*

Main category: cs.CV

TL;DR: 该论文挑战了端到端自动驾驶需要详尽场景建模的传统观点，提出仅用少量语义丰富的token就能实现有效规划。实验证明该方法在nuPlan基准上表现优异，ADE误差低至0.479米，并发现"时间模糊性"现象，标志着从重建世界到理解世界的范式转变。


<details>
  <summary>Details</summary>
Motivation: 挑战传统自动驾驶系统依赖计算密集型未来场景生成或受马尔可夫假设限制的视觉-语言-动作系统的做法，探索更高效的规划方法。

Method: 使用感知信息化的BEV表示，仅依赖最小化语义丰富的token集进行轨迹规划，无需传统的前景预测或重建损失。

Result: 在nuPlan基准上，无预测时ADE为0.548米，使用未来token预测后降至0.479米，超越现有方法；发现重建损失无益且可能损害性能；观察到"时间模糊性"现象。

Conclusion: 提出了"token is all you need"原则，标志着从重建世界到理解世界的范式转变，为基于想象而非反应的认知启发系统奠定基础。

Abstract: We challenge the long-standing assumption that exhaustive scene modeling is
required for high-performance end-to-end autonomous driving (E2EAD). Unlike
world-model approaches that rely on computationally intensive future scene
generation or vision-language-action (VLA) systems constrained by Markov
assumptions, we show that a minimal set of semantically rich tokens is
sufficient for effective planning. Experiments on the nuPlan benchmark (720
scenarios, over 11,000 samples) using perception-informed BEV representations
yield three key findings: (1) even without future prediction, our sparse
representation achieves 0.548 m ADE, comparable to or surpassing prior methods
reporting around 0.75 m on nuScenes; (2) conditioning trajectory decoding on
predicted future tokens reduces ADE to 0.479 m, a 12.6% improvement over
current-state baselines; and (3) explicit reconstruction loss offers no benefit
and may degrade performance under reliable perception inputs. Notably, we
observe the emergence of temporal fuzziness, where the model adaptively attends
to task-relevant semantics rather than aligning rigidly to fixed timestamps,
providing a cognitive advantage for planning under uncertainty. Our "token is
all you need" principle marks a paradigm shift from reconstructing the world to
understanding it, laying a foundation for cognitively inspired systems that
plan through imagination rather than reaction.

</details>


### [3] [Automated Invoice Data Extraction: Using LLM and OCR](https://arxiv.org/abs/2511.05547)
*Advait Thakur,Khushi Khanchandani,Akshita Shetty,Chaitravi Reddy,Ritisa Behera*

Main category: cs.CV

TL;DR: 本文介绍了一种综合AI平台，结合OCR、深度学习、大语言模型和图分析技术，解决传统OCR系统在处理多样化发票布局时的局限性，实现前所未有的提取质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 传统OCR系统面临变体发票布局、手写文本和低质量扫描的挑战，主要由于强模板依赖性限制了其在不同文档结构和布局中的灵活性。

Method: 提出一种全面的AI平台，整合OCR技术、深度学习模型(CNN和Transformers)、大语言模型(LLMs)以及图分析技术，实现更好的布局分析和跨各种文档类型的准确性。

Result: 通过结合视觉命名实体识别(VISUAL NER)能力和混合架构，实现了比传统方法更高的准确率、更好的上下文敏感性和更复杂的上下文关系映射，支持最大可扩展性并减少人工干预。

Conclusion: 该工作展示了一种创新的AI平台解决方案，通过融合多种先进技术，成功克服了传统OCR系统的局限性，在发票信息提取方面实现了前所未有的质量和一致性水平。

Abstract: Conventional Optical Character Recognition (OCR) systems are challenged by
variant invoice layouts, handwritten text, and low- quality scans, which are
often caused by strong template dependencies that restrict their flexibility
across different document structures and layouts. Newer solutions utilize
advanced deep learning models such as Convolutional Neural Networks (CNN) as
well as Transformers, and domain-specific models for better layout analysis and
accuracy across various sections over varied document types. Large Language
Models (LLMs) have revolutionized extraction pipelines at their core with
sophisticated entity recognition and semantic comprehension to support complex
contextual relationship mapping without direct programming specification.
Visual Named Entity Recognition (NER) capabilities permit extraction from
invoice images with greater contextual sensitivity and much higher accuracy
rates than older approaches. Existing industry best practices utilize hybrid
architectures that blend OCR technology and LLM for maximum scalability and
minimal human intervention. This work introduces a holistic Artificial
Intelligence (AI) platform combining OCR, deep learning, LLMs, and graph
analytics to achieve unprecedented extraction quality and consistency.

</details>


### [4] [In-Context-Learning-Assisted Quality Assessment Vision-Language Models for Metal Additive Manufacturing](https://arxiv.org/abs/2511.05551)
*Qiaojie Zheng,Jiucai Zhang,Xiaoli Zhang*

Main category: cs.CV

TL;DR: 本文提出利用视觉语言模型(VLMs)结合上下文学习(ICL)进行增材制造质量评估，仅需少量样本即可达到与传统机器学习模型相当的精度，同时提供可解释的判断依据。


<details>
  <summary>Details</summary>
Motivation: 传统基于视觉的增材制造质量评估需要专用的机器学习模型和应用特定的数据集，但数据收集和模型训练成本高、耗时长。

Method: 采用VLMs的推理能力，通过ICL提供应用特定知识和演示样本，探索不同的ICL采样策略，在Gemini-2.5-flash和Gemma3:27b模型上进行评估。

Result: ICL辅助的VLMs在质量分类精度上与传统机器学习模型相当，仅需极少样本；提出知识相关性和依据有效性两个指标评估VLMs的可解释性。

Conclusion: ICL辅助的VLMs能够用有限数据解决应用特定任务，在保持较高精度的同时提供有效的支持依据，提高了决策透明度。

Abstract: Vision-based quality assessment in additive manufacturing often requires
dedicated machine learning models and application-specific datasets. However,
data collection and model training can be expensive and time-consuming. In this
paper, we leverage vision-language models' (VLMs') reasoning capabilities to
assess the quality of printed parts and introduce in-context learning (ICL) to
provide VLMs with necessary application-specific knowledge and demonstration
samples. This method eliminates the requirement for large application-specific
datasets for training models. We explored different sampling strategies for ICL
to search for the optimal configuration that makes use of limited samples. We
evaluated these strategies on two VLMs, Gemini-2.5-flash and Gemma3:27b, with
quality assessment tasks in wire-laser direct energy deposition processes. The
results show that ICL-assisted VLMs can reach quality classification accuracies
similar to those of traditional machine learning models while requiring only a
minimal number of samples. In addition, unlike traditional classification
models that lack transparency, VLMs can generate human-interpretable rationales
to enhance trust. Since there are no metrics to evaluate their interpretability
in manufacturing applications, we propose two metrics, knowledge relevance and
rationale validity, to evaluate the quality of VLMs' supporting rationales. Our
results show that ICL-assisted VLMs can address application-specific tasks with
limited data, achieving relatively high accuracy while also providing valid
supporting rationales for improved decision transparency.

</details>


### [5] [EVLP:Learning Unified Embodied Vision-Language Planner with Reinforced Supervised Fine-Tuning](https://arxiv.org/abs/2511.05553)
*Xinyan Cai,Shiguang Wu,Dafeng Chi,Yuzheng Zhuang,Xingyue Quan,Jianye Hao,Qiang Guan*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In complex embodied long-horizon manipulation tasks, effective task
decomposition and execution require synergistic integration of textual logical
reasoning and visual-spatial imagination to ensure efficient and accurate
operation. Current methods fail to adopt a unified generation framework for
multimodal planning, lead to inconsistent in multimodal planning. To address
this challenge, we present \textbf{EVLP (Embodied Vision-Language Planner)}, an
innovative multimodal unified generation framework that jointly models
linguistic reasoning and visual generation. Our approach achieves multimodal
planning for long-horizon tasks through a novel training pipeline incorporating
dynamic pretraining and reinforced alignment. Our core innovations consist of
three key components: \textbf{1) Unified Multimodal Generation Framework}: For
understanding, We integrate semantic information with spatial features to
provide comprehensive visual perception. For generation, we directly learn the
joint distribution of discrete images for one-step visual synthesis, enabling
coordinated language-visual modeling through learnable cross-modal attention
mechanisms. \textbf{2) Dynamic Perception Pretraining}: We propose a
bidirectional dynamic alignment strategy employing inverse dynamics tasks and
forward dynamics tasks, effectively strengthening multimodal correlations
within a unified feature space. \textbf{3) Reinforced Supervised Fine-Tuning}:
While conducting instruction-based fine-tuning in the unified generation space,
we construct a reinforce loss to align the spatial logic between textual
actions and generated images, enabling the model to acquire spatio-awared
multimodal planning capabilities.

</details>


### [6] [Compressing Multi-Task Model for Autonomous Driving via Pruning and Knowledge Distillation](https://arxiv.org/abs/2511.05557)
*Jiayuan Wang,Q. M. Jonathan Wu,Ning Zhang,Katsuya Suto,Lei Zhong*

Main category: cs.CV

TL;DR: 提出结合任务感知安全剪枝和特征级知识蒸馏的多任务模型压缩框架，在BDD100K数据集上实现32.7%参数削减，保持较小性能损失，仍能实时运行。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统依赖全景感知处理物体检测、可行驶区域分割和车道线分割，但多任务学习增加的模型参数和复杂性使其在车载设备上部署困难。

Method: 提出任务感知安全剪枝策略（结合Taylor-based通道重要性和梯度冲突惩罚），以及任务头无关的蒸馏方法，从教师模型传递骨干和编码器特征到学生模型作为指导。

Result: 在BDD100K数据集上，压缩模型参数减少32.7%，分割性能几乎无精度损失，检测性能轻微下降（Recall -1.2%，mAP50 -1.8%），仍能达到32.7 FPS的实时速度。

Conclusion: 剪枝和知识蒸馏的结合为多任务全景感知提供了有效的压缩解决方案，能够在保证实时性的同时维持较高的感知精度。

Abstract: Autonomous driving systems rely on panoptic perception to jointly handle
object detection, drivable area segmentation, and lane line segmentation.
Although multi-task learning is an effective way to integrate these tasks, its
increasing model parameters and complexity make deployment on on-board devices
difficult. To address this challenge, we propose a multi-task model compression
framework that combines task-aware safe pruning with feature-level knowledge
distillation. Our safe pruning strategy integrates Taylor-based channel
importance with gradient conflict penalty to keep important channels while
removing redundant and conflicting channels. To mitigate performance
degradation after pruning, we further design a task head-agnostic distillation
method that transfers intermediate backbone and encoder features from a teacher
to a student model as guidance. Experiments on the BDD100K dataset demonstrate
that our compressed model achieves a 32.7% reduction in parameters while
segmentation performance shows negligible accuracy loss and only a minor
decrease in detection (-1.2% for Recall and -1.8% for mAP50) compared to the
teacher. The compressed model still runs at 32.7 FPS in real-time. These
results show that combining pruning and knowledge distillation provides an
effective compression solution for multi-task panoptic perception.

</details>


### [7] [M2S2L: Mamba-based Multi-Scale Spatial-temporal Learning for Video Anomaly Detection](https://arxiv.org/abs/2511.05564)
*Yang Liu,Boan Chen,Xiaoguang Zhu,Jing Liu,Peng Sun,Wei Zhou*

Main category: cs.CV

TL;DR: 本文提出基于Mamba的多尺度时空学习框架M2S2L，用于高效准确的视频异常检测，在保持实时推理速度的同时取得优异检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统视频异常检测方法在平衡检测准确性和计算效率方面存在根本挑战，现有方法要么缺乏全面的时空建模能力，要么需要过多计算资源，难以满足现代监控系统的实际需求。

Method: 提出M2S2L框架，采用分层空间编码器处理多粒度特征，结合多时间尺度编码器捕捉运动动态，并引入特征分解机制实现外观和运动重建的任务特定优化。

Result: 在三个基准数据集上实现帧级AUC分别为98.5%（UCSD Ped2）、92.1%（CUHK Avenue）和77.9%（ShanghaiTech），计算效率为20.1G FLOPs，推理速度达45 FPS。

Conclusion: M2S2L框架在保持较高检测精度的同时实现了实时推理性能，为实际监控部署提供了有效解决方案，平衡了准确性与效率的权衡问题。

Abstract: Video anomaly detection (VAD) is an essential task in the image processing
community with prospects in video surveillance, which faces fundamental
challenges in balancing detection accuracy with computational efficiency. As
video content becomes increasingly complex with diverse behavioral patterns and
contextual scenarios, traditional VAD approaches struggle to provide robust
assessment for modern surveillance systems. Existing methods either lack
comprehensive spatial-temporal modeling or require excessive computational
resources for real-time applications. In this regard, we present a Mamba-based
multi-scale spatial-temporal learning (M2S2L) framework in this paper. The
proposed method employs hierarchical spatial encoders operating at multiple
granularities and multi-temporal encoders capturing motion dynamics across
different time scales. We also introduce a feature decomposition mechanism to
enable task-specific optimization for appearance and motion reconstruction,
facilitating more nuanced behavioral modeling and quality-aware anomaly
assessment. Experiments on three benchmark datasets demonstrate that M2S2L
framework achieves 98.5%, 92.1%, and 77.9% frame-level AUCs on UCSD Ped2, CUHK
Avenue, and ShanghaiTech respectively, while maintaining efficiency with 20.1G
FLOPs and 45 FPS inference speed, making it suitable for practical surveillance
deployment.

</details>


### [8] [In-Context Adaptation of VLMs for Few-Shot Cell Detection in Optical Microscopy](https://arxiv.org/abs/2511.05565)
*Shreyan Ganguly,Angona Biswas,Jaydeep Rade,Md Hasibul Hasan Hasib,Nabila Masud,Nitish Singla,Abhipsa Dash,Ushashi Bhattacharjee,Aditya Balu,Anwesha Sarkar,Adarsh Krishnamurthy,Soumik Sarkar*

Main category: cs.CV

TL;DR: 本文研究了基础视觉-语言模型在生物医学显微镜图像上的少样本目标检测能力，创建了Micro-OD基准测试，发现通过上下文学习可以在少量标注样本的情况下显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 基础视觉-语言模型在自然图像上表现出色，但在生物医学显微镜领域的应用尚未充分探索。由于显微镜图像通常缺乏大规模标注数据集，需要研究如何在少样本条件下实现有效的目标检测。

Method: 创建了包含252张图像和11种细胞类型的Micro-OD基准测试；系统评估了8个VLM模型在少样本条件下的表现；比较了带和不带隐式测试时推理令牌的模型变体；实现了结合检测头和VLM少样本分类器的混合少样本目标检测管道。

Result: 零样本性能因领域差异而较弱；少样本支持持续改善检测效果，在6个样本后达到边际增益；带推理令牌的模型更适合端到端定位，简单变体更适合预定位裁剪的分类。

Conclusion: 上下文适配是显微镜应用的实用路径，建立的基准测试为推进生物医学成像中的开放词汇检测提供了可重现的测试平台。

Abstract: Foundation vision-language models (VLMs) excel on natural images, but their
utility for biomedical microscopy remains underexplored. In this paper, we
investigate how in-context learning enables state-of-the-art VLMs to perform
few-shot object detection when large annotated datasets are unavailable, as is
often the case with microscopic images. We introduce the Micro-OD benchmark, a
curated collection of 252 images specifically curated for in-context learning,
with bounding-box annotations spanning 11 cell types across four sources,
including two in-lab expert-annotated sets. We systematically evaluate eight
VLMs under few-shot conditions and compare variants with and without implicit
test-time reasoning tokens. We further implement a hybrid Few-Shot Object
Detection (FSOD) pipeline that combines a detection head with a VLM-based
few-shot classifier, which enhances the few-shot performance of recent VLMs on
our benchmark. Across datasets, we observe that zero-shot performance is weak
due to the domain gap; however, few-shot support consistently improves
detection, with marginal gains achieved after six shots. We observe that models
with reasoning tokens are more effective for end-to-end localization, whereas
simpler variants are more suitable for classifying pre-localized crops. Our
results highlight in-context adaptation as a practical path for microscopy, and
our benchmark provides a reproducible testbed for advancing open-vocabulary
detection in biomedical imaging.

</details>


### [9] [C3-Diff: Super-resolving Spatial Transcriptomics via Cross-modal Cross-content Contrastive Diffusion Modelling](https://arxiv.org/abs/2511.05571)
*Xiaofei Wang,Stephen Price,Chao Li*

Main category: cs.CV

TL;DR: 本文提出了C3-Diff框架，一种跨模态跨内容对比扩散方法，通过组织学图像指导来增强空间转录组学数据，解决当前ST平台低分辨率问题。


<details>
  <summary>Details</summary>
Motivation: 当前空间转录组学(ST)平台存在低分辨率问题，限制了空间基因表达的深入理解。尽管超分辨率方法可以通过整合组织学图像增强ST图谱，但有效建模组织学图像和基因表达之间的相互作用仍是一个挑战。

Method: 提出了C3-Diff框架：1)分析并改进传统对比学习范式，提取ST图谱和组织学图像的模态不变和内容不变特征；2)在特征单元超球面上进行基于噪声的信息增强以克服ST图谱的低测序灵敏度问题；3)提出动态跨模态插补训练策略缓解ST数据稀缺问题。

Result: 在四个公共数据集上的基准测试显示，C3-Diff相比竞争方法取得显著改进。在细胞类型定位、基因表达相关性和单细胞级基因表达预测等下游任务中也表现出色。

Conclusion: C3-Diff为AI增强的生物技术提供了有效解决方案，促进了生物医学研究和临床应用的发展，代码已开源供社区使用。

Abstract: The rapid advancement of spatial transcriptomics (ST), i.e., spatial gene
expressions, has made it possible to measure gene expression within original
tissue, enabling us to discover molecular mechanisms. However, current ST
platforms frequently suffer from low resolution, limiting the in-depth
understanding of spatial gene expression. Super-resolution approaches promise
to enhance ST maps by integrating histology images with gene expressions of
profiled tissue spots. However, it remains a challenge to model the
interactions between histology images and gene expressions for effective ST
enhancement. This study presents a cross-modal cross-content contrastive
diffusion framework, called C3-Diff, for ST enhancement with histology images
as guidance. In C3-Diff, we firstly analyze the deficiency of traditional
contrastive learning paradigm, which is then refined to extract both
modal-invariant and content-invariant features of ST maps and histology images.
Further, to overcome the problem of low sequencing sensitivity in ST maps, we
perform nosing-based information augmentation on the surface of feature unit
hypersphere. Finally, we propose a dynamic cross-modal imputation-based
training strategy to mitigate ST data scarcity. We tested C3-Diff by
benchmarking its performance on four public datasets, where it achieves
significant improvements over competing methods. Moreover, we evaluate C3-Diff
on downstream tasks of cell type localization, gene expression correlation and
single-cell-level gene expression prediction, promoting AI-enhanced
biotechnology for biomedical research and clinical applications. Codes are
available at https://github.com/XiaofeiWang2018/C3-Diff.

</details>


### [10] [Video Text Preservation with Synthetic Text-Rich Videos](https://arxiv.org/abs/2511.05573)
*Ziyang Liu,Kevin Valencia,Justin Cui*

Main category: cs.CV

TL;DR: 该研究提出了一种轻量级方法，通过合成监督改善文生视频模型中的文本渲染能力，无需修改模型架构。


<details>
  <summary>Details</summary>
Motivation: 现有文生视频模型在生成视频中嵌入清晰连贯文本方面存在困难，传统解决方案计算成本高且不适合视频生成任务。

Method: 首先使用文生图模型生成富含文本的图像，然后用文本无关的图生视频模型将其动画化为短视频，这些合成的视频-提示对用于微调预训练的Wan2.1模型。

Result: 方法显著提升了短文本的可读性和时间一致性，并为长文本生成展现了结构先验能力。

Conclusion: 精心设计的合成数据和弱监督为提升文生视频生成中的文本保真度提供了一条实用可行的技术路径。

Abstract: While Text-To-Video (T2V) models have advanced rapidly, they continue to
struggle with generating legible and coherent text within videos. In
particular, existing models often fail to render correctly even short phrases
or words and previous attempts to address this problem are computationally
expensive and not suitable for video generation. In this work, we investigate a
lightweight approach to improve T2V diffusion models using synthetic
supervision. We first generate text-rich images using a text-to-image (T2I)
diffusion model, then animate them into short videos using a text-agnostic
image-to-video (I2v) model. These synthetic video-prompt pairs are used to
fine-tune Wan2.1, a pre-trained T2V model, without any architectural changes.
Our results show improvement in short-text legibility and temporal consistency
with emerging structural priors for longer text. These findings suggest that
curated synthetic data and weak supervision offer a practical path toward
improving textual fidelity in T2V generation.

</details>


### [11] [DiffSwap++: 3D Latent-Controlled Diffusion for Identity-Preserving Face Swapping](https://arxiv.org/abs/2511.05575)
*Weston Bondurant,Arkaprava Sinha,Hieu Le,Srijan Das,Stephanie Schuckers*

Main category: cs.CV

TL;DR: DiffSwap++是一种基于扩散模型的人脸交换方法，通过融合3D面部潜在特征来提高身份保持和几何一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型人脸交换方法虽然优于传统GAN方法，但在复杂姿态和表情下仍存在细粒度伪影和身份保持不佳的问题，关键原因是没有有效利用3D面部结构来分离身份与姿态表情。

Method: 提出DiffSwap++扩散模型人脸交换流程，在训练期间融入3D面部潜在特征，通过3D感知表示指导生成过程，设计条件扩散架构，同时基于身份嵌入和面部标志点进行去噪处理。

Result: 在CelebA、FFHQ和CelebV-Text数据集上的广泛实验表明，DiffSwap++在保持源身份的同时维持目标姿态和表情方面超越先前方法，通过生物特征评估和用户研究验证了方法的真实性和有效性。

Conclusion: 通过有效融合3D面部结构和多条件约束，DiffSwap++解决了现有人脸交换方法的局限性，实现了更逼真和身份保持的人脸交换效果。

Abstract: Diffusion-based approaches have recently achieved strong results in face
swapping, offering improved visual quality over traditional GAN-based methods.
However, even state-of-the-art models often suffer from fine-grained artifacts
and poor identity preservation, particularly under challenging poses and
expressions. A key limitation of existing approaches is their failure to
meaningfully leverage 3D facial structure, which is crucial for disentangling
identity from pose and expression. In this work, we propose DiffSwap++, a novel
diffusion-based face-swapping pipeline that incorporates 3D facial latent
features during training. By guiding the generation process with 3D-aware
representations, our method enhances geometric consistency and improves the
disentanglement of facial identity from appearance attributes. We further
design a diffusion architecture that conditions the denoising process on both
identity embeddings and facial landmarks, enabling high-fidelity and
identity-preserving face swaps. Extensive experiments on CelebA, FFHQ, and
CelebV-Text demonstrate that DiffSwap++ outperforms prior methods in preserving
source identity while maintaining target pose and expression. Additionally, we
introduce a biometric-style evaluation and conduct a user study to further
validate the realism and effectiveness of our approach. Code will be made
publicly available at https://github.com/WestonBond/DiffSwapPP

</details>


### [12] [In-process 3D Deviation Mapping and Defect Monitoring (3D-DM2) in High Production-rate Robotic Additive Manufacturing](https://arxiv.org/abs/2511.05604)
*Subash Gautam,Alejandro Vargas-Uscategui,Peter King,Hans Lohr,Alireza Bab-Hadiashar,Ivan Cole,Ehsan Asadi*

Main category: cs.CV

TL;DR: 本研究提出了一种实时监控系统，用于检测高沉积率机器人增材制造中的形状偏差，通过实时采集和重建生长中的部件并与参考模型比较，实现早期偏差识别和区域跟踪，从而确保零件质量。


<details>
  <summary>Details</summary>
Motivation: 增材制造技术虽能快速生产复杂零件，但高沉积率工艺如冷喷涂增材制造在当前开环系统中面临形状精度保持的挑战，工艺不稳定性导致偏差传播，影响零件质量和增加后处理需求。

Method: 开发实时监控系统，实时采集并重建生长中的零件，直接与近净成形参考模型对比检测制造过程中的形状偏差，并对每个偏差区域进行分割和跟踪。

Result: 系统能够早期识别形状不一致性，实现对偏差区域的分段和跟踪，为及时干预和补偿提供了技术基础。

Conclusion: 该研究为实现一致性的零件质量奠定了基础，通过实时偏差检测和区域跟踪，为及时干预和补偿创造了条件，有望减少后处理需求并提高制造效率。

Abstract: Additive manufacturing (AM) is an emerging digital manufacturing technology
to produce complex and freeform objects through a layer-wise deposition. High
deposition rate robotic AM (HDRRAM) processes, such as cold spray additive
manufacturing (CSAM), offer significantly increased build speeds by delivering
large volumes of material per unit time. However, maintaining shape accuracy
remains a critical challenge, particularly due to process instabilities in
current open-loop systems. Detecting these deviations as they occur is
essential to prevent error propagation, ensure part quality, and minimize
post-processing requirements. This study presents a real-time monitoring system
to acquire and reconstruct the growing part and directly compares it with a
near-net reference model to detect the shape deviation during the manufacturing
process. The early identification of shape inconsistencies, followed by
segmenting and tracking each deviation region, paves the way for timely
intervention and compensation to achieve consistent part quality.

</details>


### [13] [Walking the Schrödinger Bridge: A Direct Trajectory for Text-to-3D Generation](https://arxiv.org/abs/2511.05609)
*Ziying Li,Xuequan Lu,Xinkui Zhao,Guanjie Cheng,Shuiguang Deng,Jianwei Yin*

Main category: cs.CV

TL;DR: 本文提出TraCe框架，将文本到3D生成重新表述为薛定谔桥问题，通过构建从当前渲染到目标分布的优化传输轨迹，有效解决了SDS方法中的过饱和和过平滑问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于优化的文本到3D生成方法严重依赖从预训练文本到图像扩散模型蒸馏知识，但SDS技术经常在生成的3D资产中引入过饱和和过平滑等伪影，影响生成质量。

Method: 理论证明SDS是薛定谔桥框架的简化实例，基于此提出TraCe框架，显式构建从当前渲染到文本条件去噪目标的扩散桥，训练LoRA自适应模型来学习轨迹的分数动力学，实现鲁棒的3D优化。

Result: 全面实验证明，TraCe在生成质量和保真度方面一致优于最先进技术，能够在更小的CFG值下实现高质量生成。

Conclusion: 通过将文本到3D生成重新表述为薛定谔桥优化问题，TraCe为解决现有方法的伪影问题提供了理论基础和实践框架，显著提升了3D资产生成质量。

Abstract: Recent advancements in optimization-based text-to-3D generation heavily rely
on distilling knowledge from pre-trained text-to-image diffusion models using
techniques like Score Distillation Sampling (SDS), which often introduce
artifacts such as over-saturation and over-smoothing into the generated 3D
assets. In this paper, we address this essential problem by formulating the
generation process as learning an optimal, direct transport trajectory between
the distribution of the current rendering and the desired target distribution,
thereby enabling high-quality generation with smaller Classifier-free Guidance
(CFG) values. At first, we theoretically establish SDS as a simplified instance
of the Schr\"odinger Bridge framework. We prove that SDS employs the reverse
process of an Schr\"odinger Bridge, which, under specific conditions (e.g., a
Gaussian noise as one end), collapses to SDS's score function of the
pre-trained diffusion model. Based upon this, we introduce Trajectory-Centric
Distillation (TraCe), a novel text-to-3D generation framework, which
reformulates the mathematically trackable framework of Schr\"odinger Bridge to
explicitly construct a diffusion bridge from the current rendering to its
text-conditioned, denoised target, and trains a LoRA-adapted model on this
trajectory's score dynamics for robust 3D optimization. Comprehensive
experiments demonstrate that TraCe consistently achieves superior quality and
fidelity to state-of-the-art techniques.

</details>


### [14] [Pose-Aware Multi-Level Motion Parsing for Action Quality Assessment](https://arxiv.org/abs/2511.05611)
*Shuaikang Zhu,Yang Yang,Chen Sun*

Main category: cs.CV

TL;DR: 提出了一种基于增强时空姿态特征的多级运动解析框架，用于动作质量评估，在跳水数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 人体姿态是动作质量评估的基础，其中细微的时空变化往往区分优秀与平庸的表现，在高级别比赛中这些细微差异成为评分的决定性因素。

Method: 提出多级运动解析框架：第一级设计动作单元解析器实现精确动作分割和全局姿态表示；第二级使用运动解析器通过时空特征学习捕获每个动作单元的姿态变化；额外设计条件解析器处理非身体相关因素（如跳水水花）；引入权重调整评分模块适应不同动作类型需求。

Result: 在大型跳水运动数据集上的广泛评估表明，该多级运动解析框架在动作分割和动作评分任务中都达到了最先进的性能。

Conclusion: 该框架通过多层次的运动解析和灵活的条件处理，有效提升了动作质量评估的准确性和适应性，为体育评分等应用提供了新的技术方案。

Abstract: Human pose serves as a cornerstone of action quality assessment (AQA), where
subtle spatial-temporal variations in pose often distinguish excellence from
mediocrity. In high-level competitions, these nuanced differences become
decisive factors in scoring. In this paper, we propose a novel multi-level
motion parsing framework for AQA based on enhanced spatial-temporal pose
features. On the first level, the Action-Unit Parser is designed with the help
of pose extraction to achieve precise action segmentation and comprehensive
local-global pose representations. On the second level, Motion Parser is used
by spatial-temporal feature learning to capture pose changes and appearance
details for each action-unit. Meanwhile, some special conditions other than
body-related will impact action scoring, like water splash in diving. In this
work, we design an additional Condition Parser to offer users more flexibility
in their choices. Finally, Weight-Adjust Scoring Module is introduced to better
accommodate the diverse requirements of various action types and the
multi-scale nature of action-units. Extensive evaluations on large-scale diving
sports datasets demonstrate that our multi-level motion parsing framework
achieves state-of-the-art performance in both action segmentation and action
scoring tasks.

</details>


### [15] [Grounding Foundational Vision Models with 3D Human Poses for Robust Action Recognition](https://arxiv.org/abs/2511.05622)
*Nicholas Babey,Tiffany Gu,Yiheng Li,Cristian Meo,Kevin Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种结合V-JEPA世界动态预测和CoMotion人体姿态数据的物理空间基础动作识别模型，在复杂遮挡场景中显著超越基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于RGB视频的动作识别模型只能学习表面的模式相关性，无法捕捉底层的物理交互动态和复杂场景中的人体姿态，需要更深入的空间理解能力。

Method: 通过融合两种互补的表示：V-JEPA的上下文感知预测世界动态和CoMotion的显式、遮挡容忍人体姿态数据，构建物理空间基础的动作识别架构。

Result: 在InHARD和UCF-19-Y-OCC基准测试上验证，模型在通用动作识别和高遮挡动作识别任务上均超越三个基线方法，尤其在复杂遮挡场景中表现突出。

Conclusion: 研究结果表明，动作识别应该基于空间理解而非统计模式识别，强调了物理空间基础对于有效动作理解的重要性。

Abstract: For embodied agents to effectively understand and interact within the world
around them, they require a nuanced comprehension of human actions grounded in
physical space. Current action recognition models, often relying on RGB video,
learn superficial correlations between patterns and action labels, so they
struggle to capture underlying physical interaction dynamics and human poses in
complex scenes. We propose a model architecture that grounds action recognition
in physical space by fusing two powerful, complementary representations: V-JEPA
2's contextual, predictive world dynamics and CoMotion's explicit,
occlusion-tolerant human pose data. Our model is validated on both the InHARD
and UCF-19-Y-OCC benchmarks for general action recognition and high-occlusion
action recognition, respectively. Our model outperforms three other baselines,
especially within complex, occlusive scenes. Our findings emphasize a need for
action recognition to be supported by spatial understanding instead of
statistical pattern recognition.

</details>


### [16] [Registration-Free Monitoring of Unstructured Point Cloud Data via Intrinsic Geometrical Properties](https://arxiv.org/abs/2511.05623)
*Mariafrancesca Patalano,Giovanna Capizzi,Kamran Paynabar*

Main category: cs.CV

TL;DR: 本文提出了一种无需配准和网格重建的点云数据监测新方法，利用拉普拉斯和测地距离提取形状的内在几何特征，通过阈值技术选择最相关特征进行监测。


<details>
  <summary>Details</summary>
Motivation: 传统点云数据监测前需要配准和网格重建等预处理步骤，但这些步骤容易出错、耗时且可能引入人工痕迹，影响监测结果的准确性和一致性。

Method: 提出两种替代的特征学习方法，分别基于拉普拉斯和测地距离捕获形状的内在几何性质，然后采用阈值技术筛选最能反映潜在失控状态的内在特征进行监测。

Result: 数值实验和案例研究表明，该方法能有效识别不同类型的缺陷，避免了传统预处理的弊端。

Conclusion: 无需配准和网格重建的新方法简化了点云监测流程，提高了监测效率和准确性，在复杂形状监测中表现出良好的缺陷识别能力。

Abstract: Modern sensing technologies have enabled the collection of unstructured point
cloud data (PCD) of varying sizes, which are used to monitor the geometric
accuracy of 3D objects. PCD are widely applied in advanced manufacturing
processes, including additive, subtractive, and hybrid manufacturing. To ensure
the consistency of analysis and avoid false alarms, preprocessing steps such as
registration and mesh reconstruction are commonly applied prior to monitoring.
However, these steps are error-prone, time-consuming and may introduce
artifacts, potentially affecting monitoring outcomes. In this paper, we present
a novel registration-free approach for monitoring PCD of complex shapes,
eliminating the need for both registration and mesh reconstruction. Our
proposal consists of two alternative feature learning methods and a common
monitoring scheme. Feature learning methods leverage intrinsic geometric
properties of the shape, captured via the Laplacian and geodesic distances. In
the monitoring scheme, thresholding techniques are used to further select
intrinsic features most indicative of potential out-of-control conditions.
Numerical experiments and case studies highlight the effectiveness of the
proposed approach in identifying different types of defects.

</details>


### [17] [Culture in Action: Evaluating Text-to-Image Models through Social Activities](https://arxiv.org/abs/2511.05681)
*Sina Malakouti,Boqing Gong,Adriana Kovashka*

Main category: cs.CV

TL;DR: 本文提出了CULTIVate基准测试，用于评估文本到图像模型在跨文化活动生成中的文化偏差，涵盖16个国家576个提示，提出四个评估指标，发现模型对全球北方国家表现优于全球南方国家。


<details>
  <summary>Details</summary>
Motivation: 现有T2I扩散模型虽然能生成逼真图像，但继承文化偏见，无法忠实描绘代表性不足地区。现有文化基准主要关注以对象为中心的类别，忽视了更能反映文化规范的社会和日常活动，且缺乏衡量文化忠实度的有效指标。

Method: 提出CULTIVate基准测试，涵盖16个国家的跨文化活动（问候、用餐、游戏、传统舞蹈、文化庆典），包含576个提示和19,000+图像。提供可解释的基于描述符的评估框架，涵盖背景、服装、对象、交互等文化维度。提出四个评估指标：文化一致性、幻觉、夸张元素和多样性。

Result: 发现系统性文化偏差：模型对全球北方国家表现优于全球南方国家，不同T2I系统存在明显失败模式。人类研究验证了所提指标比现有文本-图像指标与人类判断相关性更强。

Conclusion: CULTIVate基准和相关评估方法为量化和理解T2I模型在文化活动生成中的文化偏差提供了有效工具，有助于推动更公平、文化敏感的图像生成模型发展。

Abstract: Text-to-image (T2I) diffusion models achieve impressive photorealism by
training on large-scale web data, but models inherit cultural biases and fail
to depict underrepresented regions faithfully. Existing cultural benchmarks
focus mainly on object-centric categories (e.g., food, attire, and
architecture), overlooking the social and daily activities that more clearly
reflect cultural norms. Few metrics exist for measuring cultural faithfulness.
We introduce CULTIVate, a benchmark for evaluating T2I models on cross-cultural
activities (e.g., greetings, dining, games, traditional dances, and cultural
celebrations). CULTIVate spans 16 countries with 576 prompts and more than
19,000 images, and provides an explainable descriptor-based evaluation
framework across multiple cultural dimensions, including background, attire,
objects, and interactions. We propose four metrics to measure cultural
alignment, hallucination, exaggerated elements, and diversity. Our findings
reveal systematic disparities: models perform better for global north countries
than for the global south, with distinct failure modes across T2I systems.
Human studies confirm that our metrics correlate more strongly with human
judgments than existing text-image metrics.

</details>


### [18] [VMDT: Decoding the Trustworthiness of Video Foundation Models](https://arxiv.org/abs/2511.05682)
*Yujin Potter,Zhun Wang,Nicholas Crispino,Kyle Montgomery,Alexander Xiong,Ethan Y. Chang,Francesco Pinto,Yuqi Chen,Rahul Gupta,Morteza Ziyadi,Christos Christodoulopoulos,Bo Li,Chenguang Wang,Dawn Song*

Main category: cs.CV

TL;DR: 本文提出VMDT，首个统一的视频模型可信度评估平台，涵盖安全性、幻觉、公平性、隐私和对抗鲁棒性五个维度，对7个T2V和19个V2T模型进行评估。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型日趋复杂，确保其可信性变得至关重要；然而，与文本和图像模态不同，视频模态仍缺乏全面的可信度基准评估体系。

Method: 构建VMDT平台，用于评估文本到视频（T2V）和视频到文本（V2T）模型，从安全性、幻觉、公平性、隐私、对抗鲁棒性五个维度进行系统性评估。

Result: 所有开源T2V模型均无法识别有害查询并经常生成有害视频，公平性水平比图像模型更高；V2T模型中不公平性和隐私风险随规模增长，幻觉和对抗鲁棒性虽有改善但整体性能仍低；安全性与模型规模无相关性。

Conclusion: 研究结果凸显了开发更稳健和可信视频基础模型的紧迫性，VMDT为衡量和追踪这一目标的进展提供了系统性框架。

Abstract: As foundation models become more sophisticated, ensuring their
trustworthiness becomes increasingly critical; yet, unlike text and image, the
video modality still lacks comprehensive trustworthiness benchmarks. We
introduce VMDT (Video-Modal DecodingTrust), the first unified platform for
evaluating text-to-video (T2V) and video-to-text (V2T) models across five key
trustworthiness dimensions: safety, hallucination, fairness, privacy, and
adversarial robustness. Through our extensive evaluation of 7 T2V models and 19
V2T models using VMDT, we uncover several significant insights. For instance,
all open-source T2V models evaluated fail to recognize harmful queries and
often generate harmful videos, while exhibiting higher levels of unfairness
compared to image modality models. In V2T models, unfairness and privacy risks
rise with scale, whereas hallucination and adversarial robustness improve --
though overall performance remains low. Uniquely, safety shows no correlation
with model size, implying that factors other than scale govern current safety
levels. Our findings highlight the urgent need for developing more robust and
trustworthy video foundation models, and VMDT provides a systematic framework
for measuring and tracking progress toward this goal. The code is available at
https://sunblaze-ucb.github.io/VMDT-page/.

</details>


### [19] [Pedicle Screw Pairing and Registration for Screw Pose Estimation from Dual C-arm Images Using CAD Models](https://arxiv.org/abs/2511.05702)
*Yehyun Suh,Lin Li,Aric Plumley,Chaochao Zhou,Daniel Moyer,Kongbin Kang*

Main category: cs.CV

TL;DR: 本文提出了一种从双C臂图像中进行椎弓根螺钉对应匹配和姿态估计的方法，通过2D-3D对齐与螺钉CAD模型，实现了准确的螺钉配对和定位，为脊柱手术提供可靠的螺钉位置反馈。


<details>
  <summary>Details</summary>
Motivation: 在脊柱手术中，准确匹配前后位(AP)和侧位(LAT)图像中的椎弓根螺钉对于成功的脊柱减压和稳定至关重要，但建立螺钉对应关系，特别是在侧位视图中，仍然是一个重大的临床挑战。

Method: 该方法通过比较螺钉组合，利用2D-3D对齐技术与螺钉CAD 3D模型，从双视图中准确配对和估计螺钉姿态，包括配对和配准两个主要任务。

Result: 实验结果表明，正确的螺钉组合在所有测试案例中都持续优于错误的配对，即使在配准之前也是如此。配准后，正确组合进一步增强了投影与图像之间的对齐，显著降低了投影误差。

Conclusion: 该方法通过为螺钉定位提供可靠的反馈，显示出在改善脊柱手术结果方面的潜力，有望提高手术精度和患者预后。

Abstract: Accurate matching of pedicle screws in both anteroposterior (AP) and lateral
(LAT) images is critical for successful spinal decompression and stabilization
during surgery. However, establishing screw correspondence, especially in LAT
views, remains a significant clinical challenge. This paper introduces a method
to address pedicle screw correspondence and pose estimation from dual C-arm
images. By comparing screw combinations, the approach demonstrates consistent
accuracy in both pairing and registration tasks. The method also employs 2D-3D
alignment with screw CAD 3D models to accurately pair and estimate screw pose
from dual views. Our results show that the correct screw combination
consistently outperforms incorrect pairings across all test cases, even prior
to registration. After registration, the correct combination further enhances
alignment between projections and images, significantly reducing projection
error. This approach shows promise for improving surgical outcomes in spinal
procedures by providing reliable feedback on screw positioning.

</details>


### [20] [Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale](https://arxiv.org/abs/2511.05705)
*David Acuna,Chao-Han Huck Yang,Yuntian Deng,Jaehun Jung,Ximing Lu,Prithviraj Ammanabrolu,Hyunwoo Kim,Yuan-Hong Liao,Yejin Choi*

Main category: cs.CV

TL;DR: 本文提出了一个大规模视觉推理数据集生成框架，包含超过100万个高质量的合成视觉推理问题，通过微调模型在多个基准测试中取得领先性能，并展现出跨模态迁移能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推理的进展主要依赖未公开的数据集和专有数据合成方法，如何系统性地构建大规模、以视觉为中心的推理数据集仍是一个开放问题，特别是对于超越视觉数学的复杂任务。

Method: 提出了两阶段合成框架：(1)规模化和(2)复杂化。利用VLM和推理LLM通过两阶段过程合成推理轨迹，产生包含丰富认知行为的思维链(CoT)轨迹。数据集包含偏好数据和指令提示，支持离线和在线强化学习。

Result: 在Qwen2.5-VL-7B上微调后在V* Bench、CV-Bench和MMStar-V等视觉基准测试中超越所有开放数据基线和部分封闭数据模型。尽管完全是视觉中心数据，却能正向迁移到纯文本推理(MMLU-Pro)和音频推理(MMAU)任务，在具身QA基准(NiEH)上也取得显著提升。

Conclusion: 实证分析表明：(i)高质量非线性推理轨迹的SFT对有效在线RL至关重要；(ii)分阶段离线RL性能匹配在线RL且计算需求更低；(iii)高质量数据的SFT能显著改善跨域、跨模态迁移效果，为VLM后训练pipeline提供了重要指导。

Abstract: Recent progress in multimodal reasoning has been driven largely by
undisclosed datasets and proprietary data synthesis recipes, leaving open
questions about how to systematically build large-scale, vision-centric
reasoning datasets, particularly for tasks that go beyond visual math. In this
work, we introduce a new reasoning data generation framework spanning diverse
skills and levels of complexity with over 1M high-quality synthetic
vision-centric questions. The dataset also includes preference data and
instruction prompts supporting both offline and online RL. Our synthesis
framework proceeds in two stages: (1) scale; and (2) complexity. Reasoning
traces are then synthesized through a two-stage process that leverages VLMs and
reasoning LLMs, producing CoT traces for VLMs that capture the richness and
diverse cognitive behaviors found in frontier reasoning models. Remarkably, we
show that finetuning Qwen2.5-VL-7B on our data outperforms all open-data
baselines across all evaluated vision-centric benchmarks, and even surpasses
strong closed-data models such as MiMo-VL-7B-RL on V* Bench, CV-Bench and
MMStar-V. Perhaps most surprising, despite being entirely vision-centric, our
data transfers positively to text-only reasoning (MMLU-Pro) and audio reasoning
(MMAU), demonstrating its effectiveness. Similarly, despite not containing
videos or embodied visual data, we observe notable gains when evaluating on a
single-evidence embodied QA benchmark (NiEH). Finally, we use our data to
analyze the entire VLM post-training pipeline. Our empirical analysis
highlights that (i) SFT on high-quality data with non-linear reasoning traces
is essential for effective online RL, (ii) staged offline RL matches online
RL's performance while reducing compute demands, and (iii) careful SFT on high
quality data can substantially improve out-of-domain, cross-modality transfer.

</details>


### [21] [Towards Better Ultrasound Video Segmentation Foundation Model: An Empirical study on SAM2 Finetuning from Data Perspective](https://arxiv.org/abs/2511.05731)
*Xing Yao,Ahana Gangopadhyay,Hsi-Ming Chang,Ravi Soni*

Main category: cs.CV

TL;DR: 本研究系统探讨了SAM2模型在超声视频分割任务中的数据适配策略，发现数据规模和时序上下文比模型架构更重要，联合训练提供了模态对齐与任务专业化的有效平衡。


<details>
  <summary>Details</summary>
Motivation: 超声视频分割面临强数据变异性、运动伪影和标注数据不足等挑战。SAM2等基础模型在医学领域性能显著下降，而现有研究主要关注架构修改，缺乏对数据特性和训练策略影响的系统性分析。

Method: 采用数据为中心的方法，在三个超声数据集上系统分析了训练集大小、视频持续时间、增强策略对SAM2适配的影响。比较了任务特定微调、中间适配、多任务联合训练三种范式，测试了五个SAM2变体和多种提示模式，并设计了六种超声特定增强策略。

Result: 实验表明数据规模和时序上下文对性能影响比模型架构或初始化更关键。联合训练在模态对齐和任务专业化间提供了高效平衡点，超声特定增强策略优于通用策略。

Conclusion: 该研究为开发SAM2在超声视频分析中的高效、数据感知适配流程提供了实证指导，强调了数据特性在医学基础模型适配中的决定性作用。

Abstract: Ultrasound (US) video segmentation remains a challenging problem due to
strong inter- and intra-dataset variability, motion artifacts, and limited
annotated data. Although foundation models such as Segment Anything Model 2
(SAM2) demonstrate strong zero-shot and prompt-guided segmentation
capabilities, their performance deteriorates substantially when transferred to
medical imaging domains. Current adaptation studies mainly emphasize
architectural modifications, while the influence of data characteristics and
training regimes has not been systematically examined. In this study, we
present a comprehensive, data-centric investigation of SAM2 adaptation for
ultrasound video segmentation. We analyze how training-set size, video
duration, and augmentation schemes affect adaptation performance under three
paradigms: task-specific fine-tuning, intermediate adaptation, and multi-task
joint training, across five SAM2 variants and multiple prompting modes. We
further design six ultrasound-specific augmentations, assessing their effect
relative to generic strategies. Experiments on three representative ultrasound
datasets reveal that data scale and temporal context play a more decisive role
than model architecture or initialization. Moreover, joint training offers an
efficient compromise between modality alignment and task specialization. This
work aims to provide empirical insights for developing efficient, data-aware
adaptation pipelines for SAM2 in ultrasound video analysis.

</details>


### [22] [TCSA-UDA: Text-Driven Cross-Semantic Alignment for Unsupervised Domain Adaptation in Medical Image Segmentation](https://arxiv.org/abs/2511.05782)
*Lalit Maurya,Honghai Liu,Reyer Zwiggelaar*

Main category: cs.CV

TL;DR: 提出TCSA-UDA框架，一种基于文本驱动的跨语义对齐方法，通过利用领域不变的文本类描述引导视觉表征学习，在跨模态医学图像分割的无监督域适应任务中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割的无监督域适应面临重大挑战，主要原因在于不同成像模态（如CT和MRI）之间存在显著的域偏移。虽然最近的视觉-语言表征学习方法展现出潜力，但它们在UDA分割任务中的应用尚未得到充分探索。

Method: 提出TCSA-UDA框架，包含两个核心组件：1）视觉-语言协方差余弦损失，直接对齐图像编码器特征与类间文本语义关系，促进语义有意义且模态不变的特征表征；2）原型对齐模块，使用高层语义原型对齐跨域的类级像素级特征分布，减轻残差类别级差异并增强跨模态一致性。

Result: 在具有挑战性的跨模态心脏、腹部和脑肿瘤分割基准测试上进行的广泛实验表明，TCSA-UDA框架显著减少了域偏移，并始终优于最先进的UDA方法。

Conclusion: 该研究建立了一种将语言驱动的语义整合到域自适应医学图像分析中的新范式，为解决跨模态医学图像分割中的域适应问题提供了有效解决方案。

Abstract: Unsupervised domain adaptation for medical image segmentation remains a
significant challenge due to substantial domain shifts across imaging
modalities, such as CT and MRI. While recent vision-language representation
learning methods have shown promise, their potential in UDA segmentation tasks
remains underexplored. To address this gap, we propose TCSA-UDA, a Text-driven
Cross-Semantic Alignment framework that leverages domain-invariant textual
class descriptions to guide visual representation learning. Our approach
introduces a vision-language covariance cosine loss to directly align image
encoder features with inter-class textual semantic relations, encouraging
semantically meaningful and modality-invariant feature representations.
Additionally, we incorporate a prototype alignment module that aligns
class-wise pixel-level feature distributions across domains using high-level
semantic prototypes. This mitigates residual category-level discrepancies and
enhances cross-modal consistency. Extensive experiments on challenging
cross-modality cardiac, abdominal, and brain tumor segmentation benchmarks
demonstrate that our TCSA-UDA framework significantly reduces domain shift and
consistently outperforms state-of-the-art UDA methods, establishing a new
paradigm for integrating language-driven semantics into domain-adaptive medical
image analysis.

</details>


### [23] [MACMD: Multi-dilated Contextual Attention and Channel Mixer Decoding for Medical Image Segmentation](https://arxiv.org/abs/2511.05803)
*Lalit Maurya,Honghai Liu,Reyer Zwiggelaar*

Main category: cs.CV

TL;DR: 提出MACMD解码器，通过增强注意力机制和通道混合解决医学图像分割中局部细节与长距离依赖的平衡问题


<details>
  <summary>Details</summary>
Motivation: 医学图像分割面临解剖结构变化挑战，现有方法存在两个关键限制：浅层细粒度细节在深层传播中丢失，编码器-解码器间局部细节与全局上下文集成效率低下

Method: 提出基于MACMD的解码器，利用分层扩张卷积、注意力驱动调制和跨通道混合模块，通过跳跃连接增强编码器-解码器阶段的通道混合

Result: 在二元和多器官分割任务上，Dice分数和计算效率均优于现有最先进方法

Conclusion: 该方法有效实现了准确且鲁棒的医学图像分割性能，代码已开源

Abstract: Medical image segmentation faces challenges due to variations in anatomical
structures. While convolutional neural networks (CNNs) effectively capture
local features, they struggle with modeling long-range dependencies.
Transformers mitigate this issue with self-attention mechanisms but lack the
ability to preserve local contextual information. State-of-the-art models
primarily follow an encoder-decoder architecture, achieving notable success.
However, two key limitations remain: (1) Shallow layers, which are closer to
the input, capture fine-grained details but suffer from information loss as
data propagates through deeper layers. (2) Inefficient integration of local
details and global context between the encoder and decoder stages. To address
these challenges, we propose the MACMD-based decoder, which enhances attention
mechanisms and facilitates channel mixing between encoder and decoder stages
via skip connections. This design leverages hierarchical dilated convolutions,
attention-driven modulation, and a cross channel-mixing module to capture
long-range dependencies while preserving local contextual details, essential
for precise medical image segmentation. We evaluated our approach using
multiple transformer encoders on both binary and multi-organ segmentation
tasks. The results demonstrate that our method outperforms state-of-the-art
approaches in terms of Dice score and computational efficiency, highlighting
its effectiveness in achieving accurate and robust segmentation performance.
The code available at https://github.com/lalitmaurya47/MACMD

</details>


### [24] [LRANet++: Low-Rank Approximation Network for Accurate and Efficient Text Spotting](https://arxiv.org/abs/2511.05818)
*Yuchen Su,Zhineng Chen,Yongkun Du,Zuxuan Wu,Hongtao Xie,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: LRANet++是一个端到端文本检测框架，通过基于低秩逼近的参数化文本形状方法和三重分配检测头，实现了对任意形状文本的准确高效检测。


<details>
  <summary>Details</summary>
Motivation: 当前端到端文本检测在处理任意形状文本时存在准确性和效率的瓶颈，主要原因是缺乏可靠且高效的文本检测方法。现有的形状表示方法采用数据无关的参数化，无法充分利用文本形状的内在相关性。

Method: 提出了两个核心技术：1）基于低秩逼近的参数化文本形状方法，通过数据驱动方式从标记边界推导低秩子空间，并使用ℓ1范式恢复方法处理标注噪声；2）三重分配检测头架构，其中深度稀疏分支稳定训练并指导超轻量级稀疏分支加速推理，同时密集分支提供并行监督。将增强的检测模块与轻量级识别分支集成形成LRANet++框架。

Result: 在多个具有挑战性的基准测试上，LRANet++相比于最先进方法展现出优越性能，证明了其在准确性和效率方面的优势。

Conclusion: 通过创新的低秩逼近形状表示和三重分配检测机制，LRANet++成功解决了端到端文本检测中任意形状文本的准确高效检测问题，为该领域提供了新的解决方案。

Abstract: End-to-end text spotting aims to jointly optimize text detection and
recognition within a unified framework. Despite significant progress, designing
an accurate and efficient end-to-end text spotter for arbitrary-shaped text
remains largely unsolved. We identify the primary bottleneck as the lack of a
reliable and efficient text detection method. To address this, we propose a
novel parameterized text shape method based on low-rank approximation for
precise detection and a triple assignment detection head to enable fast
inference. Specifically, unlike other shape representation methods that employ
data-irrelevant parameterization, our data-driven approach derives a low-rank
subspace directly from labeled text boundaries. To ensure this process is
robust against the inherent annotation noise in this data, we utilize a
specialized recovery method based on an $\ell_1$-norm formulation, which
accurately reconstructs the text shape with only a few key orthogonal vectors.
By exploiting the inherent shape correlation among different text contours, our
method achieves consistency and compactness in shape representation. Next, the
triple assignment scheme introduces a novel architecture where a deep sparse
branch (for stabilized training) is used to guide the learning of an
ultra-lightweight sparse branch (for accelerated inference), while a dense
branch provides rich parallel supervision. Building upon these advancements, we
integrate the enhanced detection module with a lightweight recognition branch
to form an end-to-end text spotting framework, termed LRANet++, capable of
accurately and efficiently spotting arbitrary-shaped text. Extensive
experiments on several challenging benchmarks demonstrate the superiority of
LRANet++ compared to state-of-the-art methods. Code will be available at:
https://github.com/ychensu/LRANet-PP.git

</details>


### [25] [Hilbert-Guided Block-Sparse Local Attention](https://arxiv.org/abs/2511.05832)
*Yunge Li,Lanyu Xu*

Main category: cs.CV

TL;DR: 本文提出基于希尔伯特曲线的局部注意力优化方法，通过重新排列图像令牌来提高块稀疏性，实现4-18倍的速度提升，在高分辨率图像处理中显著降低计算和内存成本。


<details>
  <summary>Details</summary>
Motivation: 全局自注意力机制的计算和内存复杂度为二次方，严重限制了其在高分辨率图像中的应用。现有的局部注意力虽然降低了复杂度，但传统的局部注意力模式由于窗口内的令牌在一维序列中不连续，往往无法实现显著的加速效果。

Method: 提出基于希尔伯特曲线的窗口和邻域构建方法：首先将图像令牌沿希尔伯特曲线重新排序，然后在重新排序的一维序列上形成窗口和邻域。从块稀疏视角看，该策略显著增加了块稀疏性，可与现有块稀疏内核结合提高2D局部注意力效率。

Result: 实验表明，希尔伯特窗口注意力和希尔伯特滑动注意力可分别加速窗口注意力和滑动注意力约4倍和18倍。实例化的希尔伯特窗口变换器和希尔伯特邻域变换器在最小精度损失下实现了端到端加速。

Conclusion: 将希尔伯特引导的局部注意力与块稀疏内核结合，为增强图像2D局部注意力效率提供了通用且实用的方法，解决了高分辨率图像处理中计算成本过高的问题。

Abstract: The quadratic compute and memory costs of global self-attention severely
limit its use in high-resolution images. Local attention reduces complexity by
restricting attention to neighborhoods. Block-sparse kernels can further
improve the efficiency of local attention, but conventional local attention
patterns often fail to deliver significant speedups because tokens within a
window are not contiguous in the 1D sequence. This work proposes a novel method
for constructing windows and neighborhoods based on the Hilbert curve. Image
tokens are first reordered along a Hilbert curve, and windows and neighborhoods
are then formed on the reordered 1D sequence. From a block-sparse perspective,
this strategy significantly increases block sparsity and can be combined with
existing block-sparse kernels to improve the efficiency of 2D local attention.
Experiments show that the proposed Hilbert Window Attention and Hilbert Slide
Attention can accelerate window attention and slide attention by about
$4\times$ and $18\times$, respectively. To assess practicality, the strategy is
instantiated as the Hilbert Window Transformer and the Hilbert Neighborhood
Transformer, both of which achieve end-to-end speedups with minimal accuracy
loss. Overall, combining Hilbert-guided local attention with block-sparse
kernels offers a general and practical approach to enhancing the efficiency of
2D local attention for images. The code is available at
https://github.com/Yunge6666/Hilbert-Local-Attention.

</details>


### [26] [Understanding Cross Task Generalization in Handwriting-Based Alzheimer's Screening via Vision Language Adaptation](https://arxiv.org/abs/2511.05841)
*Changqing Gong,Huafeng Qin,Mounim A. El-Yacoubi*

Main category: cs.CV

TL;DR: 本文提出了一种基于CLIP的跨层融合适配器框架(Cross-Layer Fusion Adapter, CLFA)，用于通过手写体样本实现阿尔茨海默病的零样本检测，并系统研究了不同书写任务的跨任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病需要早期检测，而手写体能反映早期的运动和认知衰退。现有基于手写体的研究主要依赖在线轨迹和手工特征，未系统考察任务类型对诊断性能的影响，且大规模视觉语言模型在手写体疾病检测领域的应用尚未充分探索。

Method: 提出轻量级跨层融合适配器框架，在视觉编码器中植入多层融合适配器，逐步将表示对准到手写体特定的医学线索上，实现无提示且高效的零样本推理。

Result: 系统研究了跨任务泛化能力（在特定手写任务上训练，在未见过任务上评估），揭示了最有效区分AD的任务类型和书写模式，并强调了特征性笔画模式和任务级因素对早期AD识别的贡献。

Conclusion: 该框架不仅为AD早期识别提供了诊断洞察，还建立了基于手写体的认知评估基准，为非侵入性、低成本的神退行性疾病筛查开辟了新途径。

Abstract: Alzheimer's disease is a prevalent neurodegenerative disorder for which early
detection is critical. Handwriting-often disrupted in prodromal AD-provides a
non-invasive and cost-effective window into subtle motor and cognitive decline.
Existing handwriting-based AD studies, mostly relying on online trajectories
and hand-crafted features, have not systematically examined how task type
influences diagnostic performance and cross-task generalization. Meanwhile,
large-scale vision language models have demonstrated remarkable zero or
few-shot anomaly detection in natural images and strong adaptability across
medical modalities such as chest X-ray and brain MRI. However,
handwriting-based disease detection remains largely unexplored within this
paradigm. To close this gap, we introduce a lightweight Cross-Layer Fusion
Adapter framework that repurposes CLIP for handwriting-based AD screening. CLFA
implants multi-level fusion adapters within the visual encoder to progressively
align representations toward handwriting-specific medical cues, enabling
prompt-free and efficient zero-shot inference. Using this framework, we
systematically investigate cross-task generalization-training on a specific
handwriting task and evaluating on unseen ones-to reveal which task types and
writing patterns most effectively discriminate AD. Extensive analyses further
highlight characteristic stroke patterns and task-level factors that contribute
to early AD identification, offering both diagnostic insights and a benchmark
for handwriting-based cognitive assessment.

</details>


### [27] [Enhancing Diffusion Model Guidance through Calibration and Regularization](https://arxiv.org/abs/2511.05844)
*Seyed Alireza Javid,Amirhossein Bagheri,Nuria González-Prelcic*

Main category: cs.CV

TL;DR: 本文提出了解决分类器引导扩散模型早期去噪步骤中过度自信预测问题的两种方法：基于Smooth ECE的可微校准目标和多种增强的采样引导策略，在ImageNet上实现了2.13的FID分数，无需重新训练扩散模型。


<details>
  <summary>Details</summary>
Motivation: 分类器引导扩散模型在条件图像生成中表现强大，但在早期去噪步骤中存在过度自信预测问题，导致引导梯度消失，影响生成质量。

Method: 提出两种互补方案：1) 基于平滑期望校准误差(Smooth ECE)的可微校准目标，通过少量微调改善分类器校准；2) 增强的采样引导方法，包括批次级重权重的倾斜采样、保持多样性的自适应熵正则化采样，以及基于f散度的新型采样策略。

Result: 在ImageNet 128x128数据集上，使用ResNet-101分类器，散度正则化引导方法实现了2.13的FID分数，优于现有的分类器引导扩散方法，且不需要重新训练扩散模型。

Conclusion: 原则性校准和散度感知采样为分类器引导扩散模型提供了实用且有效的改进方案，在不需要重新训练扩散模型的情况下显著提升了生成质量。

Abstract: Classifier-guided diffusion models have emerged as a powerful approach for
conditional image generation, but they suffer from overconfident predictions
during early denoising steps, causing the guidance gradient to vanish. This
paper introduces two complementary contributions to address this issue. First,
we propose a differentiable calibration objective based on the Smooth Expected
Calibration Error (Smooth ECE), which improves classifier calibration with
minimal fine-tuning and yields measurable improvements in Frechet Inception
Distance (FID). Second, we develop enhanced sampling guidance methods that
operate on off-the-shelf classifiers without requiring retraining. These
include tilted sampling with batch-level reweighting, adaptive
entropy-regularized sampling to preserve diversity, and a novel
f-divergence-based sampling strategy that strengthens class-consistent guidance
while maintaining mode coverage. Experiments on ImageNet 128x128 demonstrate
that our divergence-regularized guidance achieves an FID of 2.13 using a
ResNet-101 classifier, improving upon existing classifier-guided diffusion
methods while requiring no diffusion model retraining. The results show that
principled calibration and divergence-aware sampling provide practical and
effective improvements for classifier-guided diffusion.

</details>


### [28] [Point Cloud Segmentation of Integrated Circuits Package Substrates Surface Defects Using Causal Inference: Dataset Construction and Methodology](https://arxiv.org/abs/2511.05853)
*Bingyang Guo,Qiang Zuo,Ruiyun Yu*

Main category: cs.CV

TL;DR: 本研究构建了一个用于陶瓷封装基板(CPS)表面缺陷3D分割的高质量点云数据集CPS3D-Seg，并提出了一种基于因果推理的新型3D分割方法CINet，在mIoU和准确率方面显著优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 3D数据的有效分割对于工业应用至关重要，特别是在集成电路(IC)领域检测微小缺陷。陶瓷封装基板作为重要的电子材料，凭借其优越的物理化学性质在IC封装中不可或缺。然而，CPS的复杂结构和微小缺陷，加上缺乏公开可用的数据集，严重阻碍了CPS表面缺陷检测技术的发展。

Method: 研究包含两个主要贡献：1)构建CPS3D-Seg数据集，包含1300个点云样本，涵盖20个产品类别，每个样本提供精确的点级别注释；2)提出基于因果推理的CINet方法，通过结构优化(SR)和质量评估(QA)模块量化点云中的潜在混杂因子。

Result: 广泛实验表明，CINet在mIoU和准确率指标上均显著优于现有算法。同时，基于最先进点云分割算法的综合基准测试验证了CPS3D-Seg数据集的有效性。

Conclusion: 该研究通过提供高精度的CPS3D-Seg数据集和创新的CINet分割方法，成功解决了CPS表面缺陷检测领域的数据集缺失和技术瓶颈问题，为工业质量控制提供了有效的解决方案。

Abstract: The effective segmentation of 3D data is crucial for a wide range of
industrial applications, especially for detecting subtle defects in the field
of integrated circuits (IC). Ceramic package substrates (CPS), as an important
electronic material, are essential in IC packaging owing to their superior
physical and chemical properties. However, the complex structure and minor
defects of CPS, along with the absence of a publically available dataset,
significantly hinder the development of CPS surface defect detection. In this
study, we construct a high-quality point cloud dataset for 3D segmentation of
surface defects in CPS, i.e., CPS3D-Seg, which has the best point resolution
and precision compared to existing 3D industrial datasets. CPS3D-Seg consists
of 1300 point cloud samples under 20 product categories, and each sample
provides accurate point-level annotations. Meanwhile, we conduct a
comprehensive benchmark based on SOTA point cloud segmentation algorithms to
validate the effectiveness of CPS3D-Seg. Additionally, we propose a novel 3D
segmentation method based on causal inference (CINet), which quantifies
potential confounders in point clouds through Structural Refine (SR) and
Quality Assessment (QA) Modules. Extensive experiments demonstrate that CINet
significantly outperforms existing algorithms in both mIoU and accuracy.

</details>


### [29] [CGCE: Classifier-Guided Concept Erasure in Generative Models](https://arxiv.org/abs/2511.05865)
*Viet Nguyen,Vishal M. Patel*

Main category: cs.CV

TL;DR: CGCE是一种基于分类器引导的概念擦除框架，通过在推理时修改不安全文本嵌入来防止有害内容生成，同时保持模型原始质量。


<details>
  <summary>Details</summary>
Motivation: 大规模生成模型存在安全隐患，现有概念擦除方法易受对抗性攻击且在安全性和性能间存在艰难权衡。

Method: 提出CGCE即插即用框架，使用轻量级分类器检测并精炼包含不安全概念的文本嵌入，支持多概念擦除且不改变原始模型权重。

Result: 在多种攻击下实现最先进鲁棒性，保持高生成效用，成功应用于T2I和T2V模型，在安全性与性能间实现最优平衡。

Conclusion: CGCE作为实用有效的安全生成AI解决方案，为现代生成模型提供了可靠的安全保护机制。

Abstract: Recent advancements in large-scale generative models have enabled the
creation of high-quality images and videos, but have also raised significant
safety concerns regarding the generation of unsafe content. To mitigate this,
concept erasure methods have been developed to remove undesirable concepts from
pre-trained models. However, existing methods remain vulnerable to adversarial
attacks that can regenerate the erased content. Moreover, achieving robust
erasure often degrades the model's generative quality for safe, unrelated
concepts, creating a difficult trade-off between safety and performance. To
address this challenge, we introduce Classifier-Guided Concept Erasure (CGCE),
an efficient plug-and-play framework that provides robust concept erasure for
diverse generative models without altering their original weights. CGCE uses a
lightweight classifier operating on text embeddings to first detect and then
refine prompts containing undesired concepts. This approach is highly scalable,
allowing for multi-concept erasure by aggregating guidance from several
classifiers. By modifying only unsafe embeddings at inference time, our method
prevents harmful content generation while preserving the model's original
quality on benign prompts. Extensive experiments show that CGCE achieves
state-of-the-art robustness against a wide range of red-teaming attacks. Our
approach also maintains high generative utility, demonstrating a superior
balance between safety and performance. We showcase the versatility of CGCE
through its successful application to various modern T2I and T2V models,
establishing it as a practical and effective solution for safe generative AI.

</details>


### [30] [Light-Field Dataset for Disparity Based Depth Estimation](https://arxiv.org/abs/2511.05866)
*Suresh Nehra,Aupendu Kar,Jayanta Mukhopadhyay,Prabir Kumar Biswas*

Main category: cs.CV

TL;DR: 本文介绍了一个新的公开可用光场图像数据集，包含真实和合成图像，用于基于视差的深度估计算法研究。


<details>
  <summary>Details</summary>
Motivation: 光场相机通过微透镜阵列同时捕获空间和角度信息，可用于实现鲁棒的深度估计。然而，角度与空间信息的权衡至关重要且依赖于相机焦点位置，现有光场数据集存在不足，迫切需要适合开发和测试新型深度估计算法的优质数据集。

Method: 研究团队使用Lytro Illum光场相机捕获了285幅真实光场图像，创建13幅合成光场图像，开发具有真实相机相似视差特征的合成数据集，并通过机械龙门系统和Blender创建了真实和合成立体光场数据集。

Result: 成功构建了一个包含298幅图像的综合光场数据集，涵盖了真实和合成场景，解决了现有数据集的缺陷，并证明了焦点位置对3D点视差的影响，数据集已在GitHub上公开发布。

Conclusion: 该研究为光场深度估计算法开发提供了重要的研究资源，通过提供包含真实和合成数据的高质量、完全表征的公开数据集，推动了该领域的发展。

Abstract: A Light Field (LF) camera consists of an additional two-dimensional array of
micro-lenses placed between the main lens and sensor, compared to a
conventional camera. The sensor pixels under each micro-lens receive light from
a sub-aperture of the main lens. This enables the image sensor to capture both
spatial information and the angular resolution of a scene point. This
additional angular information is used to estimate the depth of a 3-D scene.
The continuum of virtual viewpoints in light field data enables efficient depth
estimation using Epipolar Line Images (EPIs) with robust occlusion handling.
However, the trade-off between angular information and spatial information is
very critical and depends on the focal position of the camera. To design,
develop, implement, and test novel disparity-based light field depth estimation
algorithms, the availability of suitable light field image datasets is
essential. In this paper, a publicly available light field image dataset is
introduced and thoroughly described. We have also demonstrated the effect of
focal position on the disparity of a 3-D point as well as the shortcomings of
the currently available light field dataset. The proposed dataset contains 285
light field images captured using a Lytro Illum LF camera and 13 synthetic LF
images. The proposed dataset also comprises a synthetic dataset with similar
disparity characteristics to those of a real light field camera. A real and
synthetic stereo light field dataset is also created by using a mechanical
gantry system and Blender. The dataset is available at
https://github.com/aupendu/light-field-dataset.

</details>


### [31] [Hybrid second-order gradient histogram based global low-rank sparse regression for robust face recognition](https://arxiv.org/abs/2511.05893)
*Hongxia Li,Ying Ji,Yongxin Dong,Yuehua Feng*

Main category: cs.CV

TL;DR: 本文提出了一种H2H-GLRSR模型，结合混合二阶梯度直方图特征描述符和全局低秩稀疏回归，用于处理复杂遮挡和光照变化下的人脸识别问题。


<details>
  <summary>Details</summary>
Motivation: 传统低秩稀疏回归模型在人脸识别中面临复杂遮挡和光照变化的挑战，需要更有效的特征描述方法来更好地表征面部图像的局部结构特征，并需要更好的方法来处理结构化噪声。

Method: 1. 设计了混合二阶梯度直方图(H2H)特征描述符，更有效表征面部图像的局部结构特征；2. 将该描述符与稀疏正则化核范数矩阵回归(SR_NMR)集成；3. 在残差矩阵上施加全局低秩约束，使模型能够更好捕获结构化噪声中固有的全局相关性。

Result: 实验结果表明，在涉及遮挡、光照变化和非约束环境的挑战性场景下，提出的方法显著优于现有的基于回归的分类方法。

Conclusion: H2H-GLRSR模型通过创新的特征描述和全局低秩约束，有效解决了复杂遮挡和光照变化下的人脸识别难题，在具有挑战性的环境中表现出优越的性能。

Abstract: Low-rank sparse regression models have been widely applied in the field of
face recognition. To further address the challenges caused by complex
occlusions and illumination variations, this paper proposes a Hybrid
Second-Order Gradient Histogram based Global Low-Rank Sparse Regression
(H2H-GLRSR) model. Specifically, a novel feature descriptor called the Hybrid
Second-Order Gradient Histogram (H2H) is first designed to more effectively
characterize the local structural features of facial images. Then, this
descriptor is integrated with the Sparse Regularized Nuclear Norm based Matrix
Regression (SR$\_$NMR). Moreover, a global low-rank constraint is imposed on
the residual matrix, enabling the model to better capture the global
correlations inherent in structured noise. Experimental results demonstrate
that the proposed method significantly outperforms existing regression-based
classification approaches under challenging scenarios involving occlusions,
illumination changes, and unconstrained environments.

</details>


### [32] [Open-World 3D Scene Graph Generation for Retrieval-Augmented Reasoning](https://arxiv.org/abs/2511.05894)
*Fei Yu,Quan Deng,Shengeng Tang,Yuehua Li,Lechao Cheng*

Main category: cs.CV

TL;DR: 提出了一种开放式3D场景图生成框架，结合视觉语言模型和检索增强推理，实现可泛化和交互式的3D场景理解。


<details>
  <summary>Details</summary>
Motivation: 开放世界环境下的3D场景理解面临根本挑战，主要受限于封闭词汇表监督和静态标注的局限性。

Method: 构建统一框架整合视觉语言模型与基于检索的推理，包含两个核心组件：动态场景图生成模块（无固定标签集检测对象和推理语义关系）和检索增强推理管道（将场景图编码到向量数据库支持多模态查询）。

Result: 在3DSSG和Replica基准测试上的四项任务（场景问答、视觉定位、实例检索、任务规划）中展示了强大的泛化能力和优越性能。

Conclusion: 将开放词汇感知与基于检索的推理相结合是实现可扩展3D场景理解的有效方法，在多样化环境中表现出色。

Abstract: Understanding 3D scenes in open-world settings poses fundamental challenges
for vision and robotics, particularly due to the limitations of
closed-vocabulary supervision and static annotations. To address this, we
propose a unified framework for Open-World 3D Scene Graph Generation with
Retrieval-Augmented Reasoning, which enables generalizable and interactive 3D
scene understanding. Our method integrates Vision-Language Models (VLMs) with
retrieval-based reasoning to support multimodal exploration and language-guided
interaction. The framework comprises two key components: (1) a dynamic scene
graph generation module that detects objects and infers semantic relationships
without fixed label sets, and (2) a retrieval-augmented reasoning pipeline that
encodes scene graphs into a vector database to support text/image-conditioned
queries. We evaluate our method on 3DSSG and Replica benchmarks across four
tasks-scene question answering, visual grounding, instance retrieval, and task
planning-demonstrating robust generalization and superior performance in
diverse environments. Our results highlight the effectiveness of combining
open-vocabulary perception with retrieval-based reasoning for scalable 3D scene
understanding.

</details>


### [33] [Causal Tracing of Object Representations in Large Vision Language Models: Mechanistic Interpretability and Hallucination Mitigation](https://arxiv.org/abs/2511.05923)
*Qiming Li,Zekai Ye,Xiaocheng Feng,Weihong Zhong,Weitao Ma,Xiachong Feng*

Main category: cs.CV

TL;DR: 提出了细粒度跨模态因果追踪（FCCT）框架，用于分析大视觉语言模型的视觉对象感知机制，并基于分析结果开发了一种无需训练的推理时技术IRI，通过精确干预跨模态表示来增强感知并减轻幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉语言模型的机制可解释性研究不够全面，缺乏对视觉文本token、模型组件和所有层的系统性分析，这限制了对模型输出真实性改进和下游任务开发的深入理解。

Method: 开发FCCT框架系统量化视觉对象感知的因果效应，覆盖所有视觉文本token、MHSA、FFNs和隐藏状态三个核心组件，跨越所有解码器层。基于发现中间层最后token的MHSAs在跨模态信息聚合中的关键作用和FFNs的三阶段层次化进展，提出IRI推理时干预技术。

Result: 在五个广泛使用的基准测试和LVLMs上验证，IRI实现了最先进性能，持续改善模型表现，同时保持推理速度和其他基础性能不变。

Conclusion: 首次系统揭示了大视觉语言模型中跨模态信息处理的具体机制，为模型改进提供了可操作的见解，证明了基于机制分析的推理时干预技术可以有效增强视觉感知并减轻幻觉问题。

Abstract: Despite the remarkable advancements of Large Vision-Language Models (LVLMs),
the mechanistic interpretability remains underexplored. Existing analyses are
insufficiently comprehensive and lack examination covering visual and textual
tokens, model components, and the full range of layers. This limitation
restricts actionable insights to improve the faithfulness of model output and
the development of downstream tasks, such as hallucination mitigation. To
address this limitation, we introduce Fine-grained Cross-modal Causal Tracing
(FCCT) framework, which systematically quantifies the causal effects on visual
object perception. FCCT conducts fine-grained analysis covering the full range
of visual and textual tokens, three core model components including multi-head
self-attention (MHSA), feed-forward networks (FFNs), and hidden states, across
all decoder layers. Our analysis is the first to demonstrate that MHSAs of the
last token in middle layers play a critical role in aggregating cross-modal
information, while FFNs exhibit a three-stage hierarchical progression for the
storage and transfer of visual object representations. Building on these
insights, we propose Intermediate Representation Injection (IRI), a
training-free inference-time technique that reinforces visual object
information flow by precisely intervening on cross-modal representations at
specific components and layers, thereby enhancing perception and mitigating
hallucination. Consistent improvements across five widely used benchmarks and
LVLMs demonstrate IRI achieves state-of-the-art performance, while preserving
inference speed and other foundational performance.

</details>


### [34] [CoMA: Complementary Masking and Hierarchical Dynamic Multi-Window Self-Attention in a Unified Pre-training Framework](https://arxiv.org/abs/2511.05929)
*Jiaxuan Li,Qing Xu,Xiangjian He,Ziyu Liu,Chang Xing,Zhen Chen,Daokun Zhang,Rong Qu,Chang Wen Chen*

Main category: cs.CV

TL;DR: CoMA结合DyViT通过互补掩码策略和动态多头自注意力机制，仅用12%的训练时间和10%的每轮时间就达到了MAE的性能，显著提升了预训练效率。


<details>
  <summary>Details</summary>
Motivation: 传统MAE方法采用随机掩码策略需要更多预训练轮次来保持适应性，且ViT在MAE中因固定空间分辨率导致参数利用效率低下。

Method: 提出Complementary Masked Autoencoders (CoMA)采用互补掩码策略确保所有像素均匀采样，并引入DyViT分层视觉Transformer，使用动态多头自注意力机制(DM-MSA)减少参数和计算量。

Result: 在ImageNet-1K上预训练的DyViT仅需12%的预训练轮次即可匹配MAE的下游任务性能，每轮预训练时间减少10%，展现出更有效的学习能力。

Conclusion: CoMA和DyViT的结合解决了传统MAE训练效率低和参数利用不合理的问题，实现了更高效的视觉表示学习，为大规模视觉预训练提供了新方案。

Abstract: Masked Autoencoders (MAE) achieve self-supervised learning of image
representations by randomly removing a portion of visual tokens and
reconstructing the original image as a pretext task, thereby significantly
enhancing pretraining efficiency and yielding excellent adaptability across
downstream tasks. However, MAE and other MAE-style paradigms that adopt random
masking generally require more pre-training epochs to maintain adaptability.
Meanwhile, ViT in MAE suffers from inefficient parameter use due to fixed
spatial resolution across layers. To overcome these limitations, we propose the
Complementary Masked Autoencoders (CoMA), which employ a complementary masking
strategy to ensure uniform sampling across all pixels, thereby improving
effective learning of all features and enhancing the model's adaptability.
Furthermore, we introduce DyViT, a hierarchical vision transformer that employs
a Dynamic Multi-Window Self-Attention (DM-MSA), significantly reducing the
parameters and FLOPs while improving fine-grained feature learning. Pre-trained
on ImageNet-1K with CoMA, DyViT matches the downstream performance of MAE using
only 12% of the pre-training epochs, demonstrating more effective learning. It
also attains a 10% reduction in pre-training time per epoch, further
underscoring its superior pre-training efficiency.

</details>


### [35] [AD-DAE: Unsupervised Modeling of Longitudinal Alzheimer's Disease Progression with Diffusion Auto-Encoder](https://arxiv.org/abs/2511.05934)
*Ayantika Das,Arunima Sarkar,Keerthi Ram,Mohanasankar Sivaprakasam*

Main category: cs.CV

TL;DR: 本文提出了一种可条件扩散自编码器框架，用于无监督地从基线图像生成疾病进展图像，在阿尔茨海默病数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有生成建模方法在捕获纵向疾病进展时存在局限：对分布学习施加约束，导致潜在空间可控性有限，需要特定受试者的纵向图像监督才能生成随访图像。

Method: 引入条件扩散自编码器框架，利用图像扩散自编码器的显式编码机制构建紧凑潜在空间捕获高级语义，通过限制位移到子空间实现可控性，分离进展相关因素和身份保持组件，位移通过与进展属性相关性进行隐式指导，无需受试者特异性纵向监督。

Result: 通过图像质量指标、体积进展分析和下游分类在两个不同来源和疾病类别的阿尔茨海默病数据集上验证了方法效果，证明了该框架在阿尔茨海默病进展建模和纵向图像生成方面的有效性。

Conclusion: 所提出的可条件扩散自编码器框架成功实现了潜在空间的受控位移，能够无监督地从基线图像生成进展图像，为阿尔茨海默病等疾病的纵向进展建模提供了有效的解决方案。

Abstract: Generative modeling frameworks have emerged as an effective approach to
capture high-dimensional image distributions from large datasets without
requiring domain-specific knowledge, a capability essential for longitudinal
disease progression modeling. Recent generative modeling approaches have
attempted to capture progression by mapping images into a latent
representational space and then controlling and guiding the representations to
generate follow-up images from a baseline image. However, existing approaches
impose constraints on distribution learning, leading to latent spaces with
limited controllability to generate follow-up images without explicit
supervision from subject-specific longitudinal images. In order to enable
controlled movements in the latent representational space and generate
progression images from a baseline image in an unsupervised manner, we
introduce a conditionable Diffusion Auto-encoder framework. The explicit
encoding mechanism of image-diffusion auto-encoders forms a compact latent
space capturing high-level semantics, providing means to disentangle
information relevant for progression. Our approach leverages this latent space
to condition and apply controlled shifts to baseline representations for
generating follow-up. Controllability is induced by restricting these shifts to
a subspace, thereby isolating progression-related factors from subject
identity-preserving components. The shifts are implicitly guided by correlating
with progression attributes, without requiring subject-specific longitudinal
supervision. We validate the generations through image quality metrics,
volumetric progression analysis, and downstream classification in Alzheimer's
disease datasets from two different sources and disease categories. This
demonstrates the effectiveness of our approach for Alzheimer's progression
modeling and longitudinal image generation.

</details>


### [36] [Interaction-Centric Knowledge Infusion and Transfer for Open-Vocabulary Scene Graph Generation](https://arxiv.org/abs/2511.05935)
*Lin Li,Chuhan Zhang,Dong Zhang,Chong Sun,Chen Li,Long Chen*

Main category: cs.CV

TL;DR: 论文提出ACC框架，一个交互驱动的端到端开放词汇场景图生成方法，通过显式建模对象交互来解决现有方法在区分交互与非交互实例方面的不足，在多个基准测试上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有OVSGG方法采用两阶段流程但缺乏显式交互建模，难以区分同一对象类别中交互和非交互实例，这导致在知识注入阶段产生噪声伪监督，在知识转移阶段造成模糊查询匹配，严重影响模型性能。

Method: 提出ACC交互中心框架：1)交互中心知识注入-使用双向交互提示生成鲁棒伪监督；2)交互中心知识转移-采用交互引导查询选择优先配对交互对象，并结合交互一致知识蒸馏将关系前景与背景分离。

Result: 在三个基准测试上的广泛实验表明，ACC方法达到了最先进的性能表现，证明了交互中心范式在实际应用中的潜力。

Conclusion: ACC框架通过交互驱动范式有效解决了OVSGG中的匹配问题，为开放词汇场景图生成提供了一种新的解决方案，展现了交互中心方法在计算机视觉任务中的重要作用。

Abstract: Open-vocabulary scene graph generation (OVSGG) extends traditional SGG by
recognizing novel objects and relationships beyond predefined categories,
leveraging the knowledge from pre-trained large-scale models. Existing OVSGG
methods always adopt a two-stage pipeline: 1) \textit{Infusing knowledge} into
large-scale models via pre-training on large datasets; 2) \textit{Transferring
knowledge} from pre-trained models with fully annotated scene graphs during
supervised fine-tuning. However, due to a lack of explicit interaction
modeling, these methods struggle to distinguish between interacting and
non-interacting instances of the same object category. This limitation induces
critical issues in both stages of OVSGG: it generates noisy pseudo-supervision
from mismatched objects during knowledge infusion, and causes ambiguous query
matching during knowledge transfer. To this end, in this paper, we propose an
inter\textbf{AC}tion-\textbf{C}entric end-to-end OVSGG framework (\textbf{ACC})
in an interaction-driven paradigm to minimize these mismatches. For
\textit{interaction-centric knowledge infusion}, ACC employs a bidirectional
interaction prompt for robust pseudo-supervision generation to enhance the
model's interaction knowledge. For \textit{interaction-centric knowledge
transfer}, ACC first adopts interaction-guided query selection that prioritizes
pairing interacting objects to reduce interference from non-interacting ones.
Then, it integrates interaction-consistent knowledge distillation to bolster
robustness by pushing relational foreground away from the background while
retaining general knowledge. Extensive experimental results on three benchmarks
show that ACC achieves state-of-the-art performance, demonstrating the
potential of interaction-centric paradigms for real-world applications.

</details>


### [37] [Polymap: generating high definition map based on rasterized polygons](https://arxiv.org/abs/2511.05944)
*Shiyu Gao,Hao Jiang*

Main category: cs.CV

TL;DR: 该论文提出了一种基于实例分割的高清地图感知方法，通过将道路元素重新解释为光栅化多边形，并使用基于分割的Transformer生成实例掩码，再结合Potrace后处理模块得到矢量化地图元素，在NuScenes数据集上验证了方法的有效性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有基于检测的高清地图构建方法虽然能实现实时构建，但缺乏鲁棒的泛化能力，限制了其在自动标注系统中的应用。因此需要开发具有更好泛化性的新方法。

Method: 将道路元素重新解释为光栅化多边形，设计基于实例分割的简洁框架：首先使用基于分割的Transformer端到端生成实例掩码，然后通过基于Potrace的后处理模块最终得到矢量化地图元素。

Result: 在NuScenes数据集上的定量结果证明了所提方法的有效性和泛化能力。

Conclusion: 通过将道路元素重新解释为光栅化多边形并采用实例分割方法，成功改善了高清地图感知的泛化性，为自动标注系统提供了更可靠的解决方案。

Abstract: The perception of high-definition maps is an integral component of
environmental perception in autonomous driving systems. Existing research have
often focused on online construction of high-definition maps. For instance, the
Maptr[9] series employ a detection-based method to output vectorized map
instances parallelly in an end-to-end manner. However, despite their capability
for real-time construction, detection-based methods are observed to lack robust
generalizability[19], which hampers their applicability in auto-labeling
systems. Therefore, aiming to improve the generalizability, we reinterpret road
elements as rasterized polygons and design a concise framework based on
instance segmentation. Initially, a segmentation-based transformer is employed
to deliver instance masks in an end-to-end manner; succeeding this step, a
Potrace-based[17] post-processing module is used to ultimately yield vectorized
map elements. Quantitative results attained on the Nuscene[1] dataset
substantiate the effectiveness and generaliz-ability of our method.

</details>


### [38] [U(PM)$^2$:Unsupervised polygon matching with pre-trained models for challenging stereo images](https://arxiv.org/abs/2511.05949)
*Chang Li,Xingtao Peng*

Main category: cs.CV

TL;DR: 本文提出了U(PM)²：一种低成本的预训练模型无监督多边形匹配方法，通过结合自动学习和手工特征来解决多边形匹配中的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 多边形匹配是计算机视觉、摄影测量和遥感中的基础任务，但该领域几乎未被探索，面临视差不连续、尺度变化、训练需求和泛化能力等挑战。

Method: 方法包含三个主要组件：1)检测器使用预训练的Segment Anything模型获取掩码；2)矢量化器将掩码转换为多边形和图形结构；3)全局匹配器基于双向金字塔策略和预训练LoFTR处理全局视角变化和尺度变化；4)局部匹配器使用局部联合几何和多特征匹配策略结合匈牙利算法克服局部视差不连续和拓扑不一致性。

Result: 在ScanNet和SceneFlow数据集上使用提出的新指标进行基准测试，实现了最先进的准确率、具有竞争力的速度，且无需任何训练需求即可在低成本下获得令人满意的泛化性能。

Conclusion: 提出的U(PM)²成功解决了多边形匹配中的主要挑战，在保持低成本和无训练需求的同时，实现了优异的匹配精度和良好的泛化能力。

Abstract: Stereo image matching is a fundamental task in computer vision,
photogrammetry and remote sensing, but there is an almost unexplored field,
i.e., polygon matching, which faces the following challenges: disparity
discontinuity, scale variation, training requirement, and generalization. To
address the above-mentioned issues, this paper proposes a novel U(PM)$^2$:
low-cost unsupervised polygon matching with pre-trained models by uniting
automatically learned and handcrafted features, of which pipeline is as
follows: firstly, the detector leverages the pre-trained segment anything model
to obtain masks; then, the vectorizer converts the masks to polygons and
graphic structure; secondly, the global matcher addresses challenges from
global viewpoint changes and scale variation based on bidirectional-pyramid
strategy with pre-trained LoFTR; finally, the local matcher further overcomes
local disparity discontinuity and topology inconsistency of polygon matching by
local-joint geometry and multi-feature matching strategy with Hungarian
algorithm. We benchmark our U(PM)$^2$ on the ScanNet and SceneFlow datasets
using our proposed new metric, which achieved state-of-the-art accuracy at a
competitive speed and satisfactory generalization performance at low cost
without any training requirement.

</details>


### [39] [CSGaze: Context-aware Social Gaze Prediction](https://arxiv.org/abs/2511.05955)
*Surbhi Madan,Shreya Ghosh,Ramanathan Subramanian,Abhinav Dhall,Tom Gedeon*

Main category: cs.CV

TL;DR: CSGaze是一种结合上下文线索、视觉场景和面部信息的多模态方法，用于预测对话互动中的社交凝视模式，性能具有竞争力且具备可解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 个人的凝视可以揭示其注意力焦点、社交参与度和自信心，研究者希望通过结合多种上下文线索来更准确地预测和理解对话过程中的社交凝视模式。

Method: 提出CSGaze模型，一种上下文感知的多模态方法，将面部和场景信息作为互补输入来增强多人图像的社交凝视模式预测，并引入以主要发言者为中心的细粒度注意力机制来更好地建模社交凝视动态。

Result: 在GP-Static、UCO-LAEO和AVA-LAEO数据集上与最先进方法相比表现具有竞争力，通过注意力分数提供了初步的可解释性，并在开放集数据集上测试证明了模型在多样化场景中的鲁棒性。

Conclusion: 研究表明上下文线索在改善社交凝视预测中发挥重要作用，CSGaze模型不仅具有良好的预测性能，还提供了模型决策过程的洞察，并在不同场景下展现出良好的泛化能力。

Abstract: A person's gaze offers valuable insights into their focus of attention, level
of social engagement, and confidence. In this work, we investigate how
contextual cues combined with visual scene and facial information can be
effectively utilized to predict and interpret social gaze patterns during
conversational interactions. We introduce CSGaze, a context aware multimodal
approach that leverages facial, scene information as complementary inputs to
enhance social gaze pattern prediction from multi-person images. The model also
incorporates a fine-grained attention mechanism centered on the principal
speaker, which helps in better modeling social gaze dynamics. Experimental
results show that CSGaze performs competitively with state-of-the-art methods
on GP-Static, UCO-LAEO and AVA-LAEO. Our findings highlight the role of
contextual cues in improving social gaze prediction. Additionally, we provide
initial explainability through generated attention scores, offering insights
into the model's decision-making process. We also demonstrate our model's
generalizability by testing our model on open set datasets that demonstrating
its robustness across diverse scenarios.

</details>


### [40] [Commonality in Few: Few-Shot Multimodal Anomaly Detection via Hypergraph-Enhanced Memory](https://arxiv.org/abs/2511.05966)
*Yuxuan Lin,Hanjing Yan,Xuan Tong,Yang Chang,Huanzhen Wang,Ziheng Zhou,Shuyong Gao,Yan Wang,Wenqiang Zhang*

Main category: cs.CV

TL;DR: 提出基于结构共性的少样本多模态工业异常检测方法CIF，利用超图提取高阶相关性，通过记忆库存储类内结构先验，在MVTec 3D-AD和Eyecandies数据集上超越SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 少样本多模态工业异常检测是关键但未被充分探索的任务。少样本设置下，训练样本不足往往无法覆盖测试样本中的多样模式，这一挑战可通过从少量训练样本中提取结构共性来缓解。

Method: 提出CIF（Commonality In Few）方法，包含：1）语义感知超图构建模块，提取共性结构指导记忆库构建；2）训练无关的超图消息传递模块，更新测试样本视觉特征，缩小分布差距；3）超边引导记忆搜索模块，利用结构信息辅助搜索过程，降低误报率。使用超图建模高阶相关性，记忆库存储类内结构先验。

Result: 在MVTec 3D-AD数据集和Eyecandies数据集上的实验结果表明，该方法在少样本设置下超越了现有最先进方法的性能。

Conclusion: CIF方法有效解决了少样本多模态工业异常检测问题，通过超图机制和记忆机制成功利用结构共性，显著提升了异常检测性能并降低了误报率。

Abstract: Few-shot multimodal industrial anomaly detection is a critical yet
underexplored task, offering the ability to quickly adapt to complex industrial
scenarios. In few-shot settings, insufficient training samples often fail to
cover the diverse patterns present in test samples. This challenge can be
mitigated by extracting structural commonality from a small number of training
samples. In this paper, we propose a novel few-shot unsupervised multimodal
industrial anomaly detection method based on structural commonality, CIF
(Commonality In Few). To extract intra-class structural information, we employ
hypergraphs, which are capable of modeling higher-order correlations, to
capture the structural commonality within training samples, and use a memory
bank to store this intra-class structural prior. Firstly, we design a
semantic-aware hypergraph construction module tailored for single-semantic
industrial images, from which we extract common structures to guide the
construction of the memory bank. Secondly, we use a training-free hypergraph
message passing module to update the visual features of test samples, reducing
the distribution gap between test features and features in the memory bank. We
further propose a hyperedge-guided memory search module, which utilizes
structural information to assist the memory search process and reduce the false
positive rate. Experimental results on the MVTec 3D-AD dataset and the
Eyecandies dataset show that our method outperforms the state-of-the-art (SOTA)
methods in few-shot settings. Code is available at
https://github.com/Sunny5250/CIF.

</details>


### [41] [Adapted Foundation Models for Breast MRI Triaging in Contrast-Enhanced and Non-Contrast Enhanced Protocols](https://arxiv.org/abs/2511.05967)
*Tri-Thien Nguyen,Lorenz A. Kapsner,Tobias Hepp,Shirin Heidarikahkesh,Hannes Schreiter,Luise Brock,Dominika Skwierawska,Dominique Hadler,Julian Hossbach,Evelyn Wenkel,Sabine Ohlmeyer,Frederik B. Laun,Andrzej Liebert,Andreas Maier,Michael Uder,Sebastian Bickelhaupt*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Background: Magnetic resonance imaging (MRI) has high sensitivity for breast
cancer detection, but interpretation is time-consuming. Artificial intelligence
may aid in pre-screening. Purpose: To evaluate the DINOv2-based Medical Slice
Transformer (MST) for ruling out significant findings (Breast Imaging Reporting
and Data System [BI-RADS] >=4) in contrast-enhanced and non-contrast-enhanced
abbreviated breast MRI. Materials and Methods: This institutional review board
approved retrospective study included 1,847 single-breast MRI examinations (377
BI-RADS >=4) from an in-house dataset and 924 from an external validation
dataset (Duke). Four abbreviated protocols were tested: T1-weighted early
subtraction (T1sub), diffusion-weighted imaging with b=1500 s/mm2 (DWI1500),
DWI1500+T2-weighted (T2w), and T1sub+T2w. Performance was assessed at 90%, 95%,
and 97.5% sensitivity using five-fold cross-validation and area under the
receiver operating characteristic curve (AUC) analysis. AUC differences were
compared with the DeLong test. False negatives were characterized, and
attention maps of true positives were rated in the external dataset. Results: A
total of 1,448 female patients (mean age, 49 +/- 12 years) were included.
T1sub+T2w achieved an AUC of 0.77 +/- 0.04; DWI1500+T2w, 0.74 +/- 0.04
(p=0.15). At 97.5% sensitivity, T1sub+T2w had the highest specificity (19% +/-
7%), followed by DWI1500+T2w (17% +/- 11%). Missed lesions had a mean diameter
<10 mm at 95% and 97.5% thresholds for both T1sub and DWI1500, predominantly
non-mass enhancements. External validation yielded an AUC of 0.77, with 88% of
attention maps rated good or moderate. Conclusion: At 97.5% sensitivity, the
MST framework correctly triaged cases without BI-RADS >=4, achieving 19%
specificity for contrast-enhanced and 17% for non-contrast-enhanced MRI.
Further research is warranted before clinical implementation.

</details>


### [42] [DiA-gnostic VLVAE: Disentangled Alignment-Constrained Vision Language Variational AutoEncoder for Robust Radiology Reporting with Missing Modalities](https://arxiv.org/abs/2511.05968)
*Nagur Shareef Shaik,Teja Krishna Cherukuri,Adnan Masood,Dong Hye Ye*

Main category: cs.CV

TL;DR: 本文提出了DiA-gnostic VLVAE框架，通过解耦对齐方法解决医学影像报告生成中的缺失模态和特征纠缠问题，在IU X-Ray和MIMIC-CXR数据集上取得了竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动化放射学报告生成方法依赖资源密集的大型语言模型或静态知识图谱，面临两个关键挑战：(1)缺失模态问题，如临床背景信息不完整；(2)特征纠缠问题，模态特定和共享信息混合导致融合效果差和临床不真实的幻觉发现。

Method: 提出DiA-gnostic VLVAE框架，采用基于专家混合(MoE)的视觉-语言变分自编码器解耦共享和模态特定特征，通过约束优化目标强制潜在表示的正交性和对齐，使用紧凑的LLaMA-X解码器高效生成报告。

Result: 在IU X-Ray和MIMIC-CXR数据集上分别取得0.266和0.134的BLEU@4分数，实验结果表明该方法显著优于现有最先进模型。

Conclusion: DiA-gnostic VLVAE通过特征解耦和对齐策略有效解决了医学影像报告生成中的关键挑战，实现了对缺失模态的鲁棒性和更准确的临床报告生成，为自动化放射学报告生成提供了新的解决方案。

Abstract: The integration of medical images with clinical context is essential for
generating accurate and clinically interpretable radiology reports. However,
current automated methods often rely on resource-heavy Large Language Models
(LLMs) or static knowledge graphs and struggle with two fundamental challenges
in real-world clinical data: (1) missing modalities, such as incomplete
clinical context , and (2) feature entanglement, where mixed modality-specific
and shared information leads to suboptimal fusion and clinically unfaithful
hallucinated findings. To address these challenges, we propose the DiA-gnostic
VLVAE, which achieves robust radiology reporting through Disentangled
Alignment. Our framework is designed to be resilient to missing modalities by
disentangling shared and modality-specific features using a Mixture-of-Experts
(MoE) based Vision-Language Variational Autoencoder (VLVAE). A constrained
optimization objective enforces orthogonality and alignment between these
latent representations to prevent suboptimal fusion. A compact LLaMA-X decoder
then uses these disentangled representations to generate reports efficiently.
On the IU X-Ray and MIMIC-CXR datasets, DiA has achieved competetive BLEU@4
scores of 0.266 and 0.134, respectively. Experimental results show that the
proposed method significantly outperforms state-of-the-art models.

</details>


### [43] [A Dual-Mode ViT-Conditioned Diffusion Framework with an Adaptive Conditioning Bridge for Breast Cancer Segmentation](https://arxiv.org/abs/2511.05989)
*Prateek Singh,Moumita Dholey,P. K. Vinod*

Main category: cs.CV

TL;DR: 本文提出了一种基于条件去噪扩散模型的乳腺超声图像分割框架，结合ViT编码器和增强UNet解码器，通过ACB特征融合、TDC拓扑约束和双头架构三个创新点，实现了最先进的分割性能。


<details>
  <summary>Details</summary>
Motivation: 乳腺超声图像中病灶精确分割对早期诊断至关重要，但面临低对比度、斑点噪声和边界模糊等挑战。现有深度学习方法在捕获全局上下文方面存在局限，导致产生解剖学不一致的分割结果。

Method: 提出灵活的条件去噪扩散模型，结合Vision Transformer编码器进行全局特征提取和增强的UNet生成解码器。主要创新包括：1)自适应条件桥接(ACB)实现高效多尺度语义特征融合；2)拓扑去噪一致性(TDC)损失通过惩罚去噪过程中的结构不一致性来正则化训练；3)双头架构利用去噪目标作为强正则化器。

Result: 在公开乳腺超声数据集上建立新的最先进性能：BUSI数据集Dice得分0.96，BrEaST数据集0.90，BUS-UCLM数据集0.97。全面的消融研究验证了模型各组件对实现这些结果的关键作用。

Conclusion: 所提框架不仅实现了准确的分割，还保证了解剖学合理性。三个核心创新组件共同贡献了卓越性能，为乳腺超声图像分割提供了有效解决方案，具有重要的临床应用价值。

Abstract: In breast ultrasound images, precise lesion segmentation is essential for
early diagnosis; however, low contrast, speckle noise, and unclear boundaries
make this difficult. Even though deep learning models have demonstrated
potential, standard convolutional architectures frequently fall short in
capturing enough global context, resulting in segmentations that are
anatomically inconsistent. To overcome these drawbacks, we suggest a flexible,
conditional Denoising Diffusion Model that combines an enhanced UNet-based
generative decoder with a Vision Transformer (ViT) encoder for global feature
extraction. We introduce three primary innovations: 1) an Adaptive Conditioning
Bridge (ACB) for efficient, multi-scale fusion of semantic features; 2) a novel
Topological Denoising Consistency (TDC) loss component that regularizes
training by penalizing structural inconsistencies during denoising; and 3) a
dual-head architecture that leverages the denoising objective as a powerful
regularizer, enabling a lightweight auxiliary head to perform rapid and
accurate inference on smaller datasets and a noise prediction head. Our
framework establishes a new state-of-the-art on public breast ultrasound
datasets, achieving Dice scores of 0.96 on BUSI, 0.90 on BrEaST and 0.97 on
BUS-UCLM. Comprehensive ablation studies empirically validate that the model
components are critical for achieving these results and for producing
segmentations that are not only accurate but also anatomically plausible.

</details>


### [44] [Exploring Category-level Articulated Object Pose Tracking on SE(3) Manifolds](https://arxiv.org/abs/2511.05996)
*Xianhui Meng,Yukang Huo,Li Zhang,Liu Liu,Haonan Jiang,Yan Zhong,Pingrui Zhang,Cewu Lu,Jun Liu*

Main category: cs.CV

TL;DR: PPF-Tracker是一个基于点对特征的新型姿态跟踪框架，通过在SE(3)李群空间中进行点云准标准化和利用运动学约束，实现了对关节物体的鲁棒多帧姿态跟踪。


<details>
  <summary>Details</summary>
Motivation: 关节物体在日常生活和机器人操作任务中普遍存在，但与刚性物体相比，由于其固有的运动学约束，关节物体的姿态跟踪问题仍然是一个未被充分探索的领域。

Method: 提出了PPF-Tracker框架：首先在SE(3)李群空间中对点云进行准标准化处理，然后利用点对特征(PPF)建模关节物体，通过SE(3)的不变性性质预测姿态投票参数，最后结合关节轴的语义信息对关节物体的所有部分施加统一的运动学约束。

Result: 在合成数据集和真实场景中对PPF-Tracker进行了系统评估，结果表明该方法在多样化和具有挑战性的环境中具有很强的泛化能力，在关节物体的多帧姿态跟踪方面表现出有效性和鲁棒性。

Conclusion: 这项工作能够促进机器人学、具身智能和增强现实领域的进步，为关节物体的姿态跟踪提供了新的解决方案。

Abstract: Articulated objects are prevalent in daily life and robotic manipulation
tasks. However, compared to rigid objects, pose tracking for articulated
objects remains an underexplored problem due to their inherent kinematic
constraints. To address these challenges, this work proposes a novel
point-pair-based pose tracking framework, termed \textbf{PPF-Tracker}. The
proposed framework first performs quasi-canonicalization of point clouds in the
SE(3) Lie group space, and then models articulated objects using Point Pair
Features (PPF) to predict pose voting parameters by leveraging the invariance
properties of SE(3). Finally, semantic information of joint axes is
incorporated to impose unified kinematic constraints across all parts of the
articulated object. PPF-Tracker is systematically evaluated on both synthetic
datasets and real-world scenarios, demonstrating strong generalization across
diverse and challenging environments. Experimental results highlight the
effectiveness and robustness of PPF-Tracker in multi-frame pose tracking of
articulated objects. We believe this work can foster advances in robotics,
embodied intelligence, and augmented reality. Codes are available at
https://github.com/mengxh20/PPFTracker.

</details>


### [45] [MALeR: Improving Compositional Fidelity in Layout-Guided Generation](https://arxiv.org/abs/2511.06002)
*Shivank Saxena,Dhruv Srivastava,Makarand Tapaswi*

Main category: cs.CV

TL;DR: MALeR是一种改进的文本到图像生成方法，通过布局引导和属性感知机制，解决了多主体多属性组合场景中的生成难题。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型在生成具有多个主体和属性的组合场景时面临重大挑战。现有的布局引导方法存在诸多问题：主体可能出现在布局之外、生成图像超出分布范围且包含不自然伪影、属性在主体间泄漏导致错误的视觉输出。

Method: MALeR方法基于文本提示和对应布局，防止主体出现在给定布局之外，同时保持生成结果在分布内。提出了一种掩码的、属性感知的绑定机制，防止属性泄漏，能够准确渲染具有多个属性的主体，即使在复杂的组合场景中也能有效工作。

Result: 定性和定量评估表明，MALeR在组合准确性、生成一致性和属性绑定方面相比之前的工作实现了优越性能。该方法特别擅长生成具有多个主体且每个主体具有多个属性的场景图像。

Conclusion: MALeR成功解决了文本到图像生成中的组合场景挑战，通过创新的布局控制和属性绑定机制，实现了更准确、更可控的多主体多属性图像生成，为创意图像生成领域提供了重要的技术进步。

Abstract: Recent advances in text-to-image models have enabled a new era of creative
and controllable image generation. However, generating compositional scenes
with multiple subjects and attributes remains a significant challenge. To
enhance user control over subject placement, several layout-guided methods have
been proposed. However, these methods face numerous challenges, particularly in
compositional scenes. Unintended subjects often appear outside the layouts,
generated images can be out-of-distribution and contain unnatural artifacts, or
attributes bleed across subjects, leading to incorrect visual outputs. In this
work, we propose MALeR, a method that addresses each of these challenges. Given
a text prompt and corresponding layouts, our method prevents subjects from
appearing outside the given layouts while being in-distribution. Additionally,
we propose a masked, attribute-aware binding mechanism that prevents attribute
leakage, enabling accurate rendering of subjects with multiple attributes, even
in complex compositional scenes. Qualitative and quantitative evaluation
demonstrates that our method achieves superior performance in compositional
accuracy, generation consistency, and attribute binding compared to previous
work. MALeR is particularly adept at generating images of scenes with multiple
subjects and multiple attributes per subject.

</details>


### [46] [Distributed Deep Learning for Medical Image Denoising with Data Obfuscation](https://arxiv.org/abs/2511.06006)
*Sulaimon Oyeniyi Adebayo,Ayaz H. Khan*

Main category: cs.CV

TL;DR: 本研究探讨了分布式深度学习在胸X光图像去噪中的应用，结合轻量级混淆技术，优化了U-Net和U-Net++架构的训练流程。实验表明，U-Net++在去噪性能上表现更优，而优化的分布式训练显著提升了训练效率。


<details>
  <summary>Details</summary>
Motivation: 医学图像去噪对提高图像质量和保护敏感信息至关重要。随着大规模临床数据集的使用，需要高效的分布式学习方法来加速训练过程，同时确保数据隐私。

Method: 在NIH Chest X-ray14数据集上应用添加高斯噪声作为轻量级混淆技术，实现并评估了U-Net和U-Net++架构。通过PyTorch的DistributedDataParallel (DDP)和Automatic Mixed Precision (AMP)优化多GPU训练配置，与单GPU和标准DataParallel进行对比。

Result: U-Net++在PSNR和SSIM指标上表现更优，展现出更强的结构保真度，但在LPIPS上略逊于U-Net。优化的训练流程相比单GPU训练减少60%以上的时间，比标准DataParallel提升40%以上效率，仅有轻微精度下降。

Conclusion: 该研究证明了软件级优化在医学影像分布式学习中的有效性，展示了架构设计、轻量级混淆和先进分布式训练策略相结合的实用可行性，为临床和研究环境中的医学图像处理提供了高效解决方案。

Abstract: Medical image denoising is essential for improving image quality while
minimizing the exposure of sensitive information, particularly when working
with large-scale clinical datasets. This study explores distributed deep
learning for denoising chest X-ray images from the NIH Chest X-ray14 dataset,
using additive Gaussian noise as a lightweight obfuscation technique. We
implement and evaluate U-Net and U-Net++ architectures under single-GPU,
standard multi-GPU (DataParallel), and optimized multi-GPU training
configurations using PyTorch's DistributedDataParallel (DDP) and Automatic
Mixed Precision (AMP). Our results show that U-Net++ consistently delivers
superior denoising performance, achieving competitive Peak Signal to Noise
Ratio (PSNR) and Structured Similarity Index Method (SSIM) scores, though with
less performance in Learned Perceptual Image Patch Similarity (LPIPS) compared
to U-Net under low and moderate noise levels. This indicates U-Net++'s enhanced
structural fidelity and low perceptual similarity. Meanwhile, our optimized
training pipeline reduces training time by over 60% for both models compared to
single-GPU training, and outperforms standard DataParallel by over 40%, with
only a minor accuracy drop for both models (trading some accuracy for speed).
These findings highlight the effectiveness of software-level optimization in
distributed learning for medical imaging. This work demonstrates the practical
viability of combining architectural design, lightweight obfuscation, and
advanced distributed training strategies to accelerate and enhance medical
image processing pipelines in real-world clinical and research environments.
The full implementation is publicly available at:
https://github.com/Suadey/medical-image-denoising-ddp.

</details>


### [47] [One-Shot Knowledge Transfer for Scalable Person Re-Identification](https://arxiv.org/abs/2511.06016)
*Longhua Li,Lei Qi,Xin Geng*

Main category: cs.CV

TL;DR: OSKT是一种一次性的知识转移方法，通过权重链中间载体，将教师模型知识整合后，可根据资源需求直接扩展到目标模型大小，无需重复计算。


<details>
  <summary>Details</summary>
Motivation: 边缘计算中的行人重识别需要为不同资源条件部署多个压缩模型，传统压缩方法需要为每个学生模型单独计算，导致重复繁琐的计算过程。

Method: 提出OSKT（One-Shot Knowledge Transfer）知识继承方法，将教师模型知识整合到权重链中间载体中，当需要特定资源约束的模型时，可直接扩展权重链到目标大小。

Result: OSKT显著超越现有压缩方法性能，具有一次性知识转移的优势，消除了为每个目标模型频繁计算的需求。

Conclusion: OSKT通过权重链实现了灵活高效的模型压缩和部署，为边缘计算环境下的行人重识别提供了解决重复计算问题的有效方案。

Abstract: Edge computing in person re-identification (ReID) is crucial for reducing the
load on central cloud servers and ensuring user privacy. Conventional
compression methods for obtaining compact models require computations for each
individual student model. When multiple models of varying sizes are needed to
accommodate different resource conditions, this leads to repetitive and
cumbersome computations. To address this challenge, we propose a novel
knowledge inheritance approach named OSKT (One-Shot Knowledge Transfer), which
consolidates the knowledge of the teacher model into an intermediate carrier
called a weight chain. When a downstream scenario demands a model that meets
specific resource constraints, this weight chain can be expanded to the target
model size without additional computation. OSKT significantly outperforms
state-of-the-art compression methods, with the added advantage of one-time
knowledge transfer that eliminates the need for frequent computations for each
target model.

</details>


### [48] [Towards Implicit Aggregation: Robust Image Representation for Place Recognition in the Transformer Era](https://arxiv.org/abs/2511.06024)
*Feng Lu,Tong Jin,Canming Ye,Yunpeng Liu,Xiangyuan Lan,Chun Yuan*

Main category: cs.CV

TL;DR: 本文提出了一种在transformer时代无需专门聚合器的视觉地点识别方法，通过插入可学习的聚合token利用自注意力机制隐式生成全局描述符。


<details>
  <summary>Details</summary>
Motivation: 传统的视觉地点识别方法采用backbone+aggregator范式，在CNN时代占主导地位。本文认为在transformer时代，专门设计的聚合器不再是必要的，transformer的自注意力机制本身就具备了全局信息聚合能力。

Method: 提出可学习的聚合token，将其插入到特定transformer block的patch token之前，通过自注意力机制实现隐式信息聚合，最后从输出中提取聚合token并拼接作为全局表示。同时通过实验研究提出了最优token插入策略和初始化方法。

Result: 实验结果表明该方法在多个VPR数据集上优于现有先进方法，具有更高效率，并在MSLS挑战赛排行榜上排名第1。

Conclusion: 验证了在transformer架构中，仅依靠backbone就能实现鲁棒的全局描述符生成，为视觉地点识别提供了更简洁高效的解决方案。

Abstract: Visual place recognition (VPR) is typically regarded as a specific image
retrieval task, whose core lies in representing images as global descriptors.
Over the past decade, dominant VPR methods (e.g., NetVLAD) have followed a
paradigm that first extracts the patch features/tokens of the input image using
a backbone, and then aggregates these patch features into a global descriptor
via an aggregator. This backbone-plus-aggregator paradigm has achieved
overwhelming dominance in the CNN era and remains widely used in
transformer-based models. In this paper, however, we argue that a dedicated
aggregator is not necessary in the transformer era, that is, we can obtain
robust global descriptors only with the backbone. Specifically, we introduce
some learnable aggregation tokens, which are prepended to the patch tokens
before a particular transformer block. All these tokens will be jointly
processed and interact globally via the intrinsic self-attention mechanism,
implicitly aggregating useful information within the patch tokens to the
aggregation tokens. Finally, we only take these aggregation tokens from the
last output tokens and concatenate them as the global representation. Although
implicit aggregation can provide robust global descriptors in an extremely
simple manner, where and how to insert additional tokens, as well as the
initialization of tokens, remains an open issue worthy of further exploration.
To this end, we also propose the optimal token insertion strategy and token
initialization method derived from empirical studies. Experimental results show
that our method outperforms state-of-the-art methods on several VPR datasets
with higher efficiency and ranks 1st on the MSLS challenge leaderboard. The
code is available at https://github.com/lu-feng/image.

</details>


### [49] [Neodragon: Mobile Video Generation using Diffusion Transformer](https://arxiv.org/abs/2511.06055)
*Animesh Karnewar,Denis Korzhenkov,Ioannis Lelekas,Adil Karjauv,Noor Fathima,Hanwen Xiong,Vancheeswaran Vaidyanathan,Will Zeng,Rafael Esteves,Tushar Singhal,Fatih Porikli,Mohsen Ghafoorian,Amirhossein Habibian*

Main category: cs.CV

TL;DR: Neodragon是一个专为移动设备优化的文本到视频生成系统，能在高通Hexagon NPU上6.7秒内生成640x1024分辨率、2秒时长的高质量视频，通过文本编码器蒸馏、非对称解码器蒸馏、模型剪枝和步骤蒸馏四大技术创新实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Transformer的文本到视频生成模型通常需要云端计算资源，无法在移动设备上高效运行。Neodragon旨在解决这一问题，通过专门针对移动硬件进行优化，实现低成本、私密的端侧视频内容生成，使创作者能够无需依赖云服务即可生成高质量视频。

Method: Neodragon通过四个核心技术贡献实现移动端高效视频生成：(1)使用创新的文本编码器蒸馏方法，将4.762B的T5xxl替换为0.2B的DT5；(2)提出非对称解码器蒸馏，用更高效的解码器替代原VAE解码器而不影响生成隐空间；(3)基于重要性对去噪器MMDiT块进行剪枝，并通过两阶段蒸馏恢复性能；(4)使用适配金字塔流匹配的DMD进行步骤蒸馏，降低NFE需求。

Result: Neodragon实现了在移动硬件上的高效运行：640x1024分辨率、2秒时长(49帧@24fps)视频仅需6.7秒生成时间(7FPS)，峰值内存使用3.5GB，完整模型参数4.945B，VBench总分达到81.61，实现了高保真度的移动端视频生成。

Conclusion: Neodragon成功地将文本到视频生成技术带到了移动设备上，通过系统的优化策略在保证视频质量的同时大幅降低了计算资源需求。这项工作使AI视频内容创作变得民主化，为创作者提供了低成本、私密的端侧视频生成解决方案，推动了移动端AI生成内容的发展。

Abstract: We introduce Neodragon, a text-to-video system capable of generating 2s (49
frames @24 fps) videos at the 640x1024 resolution directly on a Qualcomm
Hexagon NPU in a record 6.7s (7 FPS). Differing from existing transformer-based
offline text-to-video generation models, Neodragon is the first to have been
specifically optimised for mobile hardware to achieve efficient and
high-fidelity video synthesis. We achieve this through four key technical
contributions: (1) Replacing the original large 4.762B T5xxl Text-Encoder with
a much smaller 0.2B DT5 (DistilT5) with minimal quality loss, enabled through a
novel Text-Encoder Distillation procedure. (2) Proposing an Asymmetric Decoder
Distillation approach allowing us to replace the native codec-latent-VAE
decoder with a more efficient one, without disturbing the generative
latent-space of the generation pipeline. (3) Pruning of MMDiT blocks within the
denoiser backbone based on their relative importance, with recovery of original
performance through a two-stage distillation process. (4) Reducing the NFE
(Neural Functional Evaluation) requirement of the denoiser by performing step
distillation using DMD adapted for pyramidal flow-matching, thereby
substantially accelerating video generation. When paired with an optimised
SSD1B first-frame image generator and QuickSRNet for 2x super-resolution, our
end-to-end Neodragon system becomes a highly parameter (4.945B full model),
memory (3.5GB peak RAM usage), and runtime (6.7s E2E latency) efficient
mobile-friendly model, while achieving a VBench total score of 81.61. By
enabling low-cost, private, and on-device text-to-video synthesis, Neodragon
democratizes AI-based video content creation, empowering creators to generate
high-quality videos without reliance on cloud services. Code and model will be
made publicly available at our website:
https://qualcomm-ai-research.github.io/neodragon

</details>


### [50] [LoopExpose: An Unsupervised Framework for Arbitrary-Length Exposure Correction](https://arxiv.org/abs/2511.06066)
*Ao Li,Chen Chen,Zhenyu Wang,Tao Huang,Fangfang Wu,Weisheng Dong*

Main category: cs.CV

TL;DR: LoopExpose提出了一种基于伪标签的无监督曝光校正方法，通过嵌套循环优化策略和自强化学习机制，在无需大规模标注数据的情况下实现了高性能的任意长度曝光校正。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习方法在曝光校正任务中依赖大规模标注数据集，但在实际场景中获取这类数据极其困难，限制了方法的实用性。因此需要开发不依赖标注数据的无监督曝光校正方法。

Method: 提出嵌套循环优化框架：上层使用多曝光融合生成的伪标签训练校正模型，下层通过融合过程生成伪标签；引入反馈机制，将校正后的图像反馈到融合过程中优化伪标签，形成自强化学习循环；设计亮度排序损失函数，利用输入序列的相对亮度顺序作为自监督约束。

Result: 在多个基准数据集上的广泛实验表明，LoopExpose在曝光校正和融合性能方面均超越了现有最先进的无监督方法，证明了方法的有效性和优越性。

Conclusion: LoopExpose通过创新的嵌套循环优化和自强化学习机制，成功解决了无监督曝光校正的关键挑战，为实际应用中的图像质量增强提供了有效的解决方案，同时证明了伪标签和自监督约束在该领域的巨大潜力。

Abstract: Exposure correction is essential for enhancing image quality under
challenging lighting conditions. While supervised learning has achieved
significant progress in this area, it relies heavily on large-scale labeled
datasets, which are difficult to obtain in practical scenarios. To address this
limitation, we propose a pseudo label-based unsupervised method called
LoopExpose for arbitrary-length exposure correction. A nested loop optimization
strategy is proposed to address the exposure correction problem, where the
correction model and pseudo-supervised information are jointly optimized in a
two-level framework. Specifically, the upper-level trains a correction model
using pseudo-labels generated through multi-exposure fusion at the lower level.
A feedback mechanism is introduced where corrected images are fed back into the
fusion process to refine the pseudo-labels, creating a self-reinforcing
learning loop. Considering the dominant role of luminance calibration in
exposure correction, a Luminance Ranking Loss is introduced to leverage the
relative luminance ordering across the input sequence as a self-supervised
constraint. Extensive experiments on different benchmark datasets demonstrate
that LoopExpose achieves superior exposure correction and fusion performance,
outperforming existing state-of-the-art unsupervised methods. Code is available
at https://github.com/FALALAS/LoopExpose.

</details>


### [51] [An Artificial Intelligence-based Assistant for the Visually Impaired](https://arxiv.org/abs/2511.06080)
*Luis Marquez-Carpintero,Francisco Gomez-Donoso,Zuria Bauer,Bessie Dominguez-Dager,Alvaro Belmonte-Baeza,Mónica Pina-Navarro,Francisco Morillas-Espejo,Felix Escalona,Miguel Cazorla*

Main category: cs.CV

TL;DR: AIDEN是一款基于人工智能的助手应用，通过YOLO架构和大型语言视觉助手技术，为视障人士提供物体识别、文本阅读和环境导航等功能，旨在提高其生活质量和独立性。


<details>
  <summary>Details</summary>
Motivation: 视障人士在物体识别、文本阅读和陌生环境导航方面面临挑战，现有的盲文、有声书和屏幕阅读器等解决方案并非在所有情况下都有效，限制了他们的独立性和生活质量。

Method: 应用采用了先进的机器学习算法，具体使用YOLO（You Only Look Once）架构进行物体识别，结合大型语言视觉助手（LLaVA）系统，实现了多种交互方法，帮助用户以适当方式获取文本和视觉信息。

Result: 系统通过物体识别、文本阅读和环境问答功能，显著提升了用户的自主性和信息获取能力，用户反馈支持了在日常可用性方面的改善。

Conclusion: AIDEN应用成功展示了AI技术在改善视障人士生活质量方面的潜力，通过集成先进的计算机视觉和自然语言处理技术，为视障群体提供了更有效的辅助解决方案，增强了他们的独立性和信息访问能力。

Abstract: This paper describes an artificial intelligence-based assistant application,
AIDEN, developed during 2023 and 2024, aimed at improving the quality of life
for visually impaired individuals. Visually impaired individuals face
challenges in identifying objects, reading text, and navigating unfamiliar
environments, which can limit their independence and reduce their quality of
life. Although solutions such as Braille, audio books, and screen readers
exist, they may not be effective in all situations. This application leverages
state-of-the-art machine learning algorithms to identify and describe objects,
read text, and answer questions about the environment. Specifically, it uses
You Only Look Once architectures and a Large Language and Vision Assistant. The
system incorporates several methods to facilitate the user's interaction with
the system and access to textual and visual information in an appropriate
manner. AIDEN aims to enhance user autonomy and access to information,
contributing to an improved perception of daily usability, as supported by user
feedback.

</details>


### [52] [Hybrid CNN-ViT Framework for Motion-Blurred Scene Text Restoration](https://arxiv.org/abs/2511.06087)
*Umar Rashid,Muhammad Arslan Arshad,Ghulam Ahmad,Muhammad Zeeshan Anjum,Rizwan Khan,Muhammad Akmal*

Main category: cs.CV

TL;DR: 本文提出了一种结合CNN和Vision Transformer的混合深度学习框架，用于处理场景文本图像的运动模糊问题，在PSNR 32.20dB和SSIM 0.934的指标下实现了高效的去模糊效果。


<details>
  <summary>Details</summary>
Motivation: 场景文本图像中的运动模糊严重影响可读性，阻碍了计算机视觉任务的可靠性，传统去模糊方法在处理空间变化模糊和建模长距离依赖方面存在不足。

Method: 采用CNN编码器-解码器保留结构细节，结合transformer模块通过自注意力增强全局感知，在TextOCR数据集上使用合成运动模糊图像进行训练，采用包含MAE、MSE、感知相似性和SSIM的复合损失函数进行优化。

Result: 模型达到32.20 dB PSNR和0.934 SSIM，仅含283万参数，平均推理时间61ms，证明了方法的有效性和计算效率。

Conclusion: CNN-ViT混合设计在运动模糊场景文本恢复方面表现出色，兼具高性能和实用性，适合实际应用场景。

Abstract: Motion blur in scene text images severely impairs readability and hinders the
reliability of computer vision tasks, including autonomous driving, document
digitization, and visual information retrieval. Conventional deblurring
approaches are often inadequate in handling spatially varying blur and
typically fall short in modeling the long-range dependencies necessary for
restoring textual clarity. To overcome these limitations, we introduce a hybrid
deep learning framework that combines convolutional neural networks (CNNs) with
vision transformers (ViTs), thereby leveraging both local feature extraction
and global contextual reasoning. The architecture employs a CNN-based
encoder-decoder to preserve structural details, while a transformer module
enhances global awareness through self-attention. Training is conducted on a
curated dataset derived from TextOCR, where sharp scene-text samples are paired
with synthetically blurred versions generated using realistic motion-blur
kernels of multiple sizes and orientations. Model optimization is guided by a
composite loss that incorporates mean absolute error (MAE), squared error
(MSE), perceptual similarity, and structural similarity (SSIM). Quantitative
eval- uations show that the proposed method attains 32.20 dB in PSNR and 0.934
in SSIM, while remaining lightweight with 2.83 million parameters and an
average inference time of 61 ms. These results highlight the effectiveness and
computational efficiency of the CNN-ViT hybrid design, establishing its
practicality for real-world motion-blurred scene-text restoration.

</details>


### [53] [Latent Refinement via Flow Matching for Training-free Linear Inverse Problem Solving](https://arxiv.org/abs/2511.06138)
*Hossein Askari,Yadan Luo,Hongfu Sun,Fred Roosta*

Main category: cs.CV

TL;DR: LFlow是一个训练免费的框架，通过预训练的潜在流先验解决线性逆问题，利用潜在空间ODE采样和理论引导的后验协方差，在重建质量上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于流的逆求解器存在两个主要问题：(i)直接在像素空间操作导致计算资源需求大，难以扩展到高分辨率图像；(ii)使用与先验无关的后验协方差引导策略，削弱了与生成轨迹的对齐性并降低后验覆盖率。

Method: 提出LFlow框架：1)利用流匹配在潜在空间沿最优路径进行ODE采样，避免像素空间的计算负担；2)从最优向量场推导理论上的后验协方差，实现有效的流引导；3)整个框架是训练免费的，基于预训练的潜在流先验。

Result: 实验结果表明，LFlow在大多数任务的重建质量上超越了最先进的潜在扩散求解器，验证了方法的有效性和优越性。

Conclusion: LFlow成功解决了现有流基逆求解器的计算效率和后验对齐问题，通过潜在空间操作和理论引导策略，为线性逆问题提供了更高效的解决方案。

Abstract: Recent advances in inverse problem solving have increasingly adopted flow
priors over diffusion models due to their ability to construct straight
probability paths from noise to data, thereby enhancing efficiency in both
training and inference. However, current flow-based inverse solvers face two
primary limitations: (i) they operate directly in pixel space, which demands
heavy computational resources for training and restricts scalability to
high-resolution images, and (ii) they employ guidance strategies with
prior-agnostic posterior covariances, which can weaken alignment with the
generative trajectory and degrade posterior coverage. In this paper, we propose
LFlow (Latent Refinement via Flows), a training-free framework for solving
linear inverse problems via pretrained latent flow priors. LFlow leverages the
efficiency of flow matching to perform ODE sampling in latent space along an
optimal path. This latent formulation further allows us to introduce a
theoretically grounded posterior covariance, derived from the optimal vector
field, enabling effective flow guidance. Experimental results demonstrate that
our proposed method outperforms state-of-the-art latent diffusion solvers in
reconstruction quality across most tasks. The code will be publicly available
at https://github.com/hosseinaskari-cs/LFlow .

</details>


### [54] [MoRA: Missing Modality Low-Rank Adaptation for Visual Recognition](https://arxiv.org/abs/2511.06225)
*Shu Zhao,Nilesh Ahuja,Tan Yu,Tianyi Shen,Vijaykrishnan Narayanan*

Main category: cs.CV

TL;DR: 本文提出MoRA，一种参数高效的视觉语言模型微调方法，通过显式建模跨模态交互并保持模态特定适配，有效解决缺失模态场景下的视觉识别问题。


<details>
  <summary>Details</summary>
Motivation: 预训练的视觉语言模型通常假设训练和推理时都有完整的多模态输入，但现实场景中由于隐私限制、收集困难或资源限制，模态可能缺失。现有基于提示学习的方法无法有效捕获必要的跨模态关系，且存在不可避免的计算开销。

Method: MoRA在文本和视觉编码器之间引入模态公共参数，实现双向知识转移；结合模态特定参数，使骨干模型能够保持模态间交互并实现模态内灵活性。

Result: 在标准基准测试中，MoRA在缺失模态场景下平均性能提升5.24%，推理时间仅为现有最优方法的25.90%，相比全参数微调仅需0.11%的可训练参数。

Conclusion: MoRA为处理视觉语言模型中的缺失模态问题提供了有效且高效的解决方案，在获得更好性能的同时显著降低了计算成本和参数需求。

Abstract: Pre-trained vision language models have shown remarkable performance on
visual recognition tasks, but they typically assume the availability of
complete multimodal inputs during both training and inference. In real-world
scenarios, however, modalities may be missing due to privacy constraints,
collection difficulties, or resource limitations. While previous approaches
have addressed this challenge using prompt learning techniques, they fail to
capture the cross-modal relationships necessary for effective multimodal visual
recognition and suffer from inevitable computational overhead. In this paper,
we introduce MoRA, a parameter-efficient fine-tuning method that explicitly
models cross-modal interactions while maintaining modality-specific
adaptations. MoRA introduces modality-common parameters between text and vision
encoders, enabling bidirectional knowledge transfer. Additionally, combined
with the modality-specific parameters, MoRA allows the backbone model to
maintain inter-modality interaction and enable intra-modality flexibility.
Extensive experiments on standard benchmarks demonstrate that MoRA achieves an
average performance improvement in missing-modality scenarios by 5.24% and uses
only 25.90% of the inference time compared to the SOTA method while requiring
only 0.11% of trainable parameters compared to full fine-tuning.

</details>


### [55] [Temporal-Guided Visual Foundation Models for Event-Based Vision](https://arxiv.org/abs/2511.06238)
*Ruihao Xia,Junhong Cai,Luziwei Leng,Liuyi Wang,Chengju Liu,Ran Cheng,Yang Tang,Pan Zhou*

Main category: cs.CV

TL;DR: TGVFM框架通过引入时序上下文融合模块，将视觉基础模型应用于事件相机视觉任务，在语义分割、深度估计和目标检测上分别实现了16%、21%和16%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有事件相机处理方法依赖专用架构或资源密集型训练，而基于图像预训练的现代视觉基础模型在事件视觉领域的潜力尚未充分挖掘。

Method: 提出时序引导VFM框架，集成三个关键组件：长程时序注意力建模全局时序依赖、双重时空注意力处理多尺度帧相关性、深度特征引导机制融合语义-时序特征，并通过重新训练事件到视频模型来保持时空动态。

Result: 在语义分割、深度估计和目标检测任务上达到业界最先进性能，相比现有方法分别提升16%、21%和16%。

Conclusion: 该工作解锁了基于图像的视觉基础模型在事件视觉中的跨模态潜力，为时序推理提供了新的解决方案。

Abstract: Event cameras offer unique advantages for vision tasks in challenging
environments, yet processing asynchronous event streams remains an open
challenge. While existing methods rely on specialized architectures or
resource-intensive training, the potential of leveraging modern Visual
Foundation Models (VFMs) pretrained on image data remains under-explored for
event-based vision. To address this, we propose Temporal-Guided VFM (TGVFM), a
novel framework that integrates VFMs with our temporal context fusion block
seamlessly to bridge this gap. Our temporal block introduces three key
components: (1) Long-Range Temporal Attention to model global temporal
dependencies, (2) Dual Spatiotemporal Attention for multi-scale frame
correlation, and (3) Deep Feature Guidance Mechanism to fuse semantic-temporal
features. By retraining event-to-video models on real-world data and leveraging
transformer-based VFMs, TGVFM preserves spatiotemporal dynamics while
harnessing pretrained representations. Experiments demonstrate SoTA performance
across semantic segmentation, depth estimation, and object detection, with
improvements of 16%, 21%, and 16% over existing methods, respectively. Overall,
this work unlocks the cross-modality potential of image-based VFMs for
event-based vision with temporal reasoning. Code is available at
https://github.com/XiaRho/TGVFM.

</details>


### [56] [Gait Recognition via Collaborating Discriminative and Generative Diffusion Models](https://arxiv.org/abs/2511.06245)
*Haijun Xiong,Bin Feng,Bang Wang,Xinggang Wang,Wenyu Liu*

Main category: cs.CV

TL;DR: CoD$^2$是一种结合扩散模型和判别模型的步态识别框架，通过多级条件控制策略实现最先进的识别性能。


<details>
  <summary>Details</summary>
Motivation: 尽管判别模型在步态识别领域取得了显著成功，但生成模型的潜力仍未得到充分探索。现有方法存在改进空间，需要结合生成和判别方法的优势来提取更鲁棒的步态特征。

Method: 提出CoD$^2$框架，结合扩散模型的数据分布建模能力和判别模型的语义表征学习能力。采用多级条件控制策略：高级身份感知语义条件指导生成身份一致的步态序列，低级视觉细节保持外观和运动的一致性。生成的序列反过来促进判别提取器的学习，使其捕获更全面的高级语义特征。

Result: 在四个数据集（SUSTech1K、CCPG、GREW和Gait3D）上的大量实验表明，CoD$^2$实现了最先进的性能，并且可以与现有判别方法无缝集成，带来一致的改进效果。

Conclusion: CoD$^2$成功桥接了生成模型和判别模型在步态识别领域的优势，通过协同学习机制提升了特征提取的鲁棒性，为步态识别技术的发展提供了新的思路和有效解决方案。

Abstract: Gait recognition offers a non-intrusive biometric solution by identifying
individuals through their walking patterns. Although discriminative models have
achieved notable success in this domain, the full potential of generative
models remains largely underexplored. In this paper, we introduce
\textbf{CoD$^2$}, a novel framework that combines the data distribution
modeling capabilities of diffusion models with the semantic representation
learning strengths of discriminative models to extract robust gait features. We
propose a Multi-level Conditional Control strategy that incorporates both
high-level identity-aware semantic conditions and low-level visual details.
Specifically, the high-level condition, extracted by the discriminative
extractor, guides the generation of identity-consistent gait sequences, whereas
low-level visual details, such as appearance and motion, are preserved to
enhance consistency. Furthermore, the generated sequences facilitate the
discriminative extractor's learning, enabling it to capture more comprehensive
high-level semantic features. Extensive experiments on four datasets
(SUSTech1K, CCPG, GREW, and Gait3D) demonstrate that CoD$^2$ achieves
state-of-the-art performance and can be seamlessly integrated with existing
discriminative methods, yielding consistent improvements.

</details>


### [57] [AdaDrive: Self-Adaptive Slow-Fast System for Language-Grounded Autonomous Driving](https://arxiv.org/abs/2511.06253)
*Ruifei Zhang,Junlin Xie,Wei Zhang,Weikai Chen,Xiao Tan,Xiang Wan,Guanbin Li*

Main category: cs.CV

TL;DR: AdaDrive提出自适应协作框架，通过智能激活LLM和连续融合策略，在保持实时效率的同时提升自动驾驶决策准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM自动驾驶方法存在频率过高导致计算开销大，或固定调度无法适应动态环境的问题，需要在高级推理和实时效率间找到平衡。

Method: 提出AdaDrive框架：(1)使用自适应激活损失基于比较学习动态决定何时调用LLM；(2)引入自适应融合策略，根据场景复杂度和置信度连续调节LLM影响程度。

Result: 在语言基础自动驾驶基准测试中达到最先进性能，在驾驶准确性和计算效率两方面都表现优异。

Conclusion: AdaDrive提供了灵活上下文感知框架，通过智能激活和融合策略，在不损害实时性能前提下最大化决策准确性，代码已开源。

Abstract: Effectively integrating Large Language Models (LLMs) into autonomous driving
requires a balance between leveraging high-level reasoning and maintaining
real-time efficiency. Existing approaches either activate LLMs too frequently,
causing excessive computational overhead, or use fixed schedules, failing to
adapt to dynamic driving conditions. To address these challenges, we propose
AdaDrive, an adaptively collaborative slow-fast framework that optimally
determines when and how LLMs contribute to decision-making. (1) When to
activate the LLM: AdaDrive employs a novel adaptive activation loss that
dynamically determines LLM invocation based on a comparative learning
mechanism, ensuring activation only in complex or critical scenarios. (2) How
to integrate LLM assistance: Instead of rigid binary activation, AdaDrive
introduces an adaptive fusion strategy that modulates a continuous, scaled LLM
influence based on scene complexity and prediction confidence, ensuring
seamless collaboration with conventional planners. Through these strategies,
AdaDrive provides a flexible, context-aware framework that maximizes decision
accuracy without compromising real-time performance. Extensive experiments on
language-grounded autonomous driving benchmarks demonstrate that AdaDrive
state-of-the-art performance in terms of both driving accuracy and
computational efficiency. Code is available at
https://github.com/ReaFly/AdaDrive.

</details>


### [58] [VLDrive: Vision-Augmented Lightweight MLLMs for Efficient Language-grounded Autonomous Driving](https://arxiv.org/abs/2511.06256)
*Ruifei Zhang,Wei Zhang,Xiao Tan,Sibei Yang,Xiang Wan,Xiaonan Luo,Guanbin Li*

Main category: cs.CV

TL;DR: VLDrive是一种轻量级多模态大语言模型，用于自动驾驶，在参数减少81%的情况下实现了更好的驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的自动驾驶方法面临两大关键挑战：(1)视觉表征限制导致的频繁碰撞和障碍问题；(2)LLM的大量参数带来的部署困难。

Method: VLDrive采用轻量级MLLM架构，结合创新的循环一致性动态视觉剪枝和内存增强特征聚合技术来生成紧凑的视觉令牌，并提出距离解耦指令注意力机制来改善视觉-语言特征联合学习。

Result: 在CARLA仿真器中，VLDrive将参数从7B减少到1.3B（减少81%），在微小、短距和长距离上的驾驶分数分别提升15.4%、16.8%和7.6%，实现了最先进的驾驶性能。

Conclusion: VLDrive有效解决了基于LLM的自动驾驶中的关键挑战，提供了更高效、更鲁棒的解决方案，为语言驱动的自动驾驶系统的实际部署铺平了道路。

Abstract: Recent advancements in language-grounded autonomous driving have been
significantly promoted by the sophisticated cognition and reasoning
capabilities of large language models (LLMs). However, current LLM-based
approaches encounter critical challenges: (1) Failure analysis reveals that
frequent collisions and obstructions, stemming from limitations in visual
representations, remain primary obstacles to robust driving performance. (2)
The substantial parameters of LLMs pose considerable deployment hurdles. To
address these limitations, we introduce VLDrive, a novel approach featuring a
lightweight MLLM architecture with enhanced vision components. VLDrive achieves
compact visual tokens through innovative strategies, including cycle-consistent
dynamic visual pruning and memory-enhanced feature aggregation. Furthermore, we
propose a distance-decoupled instruction attention mechanism to improve joint
visual-linguistic feature learning, particularly for long-range visual tokens.
Extensive experiments conducted in the CARLA simulator demonstrate VLDrive`s
effectiveness. Notably, VLDrive achieves state-of-the-art driving performance
while reducing parameters by 81% (from 7B to 1.3B), yielding substantial
driving score improvements of 15.4%, 16.8%, and 7.6% at tiny, short, and long
distances, respectively, in closed-loop evaluations. Code is available at
https://github.com/ReaFly/VLDrive.

</details>


### [59] [A Mixture-of-Experts Framework with Log-Logistic Components for Survival Analysis on Histopathology Images](https://arxiv.org/abs/2511.06266)
*Ardhendu Sekhar,Vasu Soni,Keshav Aske,Shivam Madnoorkar,Pranav Jeevan,Amit Sethi*

Main category: cs.CV

TL;DR: 提出一个模块化框架，通过整合四个关键组件从全载玻片病理图像预测癌症特异性生存期。


<details>
  <summary>Details</summary>
Motivation: 为了从全载玻片病理图像(WSIs)中准确预测癌症特异性生存期，这是一个重要的临床任务，但现有方法存在局限性。

Method: 框架包含四个组件：(i)基于分位数的门控补片选择，通过阈值分离预后信息丰富的组织区域；(ii)图引导聚类，使用k近邻图通过空间和形态学一致性捕获表型水平异质性；(iii)分层上下文注意力，学习簇内和簇间相互作用；(iv)专家驱动的对数逻辑混合框架，使用对数逻辑分布估计复杂生存分布。

Result: 在TCGA数据集上取得优异表现：LUAD数据集一致性指数0.644，KIRC数据集0.751，BRCA数据集0.752，均超过现有最先进方法。

Conclusion: 该模块化框架能有效整合多个处理步骤，显著提升了癌症生存预测的准确性，为临床预后评估提供了新的技术手段。

Abstract: We propose a modular framework for predicting cancer specific survival from
whole slide pathology images (WSIs). The method integrates four components: (i)
Quantile Gated Patch Selection via quantile based thresholding to isolate
prognostically informative tissue regions; (ii) Graph Guided Clustering using a
k nearest neighbor graph to capture phenotype level heterogeneity through
spatial and morphological coherence; (iii) Hierarchical Context Attention to
learn intra and inter cluster interactions; and (iv) an Expert Driven Mixture
of Log logistics framework to estimate complex survival distributions using Log
logistics distributions. The model attains a concordance index of 0.644 on TCGA
LUAD, 0.751 on TCGA KIRC, and 0.752 on TCGA BRCA respectively, outperforming
existing state of the art approaches.

</details>


### [60] [LLM-Driven Completeness and Consistency Evaluation for Cultural Heritage Data Augmentation in Cross-Modal Retrieval](https://arxiv.org/abs/2511.06268)
*Jian Zhang,Junyi Guo,Junyi Yuan,Huanda Lu,Yanlin Zhou,Fangyu Wu,Qiufeng Wang,Dongming Lu*

Main category: cs.CV

TL;DR: C³是一个数据增强框架，通过提升大语言模型生成文本描述的完整性和一致性来改善跨模态检索性能，在文化遗产和通用数据集上都达到了最先进水平。


<details>
  <summary>Details</summary>
Motivation: 跨模态检索对解读文化遗产数据至关重要，但由于历史数据丢失和专家标注成本高，文本描述往往不完整或不一致。虽然大语言模型可以通过丰富文本描述提供解决方案，但其输出经常出现幻觉或缺乏视觉细节支撑。

Method: C³框架引入完整性评估模块，使用视觉线索和语言模型输出来评估语义覆盖度；通过构建马尔可夫决策过程监督思维链推理，通过自适应查询控制引导一致性评估，从而减少事实不一致性。

Result: 在文化遗产数据集CulTi和TimeTravel以及通用基准数据集MSCOCO和Flickr30K上的实验表明，C³在微调和零样本设置下都实现了最先进的性能表现。

Conclusion: C³框架有效解决了大语言模型生成文本描述中的完整性和一致性问题，显著提升了跨模态检索性能，在文化遗产保护和通用应用领域都具有重要价值。

Abstract: Cross-modal retrieval is essential for interpreting cultural heritage data,
but its effectiveness is often limited by incomplete or inconsistent textual
descriptions, caused by historical data loss and the high cost of expert
annotation. While large language models (LLMs) offer a promising solution by
enriching textual descriptions, their outputs frequently suffer from
hallucinations or miss visually grounded details. To address these challenges,
we propose $C^3$, a data augmentation framework that enhances cross-modal
retrieval performance by improving the completeness and consistency of
LLM-generated descriptions. $C^3$ introduces a completeness evaluation module
to assess semantic coverage using both visual cues and language-model outputs.
Furthermore, to mitigate factual inconsistencies, we formulate a Markov
Decision Process to supervise Chain-of-Thought reasoning, guiding consistency
evaluation through adaptive query control. Experiments on the cultural heritage
datasets CulTi and TimeTravel, as well as on general benchmarks MSCOCO and
Flickr30K, demonstrate that $C^3$ achieves state-of-the-art performance in both
fine-tuned and zero-shot settings.

</details>


### [61] [RelightMaster: Precise Video Relighting with Multi-plane Light Images](https://arxiv.org/abs/2511.06271)
*Weikang Bian,Xiaoyu Shi,Zhaoyang Huang,Jianhong Bai,Qinghe Wang,Xintao Wang,Pengfei Wan,Kun Gai,Hongsheng Li*

Main category: cs.CV

TL;DR: RelightMaster是一个用于精确可控视频重新照明的框架，通过构建新数据集、提出多平面光照图像(MPLI)视觉提示和光照图像适配器，实现了在保持原始场景内容的同时生成物理合理的光照和阴影效果。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在视频生成和编辑方面表现出色，但在精确的重新照明控制方面仍存在不足。主流文本到视频模型由于文本在描述光照细节方面的固有局限性以及光照相关提示的预训练数据不足，缺乏细粒度的光照控制能力。此外，构建高质量的重新照明训练数据具有挑战性，因为现实世界中可控光照数据稀缺。

Method: 提出RelightMaster框架，包含三个核心组件：1) 基于虚幻引擎构建RelightVideo数据集，包含相同动态内容在不同精确光照条件下的样本；2) 引入多平面光照图像(MPLI)，通过K个深度对齐平面建模3D光源位置、强度和颜色，支持多光源场景并泛化到未见过的光照设置；3) 设计光照图像适配器，通过预训练视频VAE压缩MPLI并将潜在光照特征注入到DiT块中，利用基础模型的生成先验避免灾难性遗忘。

Result: 实验表明RelightMaster能够生成物理合理的光照和阴影效果，同时保持原始场景内容不变，实现了精确可控的视频重新照明。

Conclusion: RelightMaster通过创新的数据集构建、视觉提示设计和适配器架构，成功解决了视频重新照明中的精确控制难题，为塑造场景氛围和引导观众注意力提供了有效工具。

Abstract: Recent advances in diffusion models enable high-quality video generation and
editing, but precise relighting with consistent video contents, which is
critical for shaping scene atmosphere and viewer attention, remains unexplored.
Mainstream text-to-video (T2V) models lack fine-grained lighting control due to
text's inherent limitation in describing lighting details and insufficient
pre-training on lighting-related prompts. Additionally, constructing
high-quality relighting training data is challenging, as real-world
controllable lighting data is scarce. To address these issues, we propose
RelightMaster, a novel framework for accurate and controllable video
relighting. First, we build RelightVideo, the first dataset with identical
dynamic content under varying precise lighting conditions based on the Unreal
Engine. Then, we introduce Multi-plane Light Image (MPLI), a novel visual
prompt inspired by Multi-Plane Image (MPI). MPLI models lighting via K
depth-aligned planes, representing 3D light source positions, intensities, and
colors while supporting multi-source scenarios and generalizing to unseen light
setups. Third, we design a Light Image Adapter that seamlessly injects MPLI
into pre-trained Video Diffusion Transformers (DiT): it compresses MPLI via a
pre-trained Video VAE and injects latent light features into DiT blocks,
leveraging the base model's generative prior without catastrophic forgetting.
Experiments show that RelightMaster generates physically plausible lighting and
shadows and preserves original scene content. Demos are available at
https://wkbian.github.io/Projects/RelightMaster/.

</details>


### [62] [LaneDiffusion: Improving Centerline Graph Learning via Prior Injected BEV Feature Generation](https://arxiv.org/abs/2511.06272)
*Zijie Wang,Weiming Zhang,Wei Zhang,Xiao Tan,Hongxing Liu,Yaowei Wang,Guanbin Li*

Main category: cs.CV

TL;DR: LaneDiffusion是一种使用扩散模型的生成式中心线图学习方法，在BEV特征级别生成车道中心线先验，显著提升了自动驾驶路径规划的性能。


<details>
  <summary>Details</summary>
Motivation: 传统确定性方法学习中心线图缺乏空间推理能力，难以处理遮挡或不可见的中心线，而生成式方法在该领域尚未得到充分探索。

Method: 提出LaneDiffusion生成式框架，在鸟瞰图(BEV)特征级别使用扩散模型生成车道中心线先验，而非直接预测向量化中心线。包含Lane Prior Injection Module (LPIM)和Lane Prior Diffusion Module (LPDM)两个关键模块，从先验注入的BEV特征中解码向量化中心线和拓扑结构。

Result: 在nuScenes和Argoverse2数据集上显著超越现有方法：点级指标(GEO F1、TOPO F1、JTOPO F1、APLS、SDA)提升4.2%-6.4%，段级指标(IoU、mAP_cf、DET_l、TOP_ll)提升2.1%-6.8%。

Conclusion: LaneDiffusion在中心线图学习任务中建立了最先进的性能，为该任务的生成式模型应用提供了新的见解，证明了扩散模型在自动驾驶路径规划中的有效性。

Abstract: Centerline graphs, crucial for path planning in autonomous driving, are
traditionally learned using deterministic methods. However, these methods often
lack spatial reasoning and struggle with occluded or invisible centerlines.
Generative approaches, despite their potential, remain underexplored in this
domain. We introduce LaneDiffusion, a novel generative paradigm for centerline
graph learning. LaneDiffusion innovatively employs diffusion models to generate
lane centerline priors at the Bird's Eye View (BEV) feature level, instead of
directly predicting vectorized centerlines. Our method integrates a Lane Prior
Injection Module (LPIM) and a Lane Prior Diffusion Module (LPDM) to effectively
construct diffusion targets and manage the diffusion process. Furthermore,
vectorized centerlines and topologies are then decoded from these
prior-injected BEV features. Extensive evaluations on the nuScenes and
Argoverse2 datasets demonstrate that LaneDiffusion significantly outperforms
existing methods, achieving improvements of 4.2%, 4.6%, 4.7%, 6.4% and 1.8% on
fine-grained point-level metrics (GEO F1, TOPO F1, JTOPO F1, APLS and SDA) and
2.3%, 6.4%, 6.8% and 2.1% on segment-level metrics (IoU, mAP_cf, DET_l and
TOP_ll). These results establish state-of-the-art performance in centerline
graph learning, offering new insights into generative models for this task.

</details>


### [63] [VideoSSR: Video Self-Supervised Reinforcement Learning](https://arxiv.org/abs/2511.06281)
*Zefeng He,Xiaoye Qu,Yafu Li,Siyuan Huang,Daizong Liu,Yu Cheng*

Main category: cs.CV

TL;DR: 该论文提出了VideoSSR框架，通过自监督 pretext 任务利用视频内在信息自动生成高质量训练数据，显著提升了多模态大语言模型的视频理解能力，在17个基准测试中平均提升超过5%。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型(MLLMs)的视频理解能力发展迅速，但现有视频数据集的复杂性已跟不上模型发展步伐，而手工标注新的高质量数据成本过高，因此探索如何利用视频内在信息自生成高质量训练数据。

Method: 引入三个自监督 pretext 任务：异常定位、对象计数和时间拼图；构建视频内在理解基准(VIUBench)验证任务难度；开发VideoSSR-30K数据集和VideoSSR视频自监督强化学习框架。

Result: 在17个基准测试的4大视频领域(通用视频QA、长视频QA、时间定位和复杂推理)上的广泛实验表明，VideoSSR持续提升模型性能，平均改进超过5%。

Conclusion: VideoSSR被确立为开发MLLMs更高级视频理解的强大基础框架，证明了利用视频内在信息自生成训练数据的有效性和潜力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has substantially
advanced the video understanding capabilities of Multimodal Large Language
Models (MLLMs). However, the rapid progress of MLLMs is outpacing the
complexity of existing video datasets, while the manual annotation of new,
high-quality data remains prohibitively expensive. This work investigates a
pivotal question: Can the rich, intrinsic information within videos be
harnessed to self-generate high-quality, verifiable training data? To
investigate this, we introduce three self-supervised pretext tasks: Anomaly
Grounding, Object Counting, and Temporal Jigsaw. We construct the Video
Intrinsic Understanding Benchmark (VIUBench) to validate their difficulty,
revealing that current state-of-the-art MLLMs struggle significantly on these
tasks. Building upon these pretext tasks, we develop the VideoSSR-30K dataset
and propose VideoSSR, a novel video self-supervised reinforcement learning
framework for RLVR. Extensive experiments across 17 benchmarks, spanning four
major video domains (General Video QA, Long Video QA, Temporal Grounding, and
Complex Reasoning), demonstrate that VideoSSR consistently enhances model
performance, yielding an average improvement of over 5\%. These results
establish VideoSSR as a potent foundational framework for developing more
advanced video understanding in MLLMs. The code is available at
https://github.com/lcqysl/VideoSSR.

</details>


### [64] [From ACR O-RADS 2022 to Explainable Deep Learning: Comparative Performance of Expert Radiologists, Convolutional Neural Networks, Vision Transformers, and Fusion Models in Ovarian Masses](https://arxiv.org/abs/2511.06282)
*Ali Abbasian Ardakani,Afshin Mohammadi,Alisa Mohebbi,Anushya Vijayananthan,Sook Sam Leong,Lim Yi Ting,Mohd Kamil Bin Mohamad Fabell,U Rajendra Acharya,Sepideh Hatamikia*

Main category: cs.CV

TL;DR: 本研究比较了放射科医生使用O-RADS v2022标准与深度学习模型在卵巢附件病变诊断中的表现，发现AI模型（尤其是ViT）显著优于人工评估，人机结合模式能进一步提升诊断准确性。


<details>
  <summary>Details</summary>
Motivation: O-RADS 2022超声分类系统虽然改进了卵巢附件病变风险分层，但人工解读存在变异性和保守阈值问题。同时，深度学习在卵巢病变影像特征分析方面展现出潜力，需要系统评估AI与人工诊断的性能差异。

Method: 单中心回顾性研究，纳入227名患者的512个卵巢附件肿物图像。训练验证16种深度学习模型（DenseNets、EfficientNets、ResNets、VGGs、Xception和ViTs），并构建放射科医生O-RADS评分与DL预测概率的混合模型进行性能对比。

Result: 放射科医生O-RADS评估AUC为0.683，准确率68.0%；CNN模型AUC范围0.620-0.908，准确率59.2%-86.4%；ViT16-384表现最佳，AUC为0.941，准确率87.4%。人机结合显著提升CNN模型性能，但对ViT模型改进无统计学意义。

Conclusion: 深度学习模型明显超越单纯O-RADS v2022人工评估，专家评分与AI的整合获得最高诊断准确性和判别能力。人机混合模式具有标准化盆腔超声解读、减少假阳性、提高高风险病变检出率的巨大潜力。

Abstract: Background: The 2022 update of the Ovarian-Adnexal Reporting and Data System
(O-RADS) ultrasound classification refines risk stratification for adnexal
lesions, yet human interpretation remains subject to variability and
conservative thresholds. Concurrently, deep learning (DL) models have
demonstrated promise in image-based ovarian lesion characterization. This study
evaluates radiologist performance applying O-RADS v2022, compares it to leading
convolutional neural network (CNN) and Vision Transformer (ViT) models, and
investigates the diagnostic gains achieved by hybrid human-AI frameworks.
Methods: In this single-center, retrospective cohort study, a total of 512
adnexal mass images from 227 patients (110 with at least one malignant cyst)
were included. Sixteen DL models, including DenseNets, EfficientNets, ResNets,
VGGs, Xception, and ViTs, were trained and validated. A hybrid model
integrating radiologist O-RADS scores with DL-predicted probabilities was also
built for each scheme. Results: Radiologist-only O-RADS assessment achieved an
AUC of 0.683 and an overall accuracy of 68.0%. CNN models yielded AUCs of 0.620
to 0.908 and accuracies of 59.2% to 86.4%, while ViT16-384 reached the best
performance, with an AUC of 0.941 and an accuracy of 87.4%. Hybrid human-AI
frameworks further significantly enhanced the performance of CNN models;
however, the improvement for ViT models was not statistically significant
(P-value >0.05). Conclusions: DL models markedly outperform radiologist-only
O-RADS v2022 assessment, and the integration of expert scores with AI yields
the highest diagnostic accuracy and discrimination. Hybrid human-AI paradigms
hold substantial potential to standardize pelvic ultrasound interpretation,
reduce false positives, and improve detection of high-risk lesions.

</details>


### [65] [TinyChemVL: Advancing Chemical Vision-Language Models via Efficient Visual Token Reduction and Complex Reaction Tasks](https://arxiv.org/abs/2511.06283)
*Xuanle Zhao,Shuxin Zeng,Yinyuan Cai,Xiang Cheng,Duzhen Zhang,Xiuyi Chen,Bo Xu*

Main category: cs.CV

TL;DR: TinyChemVL是一个高效且强大的化学视觉语言模型，通过视觉令牌缩减和反应级任务提升了模型效率和推理能力，仅用4B参数就在分子和反应任务上实现优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在化学领域应用受限，当前方法存在计算效率低(处理带非信息背景的完整化学图像)和任务范围窄(仅限分子级任务)两个主要问题。

Method: 提出TinyChemVL模型，采用视觉令牌缩减技术和反应级任务设计，同时构建ChemRxn-V基准数据集评估基于视觉的反应识别和预测任务。

Result: TinyChemVL在仅4B参数的情况下，在分子和反应任务上均取得优越性能，推理和训练速度更快，仅使用ChemVLM 1/16的视觉令牌就超越其性能。

Conclusion: 通过协同设计模型架构和任务复杂性，该工作为化学领域构建了高效且强大的视觉语言模型，为化学推理提供了新的解决方案。

Abstract: While Vision Language Models (VLMs) have demonstrated remarkable capabilities
in general visual understanding, their application in the chemical domain has
been limited, with previous works predominantly focusing on text and thus
overlooking critical visual information, such as molecular structures. Current
approaches that directly adopt standard VLMs for chemical tasks suffer from two
primary issues: (i) computational inefficiency of processing entire chemical
images with non-informative backgrounds. (ii) a narrow scope on molecular-level
tasks that restricts progress in chemical reasoning. In this work, we propose
\textbf{TinyChemVL}, an efficient and powerful chemical VLM that leverages
visual token reduction and reaction-level tasks to improve model efficiency and
reasoning capacity. Also, we propose \textbf{ChemRxn-V}, a reaction-level
benchmark for assessing vision-based reaction recognition and prediction tasks.
Directly predicting reaction products from molecular images poses a non-trivial
challenge, as it requires models to integrate both recognition and reasoning
capacities. Our results demonstrate that with only 4B parameters, TinyChemVL
achieves superior performance on both molecular and reaction tasks while
demonstrating faster inference and training speeds compared to existing models.
Notably, TinyChemVL outperforms ChemVLM while utilizing only 1/16th of the
visual tokens. This work builds efficient yet powerful VLMs for chemical
domains by co-designing model architecture and task complexity.

</details>


### [66] [Learning-Based Vision Systems for Semi-Autonomous Forklift Operation in Industrial Warehouse Environments](https://arxiv.org/abs/2511.06295)
*Vamshika Sutar,Mahek Maheshwari,Archak Mittal*

Main category: cs.CV

TL;DR: 本文提出了一种基于单摄像头的托盘和托盘孔检测与映射框架，使用优化后的YOLOv8和YOLOv11模型，为叉车提供低成本、可改造的视觉感知方案。


<details>
  <summary>Details</summary>
Motivation: 随着仓库自动化程度提高，叉车和AGV需要更可靠、低成本的感知系统。现有的解决方案成本较高，需要一种经济实用的视觉感知技术来推动仓库自动化发展。

Method: 采用YOLOv8和YOLOv11架构，通过Optuna驱动的超参数优化和空间后处理技术进行增强；创新性地设计托盘孔映射模块，将检测结果转换为可操作的空间表示，实现叉车操作的精确托盘和托盘孔关联。

Result: 在增强真实仓库图像的自定义数据集实验中，YOLOv8实现高检测精度，YOLOv11在优化配置下展现出卓越的精确度和稳定收敛性，验证了视觉感知模块的可行性。

Conclusion: 该研究为叉车提供了一种成本效益高、可改造的视觉感知模块，提出了一种可扩展的方法来推进仓库自动化，促进更安全、经济和智能的物流操作。

Abstract: The automation of material handling in warehouses increasingly relies on
robust, low cost perception systems for forklifts and Automated Guided Vehicles
(AGVs). This work presents a vision based framework for pallet and pallet hole
detection and mapping using a single standard camera. We utilized YOLOv8 and
YOLOv11 architectures, enhanced through Optuna driven hyperparameter
optimization and spatial post processing. An innovative pallet hole mapping
module converts the detections into actionable spatial representations,
enabling accurate pallet and pallet hole association for forklift operation.
Experiments on a custom dataset augmented with real warehouse imagery show that
YOLOv8 achieves high pallet and pallet hole detection accuracy, while YOLOv11,
particularly under optimized configurations, offers superior precision and
stable convergence. The results demonstrate the feasibility of a cost
effective, retrofittable visual perception module for forklifts. This study
proposes a scalable approach to advancing warehouse automation, promoting
safer, economical, and intelligent logistics operations.

</details>


### [67] [Physics-Informed Deformable Gaussian Splatting: Towards Unified Constitutive Laws for Time-Evolving Material Field](https://arxiv.org/abs/2511.06299)
*Haoqin Hong,Ding Fan,Fubin Dou,Zhi-Li Zhou,Haoran Sun,Congcong Zhu,Jingrun Chen*

Main category: cs.CV

TL;DR: 提出物理信息可变形高斯泼溅(PIDG)方法，通过将高斯粒子视为拉格朗日材料点并结合物理约束，改善了动态场景的单目重建质量。


<details>
  <summary>Details</summary>
Motivation: 纯数据驱动的3D高斯泼溅技术在捕捉动态场景中物理驱动的运动模式时存在困难，需要引入物理先验知识来提升重建的物理一致性。

Method: 采用静态-动态解耦的4D分解哈希编码重建几何和运动，施加柯西动量残差作为物理约束，通过时变材料场独立预测粒子速度和应力，并使用相机补偿光流监督拉格朗日粒子流。

Result: 在自定义物理驱动数据集以及标准合成和真实数据集上的实验表明，该方法在物理一致性和单目动态重建质量方面取得显著提升。

Conclusion: PIDG通过物理信息的学习方法有效解决了传统3DGS在动态场景重建中的物理一致性问题，为动态新视图合成提供了更准确和物理合理的解决方案。

Abstract: Recently, 3D Gaussian Splatting (3DGS), an explicit scene representation
technique, has shown significant promise for dynamic novel-view synthesis from
monocular video input. However, purely data-driven 3DGS often struggles to
capture the diverse physics-driven motion patterns in dynamic scenes. To fill
this gap, we propose Physics-Informed Deformable Gaussian Splatting (PIDG),
which treats each Gaussian particle as a Lagrangian material point with
time-varying constitutive parameters and is supervised by 2D optical flow via
motion projection. Specifically, we adopt static-dynamic decoupled 4D
decomposed hash encoding to reconstruct geometry and motion efficiently.
Subsequently, we impose the Cauchy momentum residual as a physics constraint,
enabling independent prediction of each particle's velocity and constitutive
stress via a time-evolving material field. Finally, we further supervise data
fitting by matching Lagrangian particle flow to camera-compensated optical
flow, which accelerates convergence and improves generalization. Experiments on
a custom physics-driven dataset as well as on standard synthetic and real-world
datasets demonstrate significant gains in physical consistency and monocular
dynamic reconstruction quality.

</details>


### [68] [Adaptive 3D Reconstruction via Diffusion Priors and Forward Curvature-Matching Likelihood Updates](https://arxiv.org/abs/2511.06310)
*Seunghyeok Shin,Dabin Kim,Hongki Lim*

Main category: cs.CV

TL;DR: 本文提出了一种新的前向曲率匹配（FCM）方法，与扩散采样结合，实现了从图像到高质量点云的重建，无需重新训练即可支持多种输入模态和视图数量。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的点云重建方法存在不灵活性问题：需要条件信号训练、只支持固定输入视图、不同测量需完全重新训练，且现有改进方法依赖启发式固定步长导致收敛慢和重建质量次优。

Method: 提出前向曲率匹配（FCM）更新方法，集成到扩散采样中；仅使用前向自动微分和有限差分曲率估计动态确定最优步长，实现似然更新的精确优化；支持单视图和多视图输入，通过简单操作符替换支持多种输入模态，无需重新训练。

Result: 在ShapeNet和CO3D数据集上实验表明，该方法在匹配或更低的NFEs下实现了更优重建质量，获得更高F-score和更低CD、EMD指标，证明了其在实际应用中的高效性和适应性。

Conclusion: FCM方法有效解决了现有扩散模型在点云重建中的局限性，通过动态步长优化提升了重建质量和收敛速度，其无需重新训练的灵活性为实际应用提供了高效的解决方案。

Abstract: Reconstructing high-quality point clouds from images remains challenging in
computer vision. Existing generative-model-based approaches, particularly
diffusion-model approaches that directly learn the posterior, may suffer from
inflexibility -- they require conditioning signals during training, support
only a fixed number of input views, and need complete retraining for different
measurements. Recent diffusion-based methods have attempted to address this by
combining prior models with likelihood updates, but they rely on heuristic
fixed step sizes for the likelihood update that lead to slow convergence and
suboptimal reconstruction quality. We advance this line of approach by
integrating our novel Forward Curvature-Matching (FCM) update method with
diffusion sampling. Our method dynamically determines optimal step sizes using
only forward automatic differentiation and finite-difference curvature
estimates, enabling precise optimization of the likelihood update. This
formulation enables high-fidelity reconstruction from both single-view and
multi-view inputs, and supports various input modalities through simple
operator substitution -- all without retraining. Experiments on ShapeNet and
CO3D datasets demonstrate that our method achieves superior reconstruction
quality at matched or lower NFEs, yielding higher F-score and lower CD and EMD,
validating its efficiency and adaptability for practical applications. Code is
available at https://github.com/Seunghyeok0715/FCM

</details>


### [69] [Seq2Seq Models Reconstruct Visual Jigsaw Puzzles without Seeing Them](https://arxiv.org/abs/2511.06315)
*Gur Elkn,Ofir Itzhak Shahar,Ohad Ben-Shahar*

Main category: cs.CV

TL;DR: 该论文提出了一种创新方法，使用语言模型而非视觉信息来解决拼图问题，通过将拼图块转换为token序列，将拼图重组视为序列到序列预测任务，取得了超越视觉方法的最佳性能。


<details>
  <summary>Details</summary>
Motivation: 传统拼图求解算法主要基于视觉视角，但作者探索了一个根本不同的方向：完全不用视觉信息，仅依靠语言模型来拼图。这种"盲"求解方法旨在测试语言模型在非原生领域（视觉空间推理）的跨域能力。

Method: 引入专门分词器将每个拼图块转换为离散token序列，将拼图重组重新定义为序列到序列预测任务。使用编码器-解码器transformer架构，仅基于token序列进行推理，完全避免访问原始视觉输入。

Result: 尽管被故意限制无法访问视觉输入，该语言模型方法在多个基准测试中达到最先进水平，经常优于基于视觉的传统方法，展现了令人惊讶的性能。

Conclusion: 研究发现语言模型具有超出其原生领域的强大问题解决能力，非传统方法可以为拼图求解研究开辟有前景的新方向，挑战了视觉任务必须依赖视觉信息的固有观念。

Abstract: Jigsaw puzzles are primarily visual objects, whose algorithmic solutions have
traditionally been framed from a visual perspective. In this work, however, we
explore a fundamentally different approach: solving square jigsaw puzzles using
language models, without access to raw visual input. By introducing a
specialized tokenizer that converts each puzzle piece into a discrete sequence
of tokens, we reframe puzzle reassembly as a sequence-to-sequence prediction
task. Treated as "blind" solvers, encoder-decoder transformers accurately
reconstruct the original layout by reasoning over token sequences alone.
Despite being deliberately restricted from accessing visual input, our models
achieve state-of-the-art results across multiple benchmarks, often
outperforming vision-based methods. These findings highlight the surprising
capability of language models to solve problems beyond their native domain, and
suggest that unconventional approaches can inspire promising directions for
puzzle-solving research.

</details>


### [70] [CINEMAE: Leveraging Frozen Masked Autoencoders for Cross-Generator AI Image Detection](https://arxiv.org/abs/2511.06325)
*Minsuk Jang,Hyeonseo Jeong,Minseok Son,Changick Kim*

Main category: cs.CV

TL;DR: CINEMAE是一种基于掩码自编码器的AIGC图像检测方法，通过计算条件负对数似然来量化局部语义异常，实现了强大的跨生成器泛化能力，仅在Stable Diffusion v1.4上训练就能在8个未见过的生成器上达到95%以上的准确率。


<details>
  <summary>Details</summary>
Motivation: 当前基于上下文的AI生成文本检测器通过测量分布不一致性实现了强泛化能力，但基于图像的检测器仍然存在过度拟合生成器特定伪影的问题。需要一种能够像文本检测方法一样具有强泛化能力的图像检测范式。

Method: CINEMAE将文本检测方法的核心原理适配到视觉领域。关键洞察是，掩码自编码器(MAE)在重建被遮蔽补丁时会自然编码语义一致性期望。方法将重建过程概率化，计算条件负对数似然(p(masked|visible))来量化局部语义异常，然后通过学习的融合机制将补丁级别统计量与全局MAE特征聚合。

Result: 仅在Stable Diffusion v1.4上训练，CINEMAE在GenImage基准测试的8个未见过的生成器上实现了超过95%的准确率，显著优于最先进的检测器，展现了卓越的跨生成器泛化性能。

Conclusion: 研究表明，上下文条件重建的不确定性为AIGC检测提供了一个鲁棒且可迁移的信号，验证了将文本检测原理成功应用于视觉领域的有效性。

Abstract: While context-based detectors have achieved strong generalization for
AI-generated text by measuring distributional inconsistencies, image-based
detectors still struggle with overfitting to generator-specific artifacts. We
introduce CINEMAE, a novel paradigm for AIGC image detection that adapts the
core principles of text detection methods to the visual domain. Our key insight
is that Masked AutoEncoder (MAE), trained to reconstruct masked patches
conditioned on visible context, naturally encodes semantic consistency
expectations. We formalize this reconstruction process probabilistically,
computing conditional Negative Log-Likelihood (NLL, p(masked | visible)) to
quantify local semantic anomalies. By aggregating these patch-level statistics
with global MAE features through learned fusion, CINEMAE achieves strong
cross-generator generalization. Trained exclusively on Stable Diffusion v1.4,
our method achieves over 95% accuracy on all eight unseen generators in the
GenImage benchmark, substantially outperforming state-of-the-art detectors.
This demonstrates that context-conditional reconstruction uncertainty provides
a robust, transferable signal for AIGC detection.

</details>


### [71] [Label-Efficient 3D Forest Mapping: Self-Supervised and Transfer Learning for Individual, Structural, and Species Analysis](https://arxiv.org/abs/2511.06331)
*Aldino Rizaldy,Fabian Ewald Fassnacht,Ahmed Jamal Afifi,Hua Jiang,Richard Gloaguen,Pedram Ghamisi*

Main category: cs.CV

TL;DR: 本文提出了一种结合自监督学习和迁移学习的统一框架，用于从激光扫描点云中自动提取单木的结构和物种信息，显著减少对标注数据的依赖并提升三个关键任务的性能。


<details>
  <summary>Details</summary>
Motivation: 单木级别的详细结构和物种信息对精准林业、生物多样性保护和碳汇制图至关重要，但现有深度学习方法需要大量标注数据，而复杂森林环境下的3D点云标注费时费力且难以扩展。

Method: 采用自监督学习和迁移学习架构，通过域适应技术增强实例分割，自监督预训练改进语义分割，以及分层迁移学习实现未见物种分类，并将三个任务集成到统一框架中。

Result: 实例分割AP50提升16.98%，语义分割mIoU提升1.79%，未见物种分类Jaccard指数提升6.07%，预训练模型减少能耗和碳排放约21%。

Conclusion: 该开源统一框架有效解决了标注数据稀缺问题，显著提升了单木信息提取的实用性和可扩展性，为林业管理、生物多样性保护和碳汇制图提供了重要技术支撑。

Abstract: Detailed structural and species information on individual tree level is
increasingly important to support precision forestry, biodiversity
conservation, and provide reference data for biomass and carbon mapping. Point
clouds from airborne and ground-based laser scanning are currently the most
suitable data source to rapidly derive such information at scale. Recent
advancements in deep learning improved segmenting and classifying individual
trees and identifying semantic tree components. However, deep learning models
typically require large amounts of annotated training data which limits further
improvement. Producing dense, high-quality annotations for 3D point clouds,
especially in complex forests, is labor-intensive and challenging to scale. We
explore strategies to reduce dependence on large annotated datasets using
self-supervised and transfer learning architectures. Our objective is to
improve performance across three tasks: instance segmentation, semantic
segmentation, and tree classification using realistic and operational training
sets. Our findings indicate that combining self-supervised learning with domain
adaptation significantly enhances instance segmentation compared to training
from scratch (AP50 +16.98%), self-supervised learning suffices for semantic
segmentation (mIoU +1.79%), and hierarchical transfer learning enables accurate
classification of unseen species (Jaccard +6.07%). To simplify use and
encourage uptake, we integrated the tasks into a unified framework,
streamlining the process from raw point clouds to tree delineation, structural
analysis, and species classification. Pretrained models reduce energy
consumption and carbon emissions by ~21%. This open-source contribution aims to
accelerate operational extraction of individual tree information from laser
scanning point clouds to support forestry, biodiversity, and carbon mapping.

</details>


### [72] [BuildingWorld: A Structured 3D Building Dataset for Urban Foundation Models](https://arxiv.org/abs/2511.06337)
*Shangfeng Huang,Ruisheng Wang,Xin Wang*

Main category: cs.CV

TL;DR: BuildingWorld是一个包含约500万个LOD2建筑模型的综合3D建筑数据集，涵盖全球地理和建筑多样性，配备真实和模拟的机载LiDAR点云，旨在解决现有数据集建筑风格单一导致的模型泛化能力不足问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的3D城市建模模型大多在建筑风格有限的数据集上训练，这严重削弱了模型在异构城市环境中的泛化能力，阻碍了数字孪生等高精度城市表示应用的发展。

Method: 构建BuildingWorld数据集，收集来自北美、欧洲、亚洲、非洲和大洋洲等地理和建筑风格多样化的约500万个LOD2建筑模型，配备真实和模拟的机载LiDAR点云，并引入Cyber City虚拟城市模型来生成无限定制化的训练数据，同时提供标准化的建筑重建评估指标。

Result: 成功创建了一个具有全球代表性的大规模3D建筑数据集，包含500万建筑模型和相应的LiDAR数据，支持3D建筑重建、检测和分割等综合研究，为大规模视觉模型和基础模型在城市环境中的训练、评估和比较提供了基础设施。

Conclusion: BuildingWorld填补了3D建筑数据集在风格多样性方面的空白，通过提供全球代表性数据和评估标准，促进了结构化3D城市环境中大规模模型的发展，提高了模型在不同城市环境中的泛化能力和实用性。

Abstract: As digital twins become central to the transformation of modern cities,
accurate and structured 3D building models emerge as a key enabler of
high-fidelity, updatable urban representations. These models underpin diverse
applications including energy modeling, urban planning, autonomous navigation,
and real-time reasoning. Despite recent advances in 3D urban modeling, most
learning-based models are trained on building datasets with limited
architectural diversity, which significantly undermines their generalizability
across heterogeneous urban environments. To address this limitation, we present
BuildingWorld, a comprehensive and structured 3D building dataset designed to
bridge the gap in stylistic diversity. It encompasses buildings from
geographically and architecturally diverse regions -- including North America,
Europe, Asia, Africa, and Oceania -- offering a globally representative dataset
for urban-scale foundation modeling and analysis. Specifically, BuildingWorld
provides about five million LOD2 building models collected from diverse
sources, accompanied by real and simulated airborne LiDAR point clouds. This
enables comprehensive research on 3D building reconstruction, detection and
segmentation. Cyber City, a virtual city model, is introduced to enable the
generation of unlimited training data with customized and structurally diverse
point cloud distributions. Furthermore, we provide standardized evaluation
metrics tailored for building reconstruction, aiming to facilitate the
training, evaluation, and comparison of large-scale vision models and
foundation models in structured 3D urban environments.

</details>


### [73] [GazeVLM: A Vision-Language Model for Multi-Task Gaze Understanding](https://arxiv.org/abs/2511.06348)
*Athul M. Mathew,Haithem Hermassi,Thariq Khalid,Arshad Ali Khan,Riad Souissi*

Main category: cs.CV

TL;DR: GazeVLM是一个新型的视觉语言模型，首次将VLM应用于多任务凝视理解，实现人员检测、凝视目标检测和凝视对象识别的统一框架。


<details>
  <summary>Details</summary>
Motivation: 凝视理解对于视觉注意力和意图估计至关重要，但现有的研究缺乏一个能够同时使用视觉和语言提示的统一系统。需要开发一个集成的框架来处理人员检测、凝视目标检测和凝视对象识别等多个任务。

Method: 提出GazeVLM视觉语言模型，整合RGB图像、HHA编码的深度图和文本提示三种模态，通过视觉-语言融合实现多任务凝视理解，支持选择性执行各个任务。引入物体级凝视检测指标(AP_ob)来评估凝视对象识别性能。

Result: 消融实验表明RGB图像与HHA编码深度图融合、并由文本提示引导的方法效果最佳。在GazeFollow和VideoAttentionTarget数据集上达到最先进的评估分数，实现了显著性能提升。

Conclusion: GazeVLM成功将视觉语言模型应用于凝视理解领域，首次实现了基于VLM的统一多任务框架，为视觉注意力和意图估计提供了新的解决方案，证明了多模态融合在凝视理解中的有效性。

Abstract: Gaze understanding unifies the detection of people, their gaze targets, and
objects of interest into a single framework, offering critical insight into
visual attention and intent estimation. Although prior research has modelled
gaze cues in visual scenes, a unified system is still needed for gaze
understanding using both visual and language prompts. This paper introduces
GazeVLM, a novel Vision-Language Model (VLM) for multi-task gaze understanding
in images, addressing person detection, gaze target detection, and gaze object
identification. While other transformer-based methods exist for gaze analysis,
GazeVLM represents, to our knowledge, the first application of a VLM to these
combined tasks, allowing for selective execution of each task. Through the
integration of visual (RGB and depth) and textual modalities, our ablation
study on visual input combinations revealed that a fusion of RGB images with
HHA-encoded depth maps, guided by text prompts, yields superior performance. We
also introduce an object-level gaze detection metric for gaze object
identification ($AP_{ob}$). Through experiments, GazeVLM demonstrates
significant improvements, notably achieving state-of-the-art evaluation scores
on GazeFollow and VideoAttentionTarget datasets.

</details>


### [74] [AesTest: Measuring Aesthetic Intelligence from Perception to Production](https://arxiv.org/abs/2511.06360)
*Guolong Wang,Heng Huang,Zhiqiang Zhang,Wentian Li,Feilong Ma,Xin Jin*

Main category: cs.CV

TL;DR: AesTest是一个综合性的多模态美学感知与生成评估基准，包含10个任务，用于评估MLLM的美学判断能力


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型的美学评估基准存在感知范围狭窄、缺乏评估系统性美学生产所需的多样性等问题

Method: 构建AesTest基准：1)基于生成学习心理学理论设计10个多选题任务，涵盖感知、欣赏、创作和摄影；2)整合专业编辑流程、摄影构图教程和众包偏好等多样化数据源；3)支持属性分析、情感共鸣、构图选择和风格推理等多种美学查询类型

Result: 对指令调优的IAA MLLM和通用MLLM进行评估，揭示了构建美学智能面临重大挑战

Conclusion: 将公开发布AesTest基准以支持该领域未来研究，推动多模态美学智能发展

Abstract: Perceiving and producing aesthetic judgments is a fundamental yet
underexplored capability for multimodal large language models (MLLMs). However,
existing benchmarks for image aesthetic assessment (IAA) are narrow in
perception scope or lack the diversity needed to evaluate systematic aesthetic
production. To address this gap, we introduce AesTest, a comprehensive
benchmark for multimodal aesthetic perception and production, distinguished by
the following features: 1) It consists of curated multiple-choice questions
spanning ten tasks, covering perception, appreciation, creation, and
photography. These tasks are grounded in psychological theories of generative
learning. 2) It integrates data from diverse sources, including professional
editing workflows, photographic composition tutorials, and crowdsourced
preferences. It ensures coverage of both expert-level principles and real-world
variation. 3) It supports various aesthetic query types, such as
attribute-based analysis, emotional resonance, compositional choice, and
stylistic reasoning. We evaluate both instruction-tuned IAA MLLMs and general
MLLMs on AesTest, revealing significant challenges in building aesthetic
intelligence. We will publicly release AesTest to support future research in
this area.

</details>


### [75] [V-Shuffle: Zero-Shot Style Transfer via Value Shuffle](https://arxiv.org/abs/2511.06365)
*Haojun Tang,Qiwei Lin,Tongda Xu,Lida Huang,Yan Wang*

Main category: cs.CV

TL;DR: V-Shuffle是一种零样本风格迁移方法，通过打乱扩散模型自注意力层的值特征来消除风格图像的内容泄漏，并引入混合风格正则化增强风格保真度，在多图和单图场景下均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力注入的风格迁移方法存在内容泄漏问题，即风格图像的语义内容会错误地出现在风格化输出中。需要一种方法能够在保持内容完整性的同时提高风格保真度。

Method: V-Shuffle通过打乱扩散模型自注意力层的值特征来隐式破坏风格图像的语义内容，同时保留低级风格表示；引入混合风格正则化，将低级表示与高级风格纹理结合以增强风格保真度；支持零样本多风格图像输入。

Result: 实验表明V-Shuffle在利用多个风格图像时表现优异；在单图场景下也超越了之前的最先进方法，有效解决了内容泄漏问题。

Conclusion: V-Shuffle通过创新的特征打乱机制和混合正则化策略，成功平衡了内容保持与风格保真度，为风格迁移领域提供了一个有效的零样本解决方案。

Abstract: Attention injection-based style transfer has achieved remarkable progress in
recent years. However, existing methods often suffer from content leakage,
where the undesired semantic content of the style image mistakenly appears in
the stylized output. In this paper, we propose V-Shuffle, a zero-shot style
transfer method that leverages multiple style images from the same style domain
to effectively navigate the trade-off between content preservation and style
fidelity. V-Shuffle implicitly disrupts the semantic content of the style
images by shuffling the value features within the self-attention layers of the
diffusion model, thereby preserving low-level style representations. We further
introduce a Hybrid Style Regularization that complements these low-level
representations with high-level style textures to enhance style fidelity.
Empirical results demonstrate that V-Shuffle achieves excellent performance
when utilizing multiple style images. Moreover, when applied to a single style
image, V-Shuffle outperforms previous state-of-the-art methods.

</details>


### [76] [InfoAffect: A Dataset for Affective Analysis of Infographics](https://arxiv.org/abs/2511.06404)
*Zihang Fu,Yunchao Wang,Chenyu Huang,Guodao Sun,Ronghua Liang*

Main category: cs.CV

TL;DR: 本研究创建了InfoAffect数据集，包含3500个情感标注的信息图表样本，通过多模态大语言模型和RRF融合算法实现高精度的情感分析。


<details>
  <summary>Details</summary>
Motivation: 信息图表被广泛用于传达复杂信息，但由于数据资源稀缺，其情感维度仍未得到充分探索，需要构建高质量的情感标注数据集来支持相关研究。

Method: 从六个领域收集原始数据，通过预处理、伴随文本优先方法和质量保证策略进行处理；构建情感表约束标注；使用五个MLLMs分析多模态信息，通过RRF算法融合输出结果。

Result: 用户研究验证表明，InfoAffect数据集的复合情感一致性指数(CACI)达到0.986，证明了数据集的高准确性和可用性。

Conclusion: InfoAffect数据集成功填补了信息图表情感分析领域的数据空白，为多模态情感计算研究提供了高质量的数据资源，具有很高的实用价值。

Abstract: Infographics are widely used to convey complex information, yet their
affective dimensions remain underexplored due to the scarcity of data
resources. We introduce a 3.5k-sample affect-annotated InfoAffect dataset,
which combines textual content with real-world infographics. We first collect
the raw data from six domains and aligned them via preprocessing, the
accompanied-text-priority method, and three strategies to guarantee the quality
and compliance. After that we construct an affect table and use it to constrain
annotation. Five state-of-the-art multimodal large language models (MLLMs) then
analyze both modalities, and their outputs are fused with Reciprocal Rank
Fusion (RRF) algorithm to yield robust affects and confidences. We conducted a
user study with two experiments to validate usability and assess InfoAffect
dataset using the Composite Affect Consistency Index (CACI), achieving an
overall score of 0.986, which indicates high accuracy.

</details>


### [77] [On Modality Incomplete Infrared-Visible Object Detection: An Architecture Compatibility Perspective](https://arxiv.org/abs/2511.06406)
*Shuo Yang,Yinghui Xing,Shizhou Zhang,Zhilong Niu*

Main category: cs.CV

TL;DR: 本文提出了Scarf-DETR，一个即插即用的颈部模块，通过模态无关的可变形注意力机制和伪模态丢弃策略，解决了红外与可见光目标检测中模态不完整时的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 当前红外与可见光目标检测模型在面临模态不完整数据时性能显著下降，特别是当主导模态缺失时。这一局限性阻碍了模型在实际应用中的鲁棒性。

Method: 提出了Scarf Neck模块，引入模态无关的可变形注意力机制，使检测器能够灵活适应单模态或双模态；设计了伪模态丢弃策略在训练中充分利用多模态信息；构建了全面的模态不完整IVOD评估基准。

Result: Scarf-DETR在缺失模态场景下表现优异，同时在标准完整模态IVOD基准上也实现了优越性能，展现了在完整和不完整模态环境下的强大适应性。

Conclusion: 本文从架构兼容性角度系统解决了模态不完全IVOD问题，提出的Scarf-DETR不仅提升了模型在模态缺失情况下的鲁棒性，还保持了在完整模态下的优异性能，为实际应用提供了更可靠的解决方案。

Abstract: Infrared and visible object detection (IVOD) is essential for numerous
around-the-clock applications. Despite notable advancements, current IVOD
models exhibit notable performance declines when confronted with incomplete
modality data, particularly if the dominant modality is missing. In this paper,
we take a thorough investigation on modality incomplete IVOD problem from an
architecture compatibility perspective. Specifically, we propose a
plug-and-play Scarf Neck module for DETR variants, which introduces a
modality-agnostic deformable attention mechanism to enable the IVOD detector to
flexibly adapt to any single or double modalities during training and
inference. When training Scarf-DETR, we design a pseudo modality dropout
strategy to fully utilize the multi-modality information, making the detector
compatible and robust to both working modes of single and double modalities.
Moreover, we introduce a comprehensive benchmark for the modality-incomplete
IVOD task aimed at thoroughly assessing situations where the absent modality is
either dominant or secondary. Our proposed Scarf-DETR not only performs
excellently in missing modality scenarios but also achieves superior
performances on the standard IVOD modality complete benchmarks. Our code will
be available at https://github.com/YinghuiXing/Scarf-DETR.

</details>


### [78] [VDNeRF: Vision-only Dynamic Neural Radiance Field for Urban Scenes](https://arxiv.org/abs/2511.06408)
*Zhengyu Zou,Jingfeng Li,Hao Li,Xiaolei Hou,Jinwen Hu,Jingkun Chen,Lechao Cheng,Dingwen Zhang*

Main category: cs.CV

TL;DR: VDNeRF提出了一种仅使用视觉信息的动态神经辐射场方法，能够在不需要相机位姿信息的情况下，准确恢复相机轨迹并重建动态城市场景。


<details>
  <summary>Details</summary>
Motivation: 现有基于NeRF的方法在自动驾驶和机器人感知应用中面临挑战，主要因为难以获取准确的相机位姿以及在处理大规模动态环境时的局限性。

Method: VDNeRF采用两个分离的NeRF模型：静态NeRF优化相机位姿和静态背景，动态NeRF结合3D场景流确保动态物体的准确重建；设计有效的训练框架解决相机运动与物体运动之间的歧义。

Result: 在主流城市场景数据集上的广泛评估表明，VDNeRF在相机位姿估计和动态新视角合成方面都超越了最先进的免位姿NeRF方法。

Conclusion: VDNeRF成功实现了无需额外传感器数据的动态场景重建，为自动驾驶等应用提供了有效的解决方案，在位姿估计和新视角合成任务上取得了显著改进。

Abstract: Neural Radiance Fields (NeRFs) implicitly model continuous three-dimensional
scenes using a set of images with known camera poses, enabling the rendering of
photorealistic novel views. However, existing NeRF-based methods encounter
challenges in applications such as autonomous driving and robotic perception,
primarily due to the difficulty of capturing accurate camera poses and
limitations in handling large-scale dynamic environments. To address these
issues, we propose Vision-only Dynamic NeRF (VDNeRF), a method that accurately
recovers camera trajectories and learns spatiotemporal representations for
dynamic urban scenes without requiring additional camera pose information or
expensive sensor data. VDNeRF employs two separate NeRF models to jointly
reconstruct the scene. The static NeRF model optimizes camera poses and static
background, while the dynamic NeRF model incorporates the 3D scene flow to
ensure accurate and consistent reconstruction of dynamic objects. To address
the ambiguity between camera motion and independent object motion, we design an
effective and powerful training framework to achieve robust camera pose
estimation and self-supervised decomposition of static and dynamic elements in
a scene. Extensive evaluations on mainstream urban driving datasets demonstrate
that VDNeRF surpasses state-of-the-art NeRF-based pose-free methods in both
camera pose estimation and dynamic novel view synthesis.

</details>


### [79] [Diagnose Like A REAL Pathologist: An Uncertainty-Focused Approach for Trustworthy Multi-Resolution Multiple Instance Learning](https://arxiv.org/abs/2511.06433)
*Sungrae Hong,Sol Lee,Jisu Shin,Mun Yong Yi*

Main category: cs.CV

TL;DR: UFC-MIL是一种基于不确定性的校准多示例学习方法，通过多分辨率图像在保持高分类精度的同时提供校准的病理诊断预测。


<details>
  <summary>Details</summary>
Motivation: 现有多分辨率MIL方法仅关注性能提升，缺乏校准研究，而临床专家需要可信赖的诊断结果。病理学家在检查时会考虑不同分辨率的图像，需要模拟这种行为并确保预测的可靠性。

Method: UFC-MIL包含三个核心组件：1）新颖的块级损失函数，学习实例潜在模式并表达分类不确定性；2）基于注意力的架构配合邻居块聚合模块进行特征收集；3）通过块级不确定性校准聚合预测，无需多次迭代推理。

Result: 在挑战性公共数据集上，UFC-MIL在模型校准方面表现优越，同时达到与最先进方法相当的分类精度，实现了性能与可信度的平衡。

Conclusion: UFC-MIL成功模拟了病理学家的检查行为，为临床诊断提供了可靠的AI辅助工具，在保证精度的同时解决了预测校准问题，具有重要的实际应用价值。

Abstract: With the increasing demand for histopathological specimen examination and
diagnostic reporting, Multiple Instance Learning (MIL) has received heightened
research focus as a viable solution for AI-centric diagnostic aid. Recently, to
improve its performance and make it work more like a pathologist, several MIL
approaches based on the use of multiple-resolution images have been proposed,
delivering often higher performance than those that use single-resolution
images. Despite impressive recent developments of multiple-resolution MIL,
previous approaches only focus on improving performance, thereby lacking
research on well-calibrated MIL that clinical experts can rely on for
trustworthy diagnostic results. In this study, we propose Uncertainty-Focused
Calibrated MIL (UFC-MIL), which more closely mimics the pathologists'
examination behaviors while providing calibrated diagnostic predictions, using
multiple images with different resolutions. UFC-MIL includes a novel patch-wise
loss that learns the latent patterns of instances and expresses their
uncertainty for classification. Also, the attention-based architecture with a
neighbor patch aggregation module collects features for the classifier. In
addition, aggregated predictions are calibrated through patch-level uncertainty
without requiring multiple iterative inferences, which is a key practical
advantage. Against challenging public datasets, UFC-MIL shows superior
performance in model calibration while achieving classification accuracy
comparable to that of state-of-the-art methods.

</details>


### [80] [HiMo-CLIP: Modeling Semantic Hierarchy and Monotonicity in Vision-Language Alignment](https://arxiv.org/abs/2511.06653)
*Ruijia Wu,Ping Chen,Fei Shen,Shaoan Zhao,Qiang Hui,Huanlin Gao,Ting Lu,Zhaoxiang Liu,Fang Zhao,Kai Wang,Shiguo Lian*

Main category: cs.CV

TL;DR: HiMo-CLIP是一个无需修改编码器架构的表征级框架，通过层次分解模块和单调感知对比损失来增强CLIP模型，有效处理长篇和组合性文本描述。


<details>
  <summary>Details</summary>
Motivation: 现有的对比视觉语言模型如CLIP将文本视为平铺序列，无法有效处理复杂、组合性和长篇描述，特别缺乏对语言两个关键特性的捕捉：语义层次结构（反映文本的多级组合结构）和语义单调性（更丰富的描述应与视觉内容产生更强的对齐）。

Method: 提出HiMo-CLIP框架，包含两个核心组件：1) 层次分解(HiDe)模块，通过批量PCA从长文本中提取潜在语义组件，实现跨不同语义粒度的灵活批量感知对齐；2) 单调感知对比损失(MoLo)，联合对齐全局和组件级表征，鼓励模型内化语义排序和对齐强度作为文本完整性的函数。

Result: 在多个图像-文本检索基准测试上，HiMo-CLIP始终优于强基线模型，特别是在处理长篇或组合性描述时表现尤为突出。

Conclusion: HiMo-CLIP框架成功产生了结构化的、认知一致的多模态表征，通过语义层次和单调性的建模，显著提升了CLIP类模型在复杂文本理解任务上的性能，为视觉语言对齐提供了新的研究方向。

Abstract: Contrastive vision-language models like CLIP have achieved impressive results
in image-text retrieval by aligning image and text representations in a shared
embedding space. However, these models often treat text as flat sequences,
limiting their ability to handle complex, compositional, and long-form
descriptions. In particular, they fail to capture two essential properties of
language: semantic hierarchy, which reflects the multi-level compositional
structure of text, and semantic monotonicity, where richer descriptions should
result in stronger alignment with visual content.To address these limitations,
we propose HiMo-CLIP, a representation-level framework that enhances CLIP-style
models without modifying the encoder architecture. HiMo-CLIP introduces two key
components: a hierarchical decomposition (HiDe) module that extracts latent
semantic components from long-form text via in-batch PCA, enabling flexible,
batch-aware alignment across different semantic granularities, and a
monotonicity-aware contrastive loss (MoLo) that jointly aligns global and
component-level representations, encouraging the model to internalize semantic
ordering and alignment strength as a function of textual completeness.These
components work in concert to produce structured, cognitively-aligned
cross-modal representations. Experiments on multiple image-text retrieval
benchmarks show that HiMo-CLIP consistently outperforms strong baselines,
particularly under long or compositional descriptions. The code is available at
https://github.com/UnicomAI/HiMo-CLIP.

</details>


### [81] [Countering Multi-modal Representation Collapse through Rank-targeted Fusion](https://arxiv.org/abs/2511.06450)
*Seulgi Kim,Kiran Kokilepersaud,Mohit Prabhushankar,Ghassan AlRegib*

Main category: cs.CV

TL;DR: 论文提出了一种基于有效秩的多模态融合框架Rank-enhancing Token Fuser，统一解决了特征崩溃和模态崩溃问题，并在动作预测任务中显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 多模态融合方法存在两种表示崩溃问题：特征崩溃（各维度失去判别能力）和模态崩溃（一个模态主导另一个）。现有方法只能分别解决这些问题，缺乏统一框架来同时处理。动作预测等应用需要融合多种传感器数据，受到这两种崩溃的严重影响。

Method: 提出使用有效秩作为统一度量来量化和对抗两种表示崩溃；设计Rank-enhancing Token Fuser理论框架，选择性地融合一个模态中信息量较少的特征与另一个模态的互补特征；评估模态组合的相互有效秩提升，发现深度与RGB融合能保持表示平衡；基于此提出R3D深度信息融合框架。

Result: 在NTURGBD、UTKinect和DARai数据集上的大量实验表明，所提方法比之前最先进方法性能提升高达3.74%；方法成功提高了融合表示的有效秩；深度与RGB融合有效避免了模态崩溃。

Conclusion: 有效秩是解决多模态表示崩溃的有效统一度量；提出的融合框架能同时处理特征崩溃和模态崩溃；深度信息在保持多模态表示平衡方面具有重要作用；该方法在动作预测任务中取得了显著性能提升。

Abstract: Multi-modal fusion methods often suffer from two types of representation
collapse: feature collapse where individual dimensions lose their
discriminative power (as measured by eigenspectra), and modality collapse where
one dominant modality overwhelms the other. Applications like human action
anticipation that require fusing multifarious sensor data are hindered by both
feature and modality collapse. However, existing methods attempt to counter
feature collapse and modality collapse separately. This is because there is no
unifying framework that efficiently addresses feature and modality collapse in
conjunction. In this paper, we posit the utility of effective rank as an
informative measure that can be utilized to quantify and counter both the
representation collapses. We propose \textit{Rank-enhancing Token Fuser}, a
theoretically grounded fusion framework that selectively blends less
informative features from one modality with complementary features from another
modality. We show that our method increases the effective rank of the fused
representation. To address modality collapse, we evaluate modality combinations
that mutually increase each others' effective rank. We show that depth
maintains representational balance when fused with RGB, avoiding modality
collapse. We validate our method on action anticipation, where we present
\texttt{R3D}, a depth-informed fusion framework. Extensive experiments on
NTURGBD, UTKinect, and DARai demonstrate that our approach significantly
outperforms prior state-of-the-art methods by up to 3.74\%. Our code is
available at:
\href{https://github.com/olivesgatech/R3D}{https://github.com/olivesgatech/R3D}.

</details>


### [82] [EIDSeg: A Pixel-Level Semantic Segmentation Dataset for Post-Earthquake Damage Assessment from Social Media Images](https://arxiv.org/abs/2511.06456)
*Huili Huang,Chengeng Liu,Danrong Zhang,Shail Patel,Anastasiya Masalava,Sagar Sadak,Parisa Babolhavaeji,WeiHong Low,Max Mahdi Roozbahani,J. David Frost*

Main category: cs.CV

TL;DR: 本文介绍了EIDSeg，首个专为地震后社交媒体图像设计的大规模语义分割数据集，包含3,266张图像和5类基础设施损坏标注，并发现Encoder-only Mask Transformer为最佳方法，mIoU达到80.8%。


<details>
  <summary>Details</summary>
Motivation: 快速震后损害评估对救援和资源规划至关重要，但现有遥感方法依赖昂贵的航拍图像和专家标注，且仅生成二值损害图。虽然社交媒体地面级图像可填补这一空白，但该任务仍缺乏大规模像素级标注数据集。

Method: 创建了EIDSeg数据集，包含来自9次重大地震(2008-2023)的3,266张图像，标注了5类基础设施损坏。提出了实用的三阶段跨学科标注协议和标注指南，使非专业标注者能够实现一致的分割，标注间一致率超过70%。并评估了多个最先进的分割模型。

Result: 标注协议实现了超过70%的标注间一致率。Encoder-only Mask Transformer (EoMT)被确定为性能最佳的方法，平均交并比达到80.8%。

Conclusion: 通过EIDSeg数据集释放了社交媒体丰富的地面视角，为震后场景中更快、更细粒度的损害评估铺平了道路，填补了现有方法无法利用社交媒体图像进行精细化损害评估的空白。

Abstract: Rapid post-earthquake damage assessment is crucial for rescue and resource
planning. Still, existing remote sensing methods depend on costly aerial
images, expert labeling, and produce only binary damage maps for early-stage
evaluation. Although ground-level images from social networks provide a
valuable source to fill this gap, a large pixel-level annotated dataset for
this task is still unavailable. We introduce EIDSeg, the first large-scale
semantic segmentation dataset specifically for post-earthquake social media
imagery. The dataset comprises 3,266 images from nine major earthquakes
(2008-2023), annotated across five classes of infrastructure damage: Undamaged
Building, Damaged Building, Destroyed Building, Undamaged Road, and Damaged
Road. We propose a practical three-phase cross-disciplinary annotation protocol
with labeling guidelines that enables consistent segmentation by non-expert
annotators, achieving over 70% inter-annotator agreement. We benchmark several
state-of-the-art segmentation models, identifying Encoder-only Mask Transformer
(EoMT) as the top-performing method with a Mean Intersection over Union (mIoU)
of 80.8%. By unlocking social networks' rich ground-level perspective, our work
paves the way for a faster, finer-grained damage assessment in the
post-earthquake scenario.

</details>


### [83] [Inpaint360GS: Efficient Object-Aware 3D Inpainting via Gaussian Splatting for 360° Scenes](https://arxiv.org/abs/2511.06457)
*Shaoxiang Wang,Shihong Zhang,Christen Millerdurai,Rüdiger Westermann,Didier Stricker,Alain Pagani*

Main category: cs.CV

TL;DR: Inpaint360GS是一个基于3D高斯泼溅的360度场景编辑框架，能够实现多物体移除和高保真度3D修复，在复杂360度场景中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的单物体前向修复技术已有进展，但复杂360度场景修复研究不足，主要面临三大挑战：在360度环境中识别目标物体、处理多物体场景中的严重遮挡、以及在不同视角间保持一致高质量的外观。

Method: 提出Inpaint360GS框架，基于3D高斯泼溅技术，通过将2D分割蒸馏到3D空间，并利用虚拟相机视图提供上下文指导，实现精确的物体级编辑和一致的场景完成。同时构建了专门针对360度修复的新数据集。

Result: 实验证明Inpaint360GS优于现有基线方法，达到了最先进的性能，能够有效处理多物体移除和高质量的3D场景修复任务。

Conclusion: 该研究成功解决了360度复杂场景修复的关键挑战，为多物体编辑和场景补全提供了有效的解决方案，推动了3D场景编辑技术的发展。

Abstract: Despite recent advances in single-object front-facing inpainting using NeRF
and 3D Gaussian Splatting (3DGS), inpainting in complex 360{\deg} scenes
remains largely underexplored. This is primarily due to three key challenges:
(i) identifying target objects in the 3D field of 360{\deg} environments, (ii)
dealing with severe occlusions in multi-object scenes, which makes it hard to
define regions to inpaint, and (iii) maintaining consistent and high-quality
appearance across views effectively. To tackle these challenges, we propose
Inpaint360GS, a flexible 360{\deg} editing framework based on 3DGS that
supports multi-object removal and high-fidelity inpainting in 3D space. By
distilling 2D segmentation into 3D and leveraging virtual camera views for
contextual guidance, our method enables accurate object-level editing and
consistent scene completion. We further introduce a new dataset tailored for
360{\deg} inpainting, addressing the lack of ground truth object-free scenes.
Experiments demonstrate that Inpaint360GS outperforms existing baselines and
achieves state-of-the-art performance. Project page:
https://dfki-av.github.io/inpaint360gs/

</details>


### [84] [NOAH: Benchmarking Narrative Prior driven Hallucination and Omission in Video Large Language Models](https://arxiv.org/abs/2511.06475)
*Kyuho Lee,Euntae Kim,Jinwoo Choi,Buru Chang*

Main category: cs.CV

TL;DR: 本文揭示了视频大语言模型中存在的"叙事先验"偏见会导致幻觉和遗漏错误，并提出了NOAH基准测试来系统性评估这一问题。


<details>
  <summary>Details</summary>
Motivation: 视频大语言模型在字幕生成、摘要和问答等任务上表现优异，但它们强调事件连续性的训练方法引入了一种归纳偏见，即优先考虑故事线一致性而非严格基于视觉证据，这导致了幻觉（引入不存在的事件或错误解释现有事件）和遗漏（抑制与上下文不符的事实事件）两类错误。

Method: 作者构建了NOAH大规模基准测试，通过将来自其他来源的片段插入目标视频来构建复合视频，通过变化语义相似性和插入位置实现可控的可扩展分析。设计了一个字幕生成任务和三个问答任务（存在性、时间性和叙事性），包含超过6万个评估样本。

Result: 三个关键发现：(i) 大多数视频大语言模型表现出由叙事先验驱动的幻觉和遗漏；(ii) 错误模式在不同架构间存在差异，并依赖于事件相似性和插入位置；(iii) 在使用较少帧的采样时，对叙事先验的依赖会加剧，当事件连续性较弱时会放大错误。

Conclusion: NOAH建立了首个针对视频大语言模型中叙事先验诱导的幻觉和遗漏的标准化评估，为开发更可靠、可信赖的模型提供了基础。该基准测试和代码已公开发布。

Abstract: Video large language models (Video LLMs) have recently achieved strong
performance on tasks such as captioning, summarization, and question answering.
Many models and training methods explicitly encourage continuity across events
to enhance narrative coherence. While this improves fluency, it also introduces
an inductive bias that prioritizes storyline consistency over strict grounding
in visual evidence. We identify this bias, which we call narrative prior, as a
key driver of two errors: hallucinations, where non-existent events are
introduced or existing ones are misinterpreted, and omissions, where factual
events are suppressed because they are misaligned with surrounding context. To
systematically evaluate narrative prior-induced errors, we introduce NOAH, a
large-scale benchmark that constructs composite videos by inserting clips from
other sources into target videos. By varying semantic similarity and insertion
position, our benchmark enables controlled and scalable analysis of narrative
priors. We design one captioning task with tailored metrics and three QA tasks
- Existence, Temporal, and Narrative - yielding more than 60K evaluation
samples. Extensive experiments yield three key findings: (i) most Video LLMs
exhibit hallucinations and omissions driven by narrative priors, (ii) the
patterns of these errors vary across architectures and depend on event
similarity and insertion position, and (iii) reliance on narrative priors
intensifies under sampling with fewer frames, amplifying errors when event
continuity is weak. We establish NOAH as the first standardized evaluation of
narrative prior-induced hallucination and omission in Video LLMs, providing a
foundation for developing more reliable and trustworthy models. Our benchmark
and code are available at https://anonymous550520.github.io/.

</details>


### [85] [SpatialThinker: Reinforcing 3D Reasoning in Multimodal LLMs via Spatial Rewards](https://arxiv.org/abs/2511.07403)
*Hunar Batra,Haoqin Tu,Hardy Chen,Yuanze Lin,Cihang Xie,Ronald Clark*

Main category: cs.CV

TL;DR: SpatialThinker是一个3D感知的多模态大语言模型，通过强化学习训练结合结构化空间基础和多步推理，在空间理解任务上显著超越现有方法，甚至超过GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在空间理解方面存在严重不足，依赖显式3D输入或架构特定修改，且受限于大规模数据集或稀疏监督的约束。

Method: 提出SpatialThinker模型，通过构建任务相关对象的场景图和空间关系，结合多目标密集空间奖励的在线强化学习训练。包含两个关键贡献：数据合成管道生成STVQA-7K数据集，以及强制执行空间基础的在线RL训练。

Result: SpatialThinker-7B在空间理解和真实世界VQA基准测试中表现优异，相比稀疏RL基线实现近双倍的模型增益，并超越GPT-4o的表现。

Conclusion: 证明了将空间监督与奖励对齐推理相结合的有效性，能够在有限数据下实现鲁棒的3D空间理解，推动多模态大语言模型向人类级视觉推理发展。

Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress in
vision-language tasks, but they continue to struggle with spatial
understanding. Existing spatial MLLMs often rely on explicit 3D inputs or
architecture-specific modifications, and remain constrained by large-scale
datasets or sparse supervision. To address these limitations, we introduce
SpatialThinker, a 3D-aware MLLM trained with RL to integrate structured spatial
grounding with multi-step reasoning. The model simulates human-like spatial
perception by constructing a scene graph of task-relevant objects and spatial
relations, and reasoning towards an answer via dense spatial rewards.
SpatialThinker consists of two key contributions: (1) a data synthesis pipeline
that generates STVQA-7K, a high-quality spatial VQA dataset, and (2) online RL
with a multi-objective dense spatial reward enforcing spatial grounding.
SpatialThinker-7B outperforms supervised fine-tuning and the sparse RL baseline
on spatial understanding and real-world VQA benchmarks, nearly doubling the
base-model gain compared to sparse RL, and surpassing GPT-4o. These results
showcase the effectiveness of combining spatial supervision with reward-aligned
reasoning in enabling robust 3D spatial understanding with limited data and
advancing MLLMs towards human-level visual reasoning.

</details>


### [86] [Zooming into Comics: Region-Aware RL Improves Fine-Grained Comic Understanding in Vision-Language Models](https://arxiv.org/abs/2511.06490)
*Yule Chen,Yufan Ren,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 本文提出了AI4VA-FG基准测试来评估VLM在漫画理解上的表现，并开发了RARL方法来提升模型性能。实验显示现有模型在漫画理解上存在显著不足，而提出的RL和RARL策略能显著改善Qwen2.5-VL在基础识别和高级故事排序任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型(VLMs)在自然图像上表现出色，但在理解复杂视觉叙事(如漫画)时面临重大挑战。漫画的线条艺术风格、拟声词和密集多面板布局使VLM难以处理，现有模型在这一领域的能力不足。

Method: 1) 构建AI4VA-FG基准测试：涵盖从基础识别检测到高级角色推理和叙事构建的细粒度评估，包含角色、姿态和深度的密集标注。2) 系统研究后训练策略：包括基于解决方案的监督微调(SFT-S)、基于推理轨迹的监督微调(SFT-R)和强化学习(RL)。3) 提出区域感知强化学习(RARL)：受"用图像思考"范式启发，训练模型通过缩放操作动态关注相关区域。

Result: 评估了GPT-4o、Gemini-2.5和Qwen2.5-VL等最先进模型，发现它们在基准测试的核心任务上存在显著性能缺陷，证明漫画理解仍是未解决的挑战。当将RL和RARL应用于Qwen2.5-VL时，在低级实体识别和高级故事线排序任务上获得了显著提升。

Conclusion: 漫画理解对VLMs仍是一个重大挑战，本文提出的基准测试和训练策略为改进提供了有效途径。特别是RARL方法通过区域感知的强化学习显著提升了模型在漫画理解任务上的性能，为更准确高效的VLM漫画应用铺平了道路。

Abstract: Complex visual narratives, such as comics, present a significant challenge to
Vision-Language Models (VLMs). Despite excelling on natural images, VLMs often
struggle with stylized line art, onomatopoeia, and densely packed multi-panel
layouts. To address this gap, we introduce AI4VA-FG, the first fine-grained and
comprehensive benchmark for VLM-based comic understanding. It spans tasks from
foundational recognition and detection to high-level character reasoning and
narrative construction, supported by dense annotations for characters, poses,
and depth. Beyond that, we evaluate state-of-the-art proprietary models,
including GPT-4o and Gemini-2.5, and open-source models such as Qwen2.5-VL,
revealing substantial performance deficits across core tasks of our benchmarks
and underscoring that comic understanding remains an unsolved challenge. To
enhance VLMs' capabilities in this domain, we systematically investigate
post-training strategies, including supervised fine-tuning on solutions
(SFT-S), supervised fine-tuning on reasoning trajectories (SFT-R), and
reinforcement learning (RL). Beyond that, inspired by the emerging "Thinking
with Images" paradigm, we propose Region-Aware Reinforcement Learning (RARL)
for VLMs, which trains models to dynamically attend to relevant regions through
zoom-in operations. We observe that when applied to the Qwen2.5-VL model, RL
and RARL yield significant gains in low-level entity recognition and high-level
storyline ordering, paving the way for more accurate and efficient VLM
applications in the comics domain.

</details>


### [87] [SportR: A Benchmark for Multimodal Large Language Model Reasoning in Sports](https://arxiv.org/abs/2511.06499)
*Haotian Xia,Haonan Ge,Junbo Zou,Hyun Woo Choi,Xuebin Zhang,Danny Suradja,Botao Rui,Ethan Tran,Wendy Jin,Zhen Ye,Xiyang Lin,Christopher Lai,Shengjie Zhang,Junwen Miao,Shichao Chen,Rhys Tracy,Vicente Ordonez,Weining Shen,Hanjie Chen*

Main category: cs.CV

TL;DR: SportR是首个多模态运动推理基准数据集，包含5,017张图像和2,101个视频，通过渐进式问题层次结构和思维链注释，评估模型在运动视觉感知和规则推理方面的能力，揭示了当前模型在该领域的显著差距。


<details>
  <summary>Details</summary>
Motivation: 深度理解运动需要精细视觉感知和基于规则的推理能力，这对当前多模态模型构成挑战。现有运动基准要么仅涵盖单一运动，要么缺乏在多运动环境下稳健评估核心能力所需的详细推理链和精确视觉定位。

Method: 引入SportR基准数据集，包含5,017张图像和2,101个视频。采用渐进式问题-答案对层次结构，从简单违规识别到复杂处罚预测；为多步推理任务提供7,118条人工编写的思维链注释；同时整合图像和视频模态，并提供边界框注释测试视觉定位能力。

Result: 实验表明该基准极具挑战性，最先进的基线模型在最具挑战性的任务上表现不佳。尽管通过监督微调和强化学习训练后性能有所提升，但分数仍然相对较低，突显了当前模型能力的显著差距。

Conclusion: SportR为社区提供了新的挑战，是推动多模态运动推理未来研究的关键资源，有助于填补当前模型在运动智能方面的能力缺口。

Abstract: Deeply understanding sports requires an intricate blend of fine-grained
visual perception and rule-based reasoning - a challenge that pushes the limits
of current multimodal models. To succeed, models must master three critical
capabilities: perceiving nuanced visual details, applying abstract sport rule
knowledge, and grounding that knowledge in specific visual evidence. Current
sports benchmarks either cover single sports or lack the detailed reasoning
chains and precise visual grounding needed to robustly evaluate these core
capabilities in a multi-sport context. To address this gap, we introduce
SportR, the first multi-sports large-scale benchmark designed to train and
evaluate MLLMs on the fundamental reasoning required for sports intelligence.
Our benchmark provides a dataset of 5,017 images and 2,101 videos. To enable
granular evaluation, we structure our benchmark around a progressive hierarchy
of question-answer (QA) pairs designed to probe reasoning at increasing depths
- from simple infraction identification to complex penalty prediction. For the
most advanced tasks requiring multi-step reasoning, such as determining
penalties or explaining tactics, we provide 7,118 high-quality, human-authored
Chain of Thought (CoT) annotations. In addition, our benchmark incorporates
both image and video modalities and provides manual bounding box annotations to
test visual grounding in the image part directly. Extensive experiments
demonstrate the profound difficulty of our benchmark. State-of-the-art baseline
models perform poorly on our most challenging tasks. While training on our data
via Supervised Fine-Tuning and Reinforcement Learning improves these scores,
they remain relatively low, highlighting a significant gap in current model
capabilities. SportR presents a new challenge for the community, providing a
critical resource to drive future research in multimodal sports reasoning.

</details>


### [88] [Video Dataset for Surgical Phase, Keypoint, and Instrument Recognition in Laparoscopic Surgery (PhaKIR)](https://arxiv.org/abs/2511.06549)
*Tobias Rueckert,Raphaela Maerkl,David Rauber,Leonard Klausmann,Max Gutbrod,Daniel Rueckert,Hubertus Feussner,Dirk Wilhelm,Christoph Palm*

Main category: cs.CV

TL;DR: PhaKIR是一个多中心腹腔镜胆囊切除术视频数据集，提供了手术阶段识别、器械关键点估计和器械实例分割三个任务的联合标注。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人辅助微创手术数据集存在任务孤立、忽略时间依赖性、缺乏多中心变异性等问题，限制了计算机视觉方法在器械识别和手术流程理解中的发展。

Method: 收集了三个医疗中心的8个完整腹腔镜胆囊切除术视频，提供帧级标注：手术阶段识别(485,875帧)、器械关键点估计(19,435帧)和器械实例分割(19,435帧)。

Result: 创建了首个同时提供阶段标签、器械姿态信息和像素级器械分割的多机构数据集，支持时间上下文利用，并在MICCAI 2024 EndoVis挑战赛中得到验证。

Conclusion: PhaKIR数据集通过提供多任务、多中心、时序完整的标注资源，推动了手术场景理解方法的发展，数据集已通过Zenodo平台公开发布。

Abstract: Robotic- and computer-assisted minimally invasive surgery (RAMIS) is
increasingly relying on computer vision methods for reliable instrument
recognition and surgical workflow understanding. Developing such systems often
requires large, well-annotated datasets, but existing resources often address
isolated tasks, neglect temporal dependencies, or lack multi-center
variability. We present the Surgical Procedure Phase, Keypoint, and Instrument
Recognition (PhaKIR) dataset, comprising eight complete laparoscopic
cholecystectomy videos recorded at three medical centers. The dataset provides
frame-level annotations for three interconnected tasks: surgical phase
recognition (485,875 frames), instrument keypoint estimation (19,435 frames),
and instrument instance segmentation (19,435 frames). PhaKIR is, to our
knowledge, the first multi-institutional dataset to jointly provide phase
labels, instrument pose information, and pixel-accurate instrument
segmentations, while also enabling the exploitation of temporal context since
full surgical procedure sequences are available. It served as the basis for the
PhaKIR Challenge as part of the Endoscopic Vision (EndoVis) Challenge at MICCAI
2024 to benchmark methods in surgical scene understanding, thereby further
validating the dataset's quality and relevance. The dataset is publicly
available upon request via the Zenodo platform.

</details>


### [89] [Spatial-Frequency Enhanced Mamba for Multi-Modal Image Fusion](https://arxiv.org/abs/2511.06593)
*Hui Sun,Long Lv,Pingping Zhang,Tongdan Tang,Feng Tian,Weibing Sun,Huchuan Lu*

Main category: cs.CV

TL;DR: 本文提出了一种名为SFMFusion的新型多模态图像融合框架，通过空间-频率增强的Mamba块和动态融合机制，结合图像重建辅助任务，在保持计算效率的同时显著提升了融合效果。


<details>
  <summary>Details</summary>
Motivation: 现有的基于CNN的多模态图像融合方法感受野有限，而基于Transformer的方法计算成本过高。虽然Mamba能够以线性复杂度建模长距离依赖，但缺乏完整的空间和频率感知能力，而这些对MMIF至关重要。此外，如何高效有效地利用图像重建作为辅助任务也是一个主要挑战。

Method: 提出SFMFusion框架，包含三个核心组件：1）三分支结构耦合MMIF和图像重建任务，保留源图像完整内容；2）空间-频率增强Mamba块(SFMB)，在空间和频率域增强Mamba的特征提取能力；3）动态融合Mamba块(DFMB)，跨不同分支实现动态特征融合。

Result: 在六个MMIF数据集上的广泛实验表明，该方法优于大多数最先进方法，取得了更好的融合效果。源代码已在GitHub上公开。

Conclusion: SFMFusion成功解决了传统MMIF方法的局限性，通过结合空间-频率增强和动态融合机制，在保持计算效率的同时实现了更优的多模态图像融合性能，为MMIF领域提供了新的技术路径。

Abstract: Multi-Modal Image Fusion (MMIF) aims to integrate complementary image
information from different modalities to produce informative images. Previous
deep learning-based MMIF methods generally adopt Convolutional Neural Networks
(CNNs) or Transformers for feature extraction. However, these methods deliver
unsatisfactory performances due to the limited receptive field of CNNs and the
high computational cost of Transformers. Recently, Mamba has demonstrated a
powerful potential for modeling long-range dependencies with linear complexity,
providing a promising solution to MMIF. Unfortunately, Mamba lacks full spatial
and frequency perceptions, which are very important for MMIF. Moreover,
employing Image Reconstruction (IR) as an auxiliary task has been proven
beneficial for MMIF. However, a primary challenge is how to leverage IR
efficiently and effectively. To address the above issues, we propose a novel
framework named Spatial-Frequency Enhanced Mamba Fusion (SFMFusion) for MMIF.
More specifically, we first propose a three-branch structure to couple MMIF and
IR, which can retain complete contents from source images. Then, we propose the
Spatial-Frequency Enhanced Mamba Block (SFMB), which can enhance Mamba in both
spatial and frequency domains for comprehensive feature extraction. Finally, we
propose the Dynamic Fusion Mamba Block (DFMB), which can be deployed across
different branches for dynamic feature fusion. Extensive experiments show that
our method achieves better results than most state-of-the-art methods on six
MMIF datasets. The source code is available at
https://github.com/SunHui1216/SFMFusion.

</details>


### [90] [On Accurate and Robust Estimation of 3D and 2D Circular Center: Method and Application to Camera-Lidar Calibration](https://arxiv.org/abs/2511.06611)
*Jiajun Jiang,Xiao Hu,Wancheng Liu,Wei Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种基于几何原理的框架，通过共形几何代数和RANSAC实现鲁棒的3D圆心估计，并使用弦长方差最小化方法恢复真实2D投影中心，显著提升了LiDAR-相机外参标定的精度。


<details>
  <summary>Details</summary>
Motivation: 圆形目标在LiDAR-相机外参标定中广泛应用，但实现精确的3D-2D圆心对应关系仍具挑战性。现有方法由于3D拟合与2D椭圆中心估计的解耦以及错误的2D椭圆中心估计而经常失败。

Method: 提出了两个创新点：(i) 基于共形几何代数和RANSAC的鲁棒3D圆心估计器；(ii) 弦长方差最小化方法来恢复真实2D投影中心，通过单应性验证或准RANSAC回退解决其双最小值模糊性。

Result: 在合成和真实世界数据集上的评估表明，该框架显著优于现有最先进方法，减少了外参估计误差，能够在多种传感器和目标类型（包括天然圆形物体）上实现鲁棒标定。

Conclusion: 该方法在LiDAR-相机外参标定领域取得了重要突破，将公开发布代码以确保可复现性，为相关研究和应用提供了实用解决方案。

Abstract: Circular targets are widely used in LiDAR-camera extrinsic calibration due to
their geometric consistency and ease of detection. However, achieving accurate
3D-2D circular center correspondence remains challenging. Existing methods
often fail due to decoupled 3D fitting and erroneous 2D ellipse-center
estimation. To address this, we propose a geometrically principled framework
featuring two innovations: (i) a robust 3D circle center estimator based on
conformal geometric algebra and RANSAC; and (ii) a chord-length variance
minimization method to recover the true 2D projected center, resolving its
dual-minima ambi- guity via homography validation or a quasi-RANSAC fallback.
Evaluated on synthetic and real-world datasets, our framework significantly
outperforms state-of-the-art approaches. It reduces extrinsic estimation error
and enables robust calibration across diverse sensors and target types,
including natural circular objects. Our code will be publicly released for
reproducibility.

</details>


### [91] [Explainable Cross-Disease Reasoning for Cardiovascular Risk Assessment from LDCT](https://arxiv.org/abs/2511.06625)
*Yifei Zhang,Jiashuo Zhang,Xiaofeng Yang,Liang Zhao*

Main category: cs.CV

TL;DR: 提出一种可解释的跨疾病推理框架，通过单次低剂量CT扫描实现心肺联合评估，准确预测心血管风险并提供医学可解释的推理过程。


<details>
  <summary>Details</summary>
Motivation: 低剂量胸部CT扫描能同时捕捉肺和心脏结构，但现有方法将这些领域视为独立任务，忽略了它们之间的生理相互作用和共享影像生物标志物。

Method: 引入智能体推理过程，模拟临床诊断思维：首先感知肺部发现，然后通过医学知识推理，最后得出心血管判断并解释原因。包含三个协同组件：肺部感知模块、知识引导推理模块和心脏表征模块。

Result: 在NLST队列上实验表明，该框架在心血管疾病筛查和死亡率预测方面达到最先进性能，优于单疾病和纯基于图像的基线方法。

Conclusion: 建立了从LDCT进行心血管分析的统一可解释范式，弥合了基于图像的预测与基于机制的医学解释之间的差距，提供了与心脏病学理解一致的人工可验证推理。

Abstract: Low-dose chest computed tomography (LDCT) inherently captures both pulmonary
and cardiac structures, offering a unique opportunity for joint assessment of
lung and cardiovascular health. However, most existing approaches treat these
domains as independent tasks, overlooking their physiological interplay and
shared imaging biomarkers. We propose an Explainable Cross-Disease Reasoning
Framework that enables interpretable cardiopulmonary risk assessment from a
single LDCT scan. The framework introduces an agentic reasoning process that
emulates clinical diagnostic thinking-first perceiving pulmonary findings, then
reasoning through established medical knowledge, and finally deriving a
cardiovascular judgment with explanatory rationale. It integrates three
synergistic components: a pulmonary perception module that summarizes lung
abnormalities, a knowledge-guided reasoning module that infers their
cardiovascular implications, and a cardiac representation module that encodes
structural biomarkers. Their outputs are fused to produce a holistic
cardiovascular risk prediction that is both accurate and physiologically
grounded. Experiments on the NLST cohort demonstrate that the proposed
framework achieves state-of-the-art performance for CVD screening and mortality
prediction, outperforming single-disease and purely image-based baselines.
Beyond quantitative gains, the framework provides human-verifiable reasoning
that aligns with cardiological understanding, revealing coherent links between
pulmonary abnormalities and cardiac stress mechanisms. Overall, this work
establishes a unified and explainable paradigm for cardiovascular analysis from
LDCT, bridging the gap between image-based prediction and mechanism-based
medical interpretation.

</details>


### [92] [DIAL-GS: Dynamic Instance Aware Reconstruction for Label-free Street Scenes with 4D Gaussian Splatting](https://arxiv.org/abs/2511.06632)
*Chenpeng Su,Wenhua Wu,Chensheng Peng,Tianchen Deng,Zhe Liu,Hesheng Wang*

Main category: cs.CV

TL;DR: DIAL-GS是一种基于4D高斯泼溅技术的动态实例感知城市场景重建方法，无需人工标注即可实现精细的街景建模和实例级编辑。


<details>
  <summary>Details</summary>
Motivation: 现有监督方法依赖昂贵人工标注且缺乏可扩展性，而自监督方法常混淆静态和动态元素，无法区分独立动态对象，限制了精细编辑能力，因此需要更有效的无标签动态场景重建方案。

Method: 通过分析扭曲渲染与实际观测的外观-位置不一致性识别动态实例，采用实例感知的4D高斯作为统一体积表示，并引入身份与动力学相互增强的交互机制，实现动态自适应和实例感知的重建。

Result: 在城市驾驶场景实验中，DIAL-GS在重建质量和实例级编辑方面超越了现有自监督基线方法，验证了该方法的有效性和优越性。

Conclusion: DIAL-GS为城市场景建模提供了一个简洁而强大的解决方案，实现了高质量的动态实例感知重建，为自动驾驶应用中的数据合成和闭环测试提供了重要技术支撑。

Abstract: Urban scene reconstruction is critical for autonomous driving, enabling
structured 3D representations for data synthesis and closed-loop testing.
Supervised approaches rely on costly human annotations and lack scalability,
while current self-supervised methods often confuse static and dynamic elements
and fail to distinguish individual dynamic objects, limiting fine-grained
editing. We propose DIAL-GS, a novel dynamic instance-aware reconstruction
method for label-free street scenes with 4D Gaussian Splatting. We first
accurately identify dynamic instances by exploiting appearance-position
inconsistency between warped rendering and actual observation. Guided by
instance-level dynamic perception, we employ instance-aware 4D Gaussians as the
unified volumetric representation, realizing dynamic-adaptive and
instance-aware reconstruction. Furthermore, we introduce a reciprocal mechanism
through which identity and dynamics reinforce each other, enhancing both
integrity and consistency. Experiments on urban driving scenarios show that
DIAL-GS surpasses existing self-supervised baselines in reconstruction quality
and instance-level editing, offering a concise yet powerful solution for urban
scene modeling.

</details>


### [93] [FreqGRL: Suppressing Low-Frequency Bias and Mining High-Frequency Knowledge for Cross-Domain Few-Shot Learning](https://arxiv.org/abs/2511.06648)
*Siqi Hui,Sanping Zhou,Ye deng,Wenli Huang,Jinjun Wang*

Main category: cs.CV

TL;DR: 本文提出了FreqGRL框架，首次从频域视角解决跨域小样本学习中源域与目标域数据不平衡问题，通过低频替换、高频增强和全局频域过滤三个模块实现最优性能。


<details>
  <summary>Details</summary>
Motivation: 跨域小样本学习面临两个关键挑战：(1)模型容易偏向源域数据低频分量中编码的源域特定知识；(2)目标域数据的稀疏性阻碍了高频、域通用特征的学习。源域丰富数据与目标域稀缺数据之间的严重不平衡是有效表征学习的核心障碍。

Method: 提出了FreqGRL框架，包含三个核心模块：(1)低频替换(LFR)模块：用目标域的低频分量替换源域任务的低频分量，创建与目标特征更一致的新源任务；(2)高频增强(HFE)模块：过滤低频分量，直接在频域的高频特征上进行学习；(3)全局频域滤波(GFF)模块：抑制噪声或无关频率，强调信息丰富的频率，减少过拟合风险。

Result: 在五个标准CD-FSL基准测试上的广泛实验表明，该频域引导框架达到了最先进的性能，有效解决了数据不平衡问题并提升了跨域泛化能力。

Conclusion: 本文创新性地从频域角度分析并解决CD-FSL中的数据不平衡问题，通过精心设计的频域操作模块，成功减少了源域特定偏差，增强了域通用特征学习，为目标监督有限情况下的跨域学习提供了新的解决思路。

Abstract: Cross-domain few-shot learning (CD-FSL) aims to recognize novel classes with
only a few labeled examples under significant domain shifts. While recent
approaches leverage a limited amount of labeled target-domain data to improve
performance, the severe imbalance between abundant source data and scarce
target data remains a critical challenge for effective representation learning.
We present the first frequency-space perspective to analyze this issue and
identify two key challenges: (1) models are easily biased toward
source-specific knowledge encoded in the low-frequency components of source
data, and (2) the sparsity of target data hinders the learning of
high-frequency, domain-generalizable features. To address these challenges, we
propose \textbf{FreqGRL}, a novel CD-FSL framework that mitigates the impact of
data imbalance in the frequency space. Specifically, we introduce a
Low-Frequency Replacement (LFR) module that substitutes the low-frequency
components of source tasks with those from the target domain to create new
source tasks that better align with target characteristics, thus reducing
source-specific biases and promoting generalizable representation learning. We
further design a High-Frequency Enhancement (HFE) module that filters out
low-frequency components and performs learning directly on high-frequency
features in the frequency space to improve cross-domain generalization.
Additionally, a Global Frequency Filter (GFF) is incorporated to suppress noisy
or irrelevant frequencies and emphasize informative ones, mitigating
overfitting risks under limited target supervision. Extensive experiments on
five standard CD-FSL benchmarks demonstrate that our frequency-guided framework
achieves state-of-the-art performance.

</details>


### [94] [NOVO: Bridging LLaVA and SAM with Visual-only Prompts for Reasoning Segmentation](https://arxiv.org/abs/2511.06651)
*Kyung-Yoon Yoon,Yeong-Jun Cho*

Main category: cs.CV

TL;DR: NOVO是一个仅使用视觉提示的推理分割框架，通过从VLM生成粗糙掩码和点提示，结合SAM进行分割，并提出训练无关的细化模块和RISeg基准数据集，实现了最先进的推理分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将文本衍生的SEG词元嵌入输入分割模型，但这种做法与预训练的分割模型（如SAM）不兼容，需要重新训练。作者希望开发一个无需重新训练、能保持与预训练模型对齐的推理分割解决方案。

Method: NOVO框架从视觉语言模型输出生成粗糙掩码和点提示，这些视觉提示与SAM兼容；引入训练无关的细化模块提升边界质量和实例级分割；构建包含918张图像和2533个实例掩码的RISeg基准数据集。

Result: NOVO在多个指标和不同模型规模上都达到了最先进的性能，证明了该方法在推理分割任务上的有效性和可扩展性。

Conclusion: 通过仅使用视觉提示的方式，NOVO成功桥接了视觉语言模型和分割模型，避免了重新训练的需要，同时保持了与预训练模型的兼容性，为推理分割任务提供了新的有效解决方案。

Abstract: In this study, we propose NOVO (NO text, Visual-Only prompts), a novel
framework that bridges vision-language models (VLMs) and segmentation models
through visual-only prompts. Unlike prior approaches that feed text-derived SEG
token embeddings into segmentation models, NOVO instead generates a coarse mask
and point prompts from the VLM output. These visual prompts are compatible with
the Segment Anything Model (SAM), preserving alignment with its pretrained
capabilities. To further enhance boundary quality and enable instance-level
segmentation, we introduce a training-free refinement module that reduces
visual artifacts and improves the quality of segmentation masks. We also
present RISeg, a new benchmark comprising 918 images, 2,533 instance-level
masks, and diverse reasoning queries to evaluate this task. Experiments
demonstrate that NOVO achieves state-of-the-art performance across multiple
metrics and model sizes, demonstrating its effectiveness and scalability in
reasoning segmentation.

</details>


### [95] [Active Learning for Animal Re-Identification with Ambiguity-Aware Sampling](https://arxiv.org/abs/2511.06658)
*Depanshu Sani,Mehar Khurana,Saket Anand*

Main category: cs.CV

TL;DR: 提出一种新颖的主动学习动物重识别框架，仅需0.033%的标注数据就在13个野生动物数据集上超越基础模型、无监督学习和主动学习方法平均10.49%、11.19%和3.99%的mAP性能。


<details>
  <summary>Details</summary>
Motivation: 动物重识别对生物多样性监测至关重要，但面临细微区分模式、新物种处理和开放集本质等挑战。现有基础模型在零样本重识别上存在显著性能差距，而完整的Re-ID标注既耗时又需要领域专业知识，现有无监督和主动学习方法在动物Re-ID上表现不佳。

Method: 提出互补聚类方法的主动学习框架，通过挖掘嵌入空间中结构模糊区域的样本对，利用"必须链接"和"不能链接"约束的Oracle反馈，通过约束聚类细化算法与现有无监督方法自然集成。

Result: 在13个野生动物数据集上，仅使用0.033%的标注数据，分别超越基础模型、无监督和主动学习方法10.49%、11.19%和3.99%的mAP，在开放世界设置下对未知个体也有11.09%、8.2%和2.06%的性能提升。

Conclusion: 该主动学习框架以极少标注成本实现了动物重识别的当前最佳性能，有效解决了动物Re-ID面临的标注困难和开放集挑战。

Abstract: Animal Re-ID has recently gained substantial attention in the AI research
community due to its high impact on biodiversity monitoring and unique research
challenges arising from environmental factors. The subtle distinguishing
patterns, handling new species and the inherent open-set nature make the
problem even harder. To address these complexities, foundation models trained
on labeled, large-scale and multi-species animal Re-ID datasets have recently
been introduced to enable zero-shot Re-ID. However, our benchmarking reveals
significant gaps in their zero-shot Re-ID performance for both known and
unknown species. While this highlights the need for collecting labeled data in
new domains, exhaustive annotation for Re-ID is laborious and requires domain
expertise. Our analyses show that existing unsupervised (USL) and AL Re-ID
methods underperform for animal Re-ID. To address these limitations, we
introduce a novel AL Re-ID framework that leverages complementary clustering
methods to uncover and target structurally ambiguous regions in the embedding
space for mining pairs of samples that are both informative and broadly
representative. Oracle feedback on these pairs, in the form of must-link and
cannot-link constraints, facilitates a simple annotation interface, which
naturally integrates with existing USL methods through our proposed constrained
clustering refinement algorithm. Through extensive experiments, we demonstrate
that, by utilizing only 0.033% of all annotations, our approach consistently
outperforms existing foundational, USL and AL baselines. Specifically, we
report an average improvement of 10.49%, 11.19% and 3.99% (mAP) on 13 wildlife
datasets over foundational, USL and AL methods, respectively, while attaining
state-of-the-art performance on each dataset. Furthermore, we also show an
improvement of 11.09%, 8.2% and 2.06% for unknown individuals in an open-world
setting.

</details>


### [96] [Sim4Seg: Boosting Multimodal Multi-disease Medical Diagnosis Segmentation with Region-Aware Vision-Language Similarity Masks](https://arxiv.org/abs/2511.06665)
*Lingran Song,Yucheng Zhou,Jianbing Shen*

Main category: cs.CV

TL;DR: 本文提出了一种新的医学视觉语言任务Medical Diagnosis Segmentation (MDS)，创建了M3DS数据集，并设计了Sim4Seg框架来联合实现医学图像分割和诊断任务。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割模型很少将分割和诊断任务联合进行，但为患者提供可解释的诊断结果至关重要。需要开发能够同时生成分割掩码和诊断结果的模型。

Method: 1. 提出MDS任务：理解临床查询并生成对应分割掩码和诊断结果；2. 构建M3DS数据集：包含多模态多疾病医学图像、分割掩码和诊断思维链；3. 设计Sim4Seg框架：利用RVLS2M模块提升区域感知的视觉语言相似性；4. 研究测试时缩放策略。

Result: 实验结果表明，该方法在分割和诊断任务上都优于基线方法。

Conclusion: 通过联合分割和诊断任务，结合多模态数据和区域感知的视觉语言相似性技术，能够有效提升医学图像分析的性能和可解释性。

Abstract: Despite significant progress in pixel-level medical image analysis, existing
medical image segmentation models rarely explore medical segmentation and
diagnosis tasks jointly. However, it is crucial for patients that models can
provide explainable diagnoses along with medical segmentation results. In this
paper, we introduce a medical vision-language task named Medical Diagnosis
Segmentation (MDS), which aims to understand clinical queries for medical
images and generate the corresponding segmentation masks as well as diagnostic
results. To facilitate this task, we first present the Multimodal Multi-disease
Medical Diagnosis Segmentation (M3DS) dataset, containing diverse multimodal
multi-disease medical images paired with their corresponding segmentation masks
and diagnosis chain-of-thought, created via an automated diagnosis
chain-of-thought generation pipeline. Moreover, we propose Sim4Seg, a novel
framework that improves the performance of diagnosis segmentation by taking
advantage of the Region-Aware Vision-Language Similarity to Mask (RVLS2M)
module. To improve overall performance, we investigate a test-time scaling
strategy for MDS tasks. Experimental results demonstrate that our method
outperforms the baselines in both segmentation and diagnosis.

</details>


### [97] [AnoStyler: Text-Driven Localized Anomaly Generation via Lightweight Style Transfer](https://arxiv.org/abs/2511.06687)
*Yulim So,Seokho Kang*

Main category: cs.CV

TL;DR: AnoStyler是一种轻量级的零样本异常生成方法，通过文本引导的风格转移技术，使用单张正常图像即可生成视觉逼真、多样化的异常图像，有效解决了现有方法在视觉真实感、数据依赖性和计算效率方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的异常生成方法普遍存在三个关键问题：生成的异常图像视觉真实感不足、依赖大量真实图像、使用内存密集的重量级模型架构，这些问题严重阻碍了它们在实际应用中的部署。

Method: AnoStyler将零样本异常生成问题建模为文本引导的风格转移任务。该方法首先基于单张正常图像及其类别标签和预期缺陷类型，通过可泛化的类别无关程序生成异常掩码和代表正常/异常状态的双类文本提示，然后使用轻量级U-Net模型配合基于CLIP的损失函数，将正常图像风格转换为视觉逼真的异常图像。

Result: 在MVTec-AD和VisA数据集上的广泛实验表明，AnoStyler在生成高质量和多样化的异常图像方面优于现有的异常生成方法，并且使用这些生成的异常图像能有效提升异常检测的性能。

Conclusion: AnoStyler成功解决了现有异常生成方法的三个主要局限性，通过轻量级架构和文本引导的风格转移技术实现了高效、逼真的零样本异常生成，为实际应用中的异常检测任务提供了有价值的解决方案。

Abstract: Anomaly generation has been widely explored to address the scarcity of
anomaly images in real-world data. However, existing methods typically suffer
from at least one of the following limitations, hindering their practical
deployment: (1) lack of visual realism in generated anomalies; (2) dependence
on large amounts of real images; and (3) use of memory-intensive, heavyweight
model architectures. To overcome these limitations, we propose AnoStyler, a
lightweight yet effective method that frames zero-shot anomaly generation as
text-guided style transfer. Given a single normal image along with its category
label and expected defect type, an anomaly mask indicating the localized
anomaly regions and two-class text prompts representing the normal and anomaly
states are generated using generalizable category-agnostic procedures. A
lightweight U-Net model trained with CLIP-based loss functions is used to
stylize the normal image into a visually realistic anomaly image, where
anomalies are localized by the anomaly mask and semantically aligned with the
text prompts. Extensive experiments on the MVTec-AD and VisA datasets show that
AnoStyler outperforms existing anomaly generation methods in generating
high-quality and diverse anomaly images. Furthermore, using these generated
anomalies helps enhance anomaly detection performance.

</details>


### [98] [SPAN: Spatial-Projection Alignment for Monocular 3D Object Detection](https://arxiv.org/abs/2511.06702)
*Yifan Wang,Yian Zhao,Fanqi Pu,Xiaochen Yang,Yang Tang,Xi Chen,Wenming Yang*

Main category: cs.CV

TL;DR: 本文提出SPAN方法，通过空间点对齐和3D-2D投影对齐解决单目3D检测中解耦预测导致的几何一致性问题，并采用分层任务学习确保训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有单目3D检测器采用解耦预测范式分别估计几何中心、深度、尺寸和旋转角，虽然简化了学习过程，但忽略了不同属性间的几何协作约束，缺乏几何一致性先验，导致性能次优。

Method: 提出SPAN方法包含两个核心组件：(1)空间点对齐强制预测3D框与真实框间的全局空间约束，纠正解耦回归造成的空间漂移；(2)3D-2D投影对齐确保投影3D框与2D检测框紧密对齐，缓解投影失配问题；(3)分层任务学习策略渐进引入空间投影对齐，防止早期阶段属性间误差传播。

Result: 通过大量实验证明，该方法可轻松集成到任何现有单目3D检测器中，并带来显著的性能提升。

Conclusion: SPAN通过引入几何协作约束和一致性先验，有效解决了单目3D检测中解耦预测的固有缺陷，为提升检测性能提供了新的思路。

Abstract: Existing monocular 3D detectors typically tame the pronounced nonlinear
regression of 3D bounding box through decoupled prediction paradigm, which
employs multiple branches to estimate geometric center, depth, dimensions, and
rotation angle separately. Although this decoupling strategy simplifies the
learning process, it inherently ignores the geometric collaborative constraints
between different attributes, resulting in the lack of geometric consistency
prior, thereby leading to suboptimal performance. To address this issue, we
propose novel Spatial-Projection Alignment (SPAN) with two pivotal components:
(i). Spatial Point Alignment enforces an explicit global spatial constraint
between the predicted and ground-truth 3D bounding boxes, thereby rectifying
spatial drift caused by decoupled attribute regression. (ii). 3D-2D Projection
Alignment ensures that the projected 3D box is aligned tightly within its
corresponding 2D detection bounding box on the image plane, mitigating
projection misalignment overlooked in previous works. To ensure training
stability, we further introduce a Hierarchical Task Learning strategy that
progressively incorporates spatial-projection alignment as 3D attribute
predictions refine, preventing early stage error propagation across attributes.
Extensive experiments demonstrate that the proposed method can be easily
integrated into any established monocular 3D detector and delivers significant
performance improvements.

</details>


### [99] [K-Stain: Keypoint-Driven Correspondence for H&E-to-IHC Virtual Staining](https://arxiv.org/abs/2511.06709)
*Sicheng Yang,Zhaohu Xing,Haipeng Zhou,Lei Zhu*

Main category: cs.CV

TL;DR: K-Stain是一个基于关键点的虚拟染色框架，通过利用空间对应关系将H&E图像转换为高质量的IHC图像，有效解决了组织切片错位问题。


<details>
  <summary>Details</summary>
Motivation: 现有的虚拟染色方法在处理H&E到IHC图像转换时，由于组织切片间的错位问题，难以有效利用空间信息，限制了合成图像的准确性和视觉一致性。

Method: 提出K-Stain框架，包含三个核心组件：(1)分层空间关键点检测器(HSKD)用于识别染色图像中的关键点；(2)关键点感知增强生成器(KEG)在图像生成过程中集成关键点信息；(3)关键点引导判别器(KGD)提高判别器对空间细节的敏感性。

Result: 实验结果表明，K-Stain在定量指标和视觉质量方面均优于最先进的方法，能够生成更准确且视觉一致的IHC图像。

Conclusion: 通过利用相邻切片的上下文信息和关键点空间对应关系，K-Stain为虚拟染色技术提供了一种有效的解决方案，显著提升了合成IHC图像的保真度和实用性。

Abstract: Virtual staining offers a promising method for converting Hematoxylin and
Eosin (H&E) images into Immunohistochemical (IHC) images, eliminating the need
for costly chemical processes. However, existing methods often struggle to
utilize spatial information effectively due to misalignment in tissue slices.
To overcome this challenge, we leverage keypoints as robust indicators of
spatial correspondence, enabling more precise alignment and integration of
structural details in synthesized IHC images. We introduce K-Stain, a novel
framework that employs keypoint-based spatial and semantic relationships to
enhance synthesized IHC image fidelity. K-Stain comprises three main
components: (1) a Hierarchical Spatial Keypoint Detector (HSKD) for identifying
keypoints in stain images, (2) a Keypoint-aware Enhancement Generator (KEG)
that integrates these keypoints during image generation, and (3) a Keypoint
Guided Discriminator (KGD) that improves the discriminator's sensitivity to
spatial details. Our approach leverages contextual information from adjacent
slices, resulting in more accurate and visually consistent IHC images.
Extensive experiments show that K-Stain outperforms state-of-the-art methods in
quantitative metrics and visual quality.

</details>


### [100] [MirrorMamba: Towards Scalable and Robust Mirror Detection in Videos](https://arxiv.org/abs/2511.06716)
*Rui Song,Jiaying Lin,Rynson W. H. Lau*

Main category: cs.CV

TL;DR: MirrorMamba是一种基于Mamba架构的视频镜像检测方法，通过融合深度、对应关系和光流等多种线索，利用Mamba的全局感受野和线性复杂度优势，实现了高效且鲁棒的镜像检测。


<details>
  <summary>Details</summary>
Motivation: 现有视频镜像检测方法存在性能和鲁棒性不足的问题，这些方法过度依赖单一不可靠的动态特征，且基于CNN的方法感受野有限，基于Transformer的方法计算复杂度高（二次复杂度）。

Method: 提出MirrorMamba方法，包含三个核心组件：1）融合感知深度、对应关系和光流等多种线索；2）创新的基于Mamba的多方向对应关系提取器，利用Mamba的全局感受野和线性复杂度；3）基于Mamba的分层边界强制解码器，解决深度图模糊导致的边界不清问题。

Result: 在基准数据集上的大量实验证明，该方法在视频镜像检测任务上优于现有最先进方法，同时在最具挑战性的图像镜像检测数据集上也达到了最先进性能，证明了其鲁棒性和泛化能力。

Conclusion: 这项工作首次成功将Mamba架构应用于镜像检测领域，证明了Mamba在计算机视觉任务中的潜力，MirrorMamba通过多线索融合和Mamba架构的优势，为镜像检测提供了新的有效解决方案。

Abstract: Video mirror detection has received significant research attention, yet
existing methods suffer from limited performance and robustness. These
approaches often over-rely on single, unreliable dynamic features, and are
typically built on CNNs with limited receptive fields or Transformers with
quadratic computational complexity. To address these limitations, we propose a
new effective and scalable video mirror detection method, called MirrorMamba.
Our approach leverages multiple cues to adapt to diverse conditions,
incorporating perceived depth, correspondence and optical. We also introduce an
innovative Mamba-based Multidirection Correspondence Extractor, which benefits
from the global receptive field and linear complexity of the emerging Mamba
spatial state model to effectively capture correspondence properties.
Additionally, we design a Mamba-based layer-wise boundary enforcement decoder
to resolve the unclear boundary caused by the blurred depth map. Notably, this
work marks the first successful application of the Mamba-based architecture in
the field of mirror detection. Extensive experiments demonstrate that our
method outperforms existing state-of-the-art approaches for video mirror
detection on the benchmark datasets. Furthermore, on the most challenging and
representative image-based mirror detection dataset, our approach achieves
state-of-the-art performance, proving its robustness and generalizability.

</details>


### [101] [MRT: Learning Compact Representations with Mixed RWKV-Transformer for Extreme Image Compression](https://arxiv.org/abs/2511.06717)
*Han Liu,Hengyu Man,Xingtao Wang,Wenrui Li,Debin Zhao*

Main category: cs.CV

TL;DR: 本文提出Mixed RWKV-Transformer (MRT)架构，通过将图像压缩到更紧凑的1D潜在空间而非传统2D空间，显著提升了极端图像压缩效率。在低比特率下(MRT相比现有方法节省30-44%比特率，重构质量显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有图像压缩方法使用CNN或Swin Transformer将图像压缩到2D潜在空间，但这些方法保留了大量空间冗余，限制了压缩性能。需要更紧凑的表示方法来减少冗余，提高编码效率。

Method: 提出MRT架构：1) 将图像划分为固定窗口；2) 使用RWKV模块捕获跨窗口全局依赖；3) 使用Transformer块建模窗口内局部冗余；4) 专门设计RCM模型压缩1D潜在特征。通过分层注意力机制实现1D域高效紧凑的表示学习。

Result: 在标准基准测试中，MRT在0.02 bpp以下比特率始终保持卓越重构质量。基于DISTS指标，MRT相比SOTA的GLC架构，在Kodak和CLIC2020数据集上分别节省43.75%和30.59%比特率。

Conclusion: 通过创新性地结合RWKV和Transformer的优势，将图像压缩从2D潜在空间转向1D表示，有效减少了空间冗余。MRT框架为极端图像压缩提供了新的解决思路，在超低比特率场景下表现出显著优势。

Abstract: Recent advances in extreme image compression have revealed that mapping pixel
data into highly compact latent representations can significantly improve
coding efficiency. However, most existing methods compress images into 2-D
latent spaces via convolutional neural networks (CNNs) or Swin Transformers,
which tend to retain substantial spatial redundancy, thereby limiting overall
compression performance. In this paper, we propose a novel Mixed
RWKV-Transformer (MRT) architecture that encodes images into more compact 1-D
latent representations by synergistically integrating the complementary
strengths of linear-attention-based RWKV and self-attention-based Transformer
models. Specifically, MRT partitions each image into fixed-size windows,
utilizing RWKV modules to capture global dependencies across windows and
Transformer blocks to model local redundancies within each window. The
hierarchical attention mechanism enables more efficient and compact
representation learning in the 1-D domain. To further enhance compression
efficiency, we introduce a dedicated RWKV Compression Model (RCM) tailored to
the structure characteristics of the intermediate 1-D latent features in MRT.
Extensive experiments on standard image compression benchmarks validate the
effectiveness of our approach. The proposed MRT framework consistently achieves
superior reconstruction quality at bitrates below 0.02 bits per pixel (bpp).
Quantitative results based on the DISTS metric show that MRT significantly
outperforms the state-of-the-art 2-D architecture GLC, achieving bitrate
savings of 43.75%, 30.59% on the Kodak and CLIC2020 test datasets,
respectively.

</details>


### [102] [Relative Energy Learning for LiDAR Out-of-Distribution Detection](https://arxiv.org/abs/2511.06720)
*Zizhao Li,Zhengkang Xiang,Jiayang Ao,Joseph West,Kourosh Khoshelham*

Main category: cs.CV

TL;DR: 本文提出了一种用于LiDAR点云OOD检测的相对能量学习（REL）框架，通过计算正负logits之间的能量差作为相对评分函数，并结合Point Raise数据合成策略，在SemanticKITTI和STU基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法从2D图像直接迁移到3D LiDAR点云效果不佳，当前LiDAR OOD方法难以区分罕见异常和常见类别，导致高误报率和过自信错误，对安全关键场景构成威胁。

Method: 提出相对能量学习（REL）框架，利用正负logits之间的能量差作为相对评分函数来解决校准问题；同时提出轻量级数据合成策略Point Raise，通过扰动现有点云生成辅助异常样本而不改变原始语义。

Result: 在SemanticKITTI和STU基准测试中，REL方法一致性地大幅超越现有方法，证明了相对能量建模与简单合成异常相结合的有效性。

Conclusion: REL结合相对能量学习和轻量级合成异常，为开放世界自动驾驶中的可靠OOD检测提供了一个原则性和可扩展的解决方案。

Abstract: Out-of-distribution (OOD) detection is a critical requirement for reliable
autonomous driving, where safety depends on recognizing road obstacles and
unexpected objects beyond the training distribution. Despite extensive research
on OOD detection in 2D images, direct transfer to 3D LiDAR point clouds has
been proven ineffective. Current LiDAR OOD methods struggle to distinguish rare
anomalies from common classes, leading to high false-positive rates and
overconfident errors in safety-critical settings. We propose Relative Energy
Learning (REL), a simple yet effective framework for OOD detection in LiDAR
point clouds. REL leverages the energy gap between positive (in-distribution)
and negative logits as a relative scoring function, mitigating calibration
issues in raw energy values and improving robustness across various scenes. To
address the absence of OOD samples during training, we propose a lightweight
data synthesis strategy called Point Raise, which perturbs existing point
clouds to generate auxiliary anomalies without altering the inlier semantics.
Evaluated on SemanticKITTI and the Spotting the Unexpected (STU) benchmark, REL
consistently outperforms existing methods by a large margin. Our results
highlight that modeling relative energy, combined with simple synthetic
outliers, provides a principled and scalable solution for reliable OOD
detection in open-world autonomous driving.

</details>


### [103] [TiS-TSL: Image-Label Supervised Surgical Video Stereo Matching via Time-Switchable Teacher-Student Learning](https://arxiv.org/abs/2511.06817)
*Rui Wang,Ying Zhou,Hao Wang,Wenwei Zhang,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: TiS-TSL是一种时序可切换的师生学习框架，用于微创手术视频立体匹配，通过双向时空一致性解决监督不足和时序不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 微创手术立体匹配对导航和增强现实至关重要，但由于解剖学限制，密集视差监督几乎不可能，通常只能在术前获取少量图像级标签。现有师生学习方法局限于图像级监督，缺乏时序一致性估计，导致视差预测不稳定和严重闪烁伪影。

Method: 提出TiS-TSL框架，包含统一的三模式模型（图像预测IP、前向视频预测FVP、后向视频预测BVP）和两阶段学习策略。图像到视频阶段(I2V)转移稀疏图像知识初始化时序建模，视频到视频阶段(V2V)通过比较前后向预测计算双向时空一致性，识别不可靠区域并过滤噪声伪标签。

Result: 在两个公共数据集上，TiS-TSL显著超越其他基于图像的最先进方法，TEPE和EPE指标分别至少提升2.11%和4.54%。

Conclusion: TiS-TSL通过创新的时序可切换师生学习和双向时空一致性机制，有效解决了微创手术视频立体匹配中的监督不足和时序不一致问题，为手术导航和增强现实应用提供了更可靠的立体视觉解决方案。

Abstract: Stereo matching in minimally invasive surgery (MIS) is essential for
next-generation navigation and augmented reality. Yet, dense disparity
supervision is nearly impossible due to anatomical constraints, typically
limiting annotations to only a few image-level labels acquired before the
endoscope enters deep body cavities. Teacher-Student Learning (TSL) offers a
promising solution by leveraging a teacher trained on sparse labels to generate
pseudo labels and associated confidence maps from abundant unlabeled surgical
videos. However, existing TSL methods are confined to image-level supervision,
providing only spatial confidence and lacking temporal consistency estimation.
This absence of spatio-temporal reliability results in unstable disparity
predictions and severe flickering artifacts across video frames. To overcome
these challenges, we propose TiS-TSL, a novel time-switchable teacher-student
learning framework for video stereo matching under minimal supervision. At its
core is a unified model that operates in three distinct modes: Image-Prediction
(IP), Forward Video-Prediction (FVP), and Backward Video-Prediction (BVP),
enabling flexible temporal modeling within a single architecture. Enabled by
this unified model, TiS-TSL adopts a two-stage learning strategy. The
Image-to-Video (I2V) stage transfers sparse image-level knowledge to initialize
temporal modeling. The subsequent Video-to-Video (V2V) stage refines temporal
disparity predictions by comparing forward and backward predictions to
calculate bidirectional spatio-temporal consistency. This consistency
identifies unreliable regions across frames, filters noisy video-level pseudo
labels, and enforces temporal coherence. Experimental results on two public
datasets demonstrate that TiS-TSL exceeds other image-based state-of-the-arts
by improving TEPE and EPE by at least 2.11% and 4.54%, respectively..

</details>


### [104] [AvatarTex: High-Fidelity Facial Texture Reconstruction from Single-Image Stylized Avatars](https://arxiv.org/abs/2511.06721)
*Yuda Qiu,Zitong Xiao,Yiwei Zuo,Zisheng Ye,Weikai Chen,Xiaoguang Han*

Main category: cs.CV

TL;DR: AvatarTex是一个高保真面部纹理重建框架，通过三阶段diffusion-to-GAN流程，从单张图像生成风格化和照片级真实纹理，并推出了20,000个多风格UV纹理数据集TexHub。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理风格化头像，主要因为缺乏多样化多风格数据集以及在非标准纹理中保持几何一致性的挑战。需要一种能够同时处理艺术风格和几何约束的纹理重建方法。

Method: 提出三阶段diffusion-to-GAN流程：1) 基于扩散的修复完成缺失纹理区域；2) 基于GAN的潜在优化优化风格和结构一致性；3) 基于扩散的重新绘制增强细节。同时构建TexHub数据集，包含20,000个高分辨率多风格UV纹理。

Result: AvatarTex在多风格面部纹理重建方面达到了新的最先进水平，能够生成高质量、拓扑对齐的纹理，兼具艺术和几何一致性。TexHub数据集将公开发布以促进该领域研究。

Conclusion: 通过结合扩散模型多样化生成能力和GAN结构化潜在空间优势，AvatarTex有效解决了风格化纹理重建的挑战，为单张图像到高质量纹理的转换提供了新的解决方案。

Abstract: We present AvatarTex, a high-fidelity facial texture reconstruction framework
capable of generating both stylized and photorealistic textures from a single
image. Existing methods struggle with stylized avatars due to the lack of
diverse multi-style datasets and challenges in maintaining geometric
consistency in non-standard textures. To address these limitations, AvatarTex
introduces a novel three-stage diffusion-to-GAN pipeline. Our key insight is
that while diffusion models excel at generating diversified textures, they lack
explicit UV constraints, whereas GANs provide a well-structured latent space
that ensures style and topology consistency. By integrating these strengths,
AvatarTex achieves high-quality topology-aligned texture synthesis with both
artistic and geometric coherence. Specifically, our three-stage pipeline first
completes missing texture regions via diffusion-based inpainting, refines style
and structure consistency using GAN-based latent optimization, and enhances
fine details through diffusion-based repainting. To address the need for a
stylized texture dataset, we introduce TexHub, a high-resolution collection of
20,000 multi-style UV textures with precise UV-aligned layouts. By leveraging
TexHub and our structured diffusion-to-GAN pipeline, AvatarTex establishes a
new state-of-the-art in multi-style facial texture reconstruction. TexHub will
be released upon publication to facilitate future research in this field.

</details>


### [105] [NeuroBridge: Bio-Inspired Self-Supervised EEG-to-Image Decoding via Cognitive Priors and Bidirectional Semantic Alignment](https://arxiv.org/abs/2511.06836)
*Wenjiang Zhang,Sifeng Wang,Yuwei Su,Xinyu Li,Chen Zhang,Suyu Zhong*

Main category: cs.CV

TL;DR: NeuroBridge是一种新型自监督架构，通过认知先验增强(CPA)和共享语义投影器(SSP)解决视觉神经解码中的数据稀缺和语义不匹配问题，在200路零样本检索任务中达到63.2% top-1和89.9% top-5准确率。


<details>
  <summary>Details</summary>
Motivation: 当前视觉神经解码方法面临两个主要限制：高质量刺激-大脑响应对的稀缺性，以及神经表征与视觉内容之间的内在语义不匹配，这阻碍了该领域在脑机接口和人工智能中的应用发展。

Method: 提出NeuroBridge自监督架构，包含两个核心组件：1) 认知先验增强(CPA)通过不对称、模态特定的变换模拟感知变异性，增强语义多样性；2) 共享语义投影器(SSP)通过共适应策略建立双向对齐过程，将两种模态的特征相互对齐到共享语义空间。

Result: 在跨被试和被试内设置下均超越现有最优方法，被试内场景中top-1准确率提升12.3%达到63.2%，top-5准确率提升10.2%达到89.9%，在200路零样本检索任务上表现优异。

Conclusion: 广泛实验证明了NeuroBridge框架在神经视觉解码中的有效性、鲁棒性和可扩展性，为脑机接口和人工智能应用提供了新的技术路径，通过模拟生物系统的感知变异性和共适应策略实现了突破性性能提升。

Abstract: Visual neural decoding seeks to reconstruct or infer perceived visual stimuli
from brain activity patterns, providing critical insights into human cognition
and enabling transformative applications in brain-computer interfaces and
artificial intelligence. Current approaches, however, remain constrained by the
scarcity of high-quality stimulus-brain response pairs and the inherent
semantic mismatch between neural representations and visual content. Inspired
by perceptual variability and co-adaptive strategy of the biological systems,
we propose a novel self-supervised architecture, named NeuroBridge, which
integrates Cognitive Prior Augmentation (CPA) with Shared Semantic Projector
(SSP) to promote effective cross-modality alignment. Specifically, CPA
simulates perceptual variability by applying asymmetric, modality-specific
transformations to both EEG signals and images, enhancing semantic diversity.
Unlike previous approaches, SSP establishes a bidirectional alignment process
through a co-adaptive strategy, which mutually aligns features from two
modalities into a shared semantic space for effective cross-modal learning.
NeuroBridge surpasses previous state-of-the-art methods under both
intra-subject and inter-subject settings. In the intra-subject scenario, it
achieves the improvements of 12.3% in top-1 accuracy and 10.2% in top-5
accuracy, reaching 63.2% and 89.9% respectively on a 200-way zero-shot
retrieval task. Extensive experiments demonstrate the effectiveness,
robustness, and scalability of the proposed framework for neural visual
decoding.

</details>


### [106] [Argus: Quality-Aware High-Throughput Text-to-Image Inference Serving System](https://arxiv.org/abs/2511.06724)
*Shubham Agarwal,Subrata Mitra,Saud Iqbal*

Main category: cs.CV

TL;DR: Argus是一个高通量文本到图像推理系统，通过智能选择近似策略，在保证质量的同时显著提升系统性能。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型计算开销大、推理时间长，传统系统难以满足高通量需求；发现部分提示可用近似模型处理，但需仔细校准以避免质量下降。

Method: Argus系统为每个提示选择合适的近似级别，智能切换不同近似策略，在固定规模集群上满足吞吐量目标的同时保持图像质量。

Result: 相比基线系统，Argus实现延迟SLO违规减少10倍，平均质量提升10%，吞吐量提高40%。

Conclusion: Argus有效解决了T2I模型的高通量推理挑战，证明了通过智能近似选择可以在保证质量的前提下大幅提升系统性能。

Abstract: Text-to-image (T2I) models have gained significant popularity. Most of these
are diffusion models with unique computational characteristics, distinct from
both traditional small-scale ML models and large language models. They are
highly compute-bound and use an iterative denoising process to generate images,
leading to very high inference time. This creates significant challenges in
designing a high-throughput system. We discovered that a large fraction of
prompts can be served using faster, approximated models. However, the
approximation setting must be carefully calibrated for each prompt to avoid
quality degradation. Designing a high-throughput system that assigns each
prompt to the appropriate model and compatible approximation setting remains a
challenging problem. We present Argus, a high-throughput T2I inference system
that selects the right level of approximation for each prompt to maintain
quality while meeting throughput targets on a fixed-size cluster. Argus
intelligently switches between different approximation strategies to satisfy
both throughput and quality requirements. Overall, Argus achieves 10x fewer
latency service-level objective (SLO) violations, 10% higher average quality,
and 40% higher throughput compared to baselines on two real-world workload
traces.

</details>


### [107] [Rethinking Rainy 3D Scene Reconstruction via Perspective Transforming and Brightness Tuning](https://arxiv.org/abs/2511.06734)
*Qianfeng Yang,Xiang Chen,Pengpeng Li,Qiyuan Guan,Guiyue Jin,Jiyu Jin*

Main category: cs.CV

TL;DR: 本文提出了OmniRain3D数据集和REVR-GSNet框架，解决了雨天多视图图像3D重建中雨条视角依赖性和亮度变化的问题，实现了从雨退化输入中高保真重建清洁3D场景。


<details>
  <summary>Details</summary>
Motivation: 雨天会降低多视图图像的视觉质量，影响3D场景重建的准确性和完整性。现有数据集忽视了真实雨天3D场景的两个关键特征：雨条投影到2D图像上的视角依赖性外观变化，以及雨天云层覆盖导致的环境亮度降低。

Method: 构建了OmniRain3D数据集，结合视角异质性和亮度动态性；提出了REVR-GSNet端到端重建框架，通过联合交替优化集成递归亮度增强、高斯基元优化和GS引导的雨消除，实现统一架构的高保真重建。

Result: 广泛实验证明了所提出数据集和方法的有效性，能够从雨退化输入中重建出高质量的清洁3D场景，为多视图图像去雨和雨天3D场景重建研究提供了基础。

Conclusion: OmniRain3D数据集和REVR-GSNet方法有效解决了雨天3D重建中的关键挑战，通过模拟真实雨天场景的复杂特性，显著提升了重建质量，为未来相关研究奠定了重要基础。

Abstract: Rain degrades the visual quality of multi-view images, which are essential
for 3D scene reconstruction, resulting in inaccurate and incomplete
reconstruction results. Existing datasets often overlook two critical
characteristics of real rainy 3D scenes: the viewpoint-dependent variation in
the appearance of rain streaks caused by their projection onto 2D images, and
the reduction in ambient brightness resulting from cloud coverage during
rainfall. To improve data realism, we construct a new dataset named OmniRain3D
that incorporates perspective heterogeneity and brightness dynamicity, enabling
more faithful simulation of rain degradation in 3D scenes. Based on this
dataset, we propose an end-to-end reconstruction framework named REVR-GSNet
(Rain Elimination and Visibility Recovery for 3D Gaussian Splatting).
Specifically, REVR-GSNet integrates recursive brightness enhancement, Gaussian
primitive optimization, and GS-guided rain elimination into a unified
architecture through joint alternating optimization, achieving high-fidelity
reconstruction of clean 3D scenes from rain-degraded inputs. Extensive
experiments show the effectiveness of our dataset and method. Our dataset and
method provide a foundation for future research on multi-view image deraining
and rainy 3D scene reconstruction.

</details>


### [108] [PlantTraitNet: An Uncertainty-Aware Multimodal Framework for Global-Scale Plant Trait Inference from Citizen Science Data](https://arxiv.org/abs/2511.06943)
*Ayushi Sharma,Johanna Trost,Daniel Lusk,Johannes Dollinger,Julian Schrader,Christian Rossi,Javier Lopatin,Etienne Laliberté,Simon Haberstroh,Jana Eichel,Daniel Mederer,Jose Miguel Cerda-Paredes,Shyam S. Phartyal,Lisa-Maricia Schwarz,Anja Linstädter,Maria Conceição Caldeira,Teja Kattenborn*

Main category: cs.CV

TL;DR: 本研究提出了PlantTraitNet深度学习框架，利用公民科学照片预测植物性状并生成全球地图，其性能优于现有产品。


<details>
  <summary>Details</summary>
Motivation: 现有的全球植物性状地图受限于基于实地测量的高成本和稀疏地理覆盖，而公民科学倡议提供了超过5000万张地理标记植物照片的丰富资源，可作为解决方案。

Method: 开发了PlantTraitNet，一个多模态、多任务、不确定性感知的深度学习框架，通过弱监督从公民科学照片中预测四种关键植物性状（植株高度、叶面积、比叶面积和氮含量），并汇总生成全球性状分布地图。

Result: 生成的全球植物性状地图在独立植被调查数据（sPlotOpen）上得到验证，并在所有评估性状上持续优于现有全球性状产品。

Conclusion: 公民科学影像与计算机视觉和地理空间AI的集成，不仅实现了可扩展的全球性状测绘，还提高了准确性，为生态研究和地球系统建模提供了强大新途径。

Abstract: Global plant maps of plant traits, such as leaf nitrogen or plant height, are
essential for understanding ecosystem processes, including the carbon and
energy cycles of the Earth system. However, existing trait maps remain limited
by the high cost and sparse geographic coverage of field-based measurements.
Citizen science initiatives offer a largely untapped resource to overcome these
limitations, with over 50 million geotagged plant photographs worldwide
capturing valuable visual information on plant morphology and physiology. In
this study, we introduce PlantTraitNet, a multi-modal, multi-task
uncertainty-aware deep learning framework that predictsfour key plant traits
(plant height, leaf area, specific leaf area, and nitrogen content) from
citizen science photos using weak supervision. By aggregating individual trait
predictions across space, we generate global maps of trait distributions. We
validate these maps against independent vegetation survey data (sPlotOpen) and
benchmark them against leading global trait products. Our results show that
PlantTraitNet consistently outperforms existing trait maps across all evaluated
traits, demonstrating that citizen science imagery, when integrated with
computer vision and geospatial AI, enables not only scalable but also more
accurate global trait mapping. This approach offers a powerful new pathway for
ecological research and Earth system modeling.

</details>


### [109] [SinSEMI: A One-Shot Image Generation Model and Data-Efficient Evaluation Framework for Semiconductor Inspection Equipment](https://arxiv.org/abs/2511.06740)
*ChunLiang Wu,Xiaochun Li*

Main category: cs.CV

TL;DR: SinSEMI是一种新颖的单样本学习方法，能够从单张光学图像生成多样化和高度逼真的半导体图像


<details>
  <summary>Details</summary>
Motivation: 在半导体设备开发的早期阶段，获取大量原始光学图像面临重大挑战，这种数据稀缺阻碍了半导体制造中AI驱动解决方案的进步

Method: SinSEMI采用多尺度流模型，在采样过程中结合LPIPS（学习感知图像块相似性）能量引导，确保感知真实性和输出多样性；同时引入了仅需两张参考图像的专门评估框架

Result: 实验结果表明SinSEMI在视觉质量、定量指标和下游任务方面优于其他单样本生成技术，生成的图像同时实现了高保真度和有意义的多样性

Conclusion: SinSEMI生成的图像适合作为半导体AI应用的训练数据，有效解决了早期半导体设备开发中的数据稀缺问题

Abstract: In the early stages of semiconductor equipment development, obtaining large
quantities of raw optical images poses a significant challenge. This data
scarcity hinder the advancement of AI-powered solutions in semiconductor
manufacturing. To address this challenge, we introduce SinSEMI, a novel
one-shot learning approach that generates diverse and highly realistic images
from single optical image. SinSEMI employs a multi-scale flow-based model
enhanced with LPIPS (Learned Perceptual Image Patch Similarity) energy guidance
during sampling, ensuring both perceptual realism and output variety. We also
introduce a comprehensive evaluation framework tailored for this application,
which enables a thorough assessment using just two reference images. Through
the evaluation against multiple one-shot generation techniques, we demonstrate
SinSEMI's superior performance in visual quality, quantitative measures, and
downstream tasks. Our experimental results demonstrate that SinSEMI-generated
images achieve both high fidelity and meaningful diversity, making them
suitable as training data for semiconductor AI applications.

</details>


### [110] [From Attribution to Action: Jointly ALIGNing Predictions and Explanations](https://arxiv.org/abs/2511.06944)
*Dongsheng Hong,Chao Chen,Yanhui Chen,Shanshan Lin,Zhihao Chen,Xiangwen Liao*

Main category: cs.CV

TL;DR: ALIGN是一个迭代式联合训练框架，通过掩码器生成高质量软掩码来指导分类器，显著提升模型的可解释性和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有的解释引导学习方法依赖外部标注或启发式分割产生的监督信号，这些信号质量低、噪声大，反而会降低模型性能。

Method: 提出ALIGN框架，迭代式联合训练分类器和掩码器：掩码器学习生成与任务相关的软掩码突出信息区域；分类器同时优化预测准确性和其显著性图与学习掩码的对齐程度。

Result: 在VLCS和Terra Incognita领域泛化基准上，ALIGN在分布内和分布外设置下均持续超越6个强基线，并产生具有更优充分性和全面性的解释质量。

Conclusion: ALIGN通过利用高质量掩码作为指导，有效解决了低质量监督信号的问题，在提升模型可解释性和泛化能力方面展现出优越性。

Abstract: Explanation-guided learning (EGL) has shown promise in aligning model
predictions with interpretable reasoning, particularly in computer vision
tasks. However, most approaches rely on external annotations or heuristic-based
segmentation to supervise model explanations, which can be noisy, imprecise and
difficult to scale. In this work, we provide both empirical and theoretical
evidence that low-quality supervision signals can degrade model performance
rather than improve it. In response, we propose ALIGN, a novel framework that
jointly trains a classifier and a masker in an iterative manner. The masker
learns to produce soft, task-relevant masks that highlight informative regions,
while the classifier is optimized for both prediction accuracy and alignment
between its saliency maps and the learned masks. By leveraging high-quality
masks as guidance, ALIGN improves both interpretability and generalizability,
showing its superiority across various settings. Experiments on the two domain
generalization benchmarks, VLCS and Terra Incognita, show that ALIGN
consistently outperforms six strong baselines in both in-distribution and
out-of-distribution settings. Besides, ALIGN also yields superior explanation
quality concerning sufficiency and comprehensiveness, highlighting its
effectiveness in producing accurate and interpretable models.

</details>


### [111] [FoCLIP: A Feature-Space Misalignment Framework for CLIP-Based Image Manipulation and Detection](https://arxiv.org/abs/2511.06947)
*Yulin Chen,Zeyuan Wang,Tianyuan Yu,Yingmei Wei,Liang Bai*

Main category: cs.CV

TL;DR: 提出了FoCLIP框架，通过特征空间错位技术欺骗基于CLIP的图像质量评估指标，同时开发了相应的防御机制。


<details>
  <summary>Details</summary>
Motivation: 基于CLIP的图像质量评估指标（如CLIPscore）因其良好的多模态对齐特性而被广泛采用，但这种对齐机制是脆弱的，容易被攻击者利用生成欺骗性图像。

Method: 提出FoCLIP框架，基于随机梯度下降技术，集成三个核心组件：1）特征对齐模块减少图像-文本模态间隙；2）分数分布平衡模块；3）像素守卫正则化，共同优化CLIPscore性能和图像质量间的多模态输出平衡。

Result: 在10个艺术杰作提示词和ImageNet子集实验中，优化图像在保持高视觉保真度的同时显著提升了CLIPscore。发现灰度转换会诱导欺骗图像的特征显著退化，基于此提出了颜色通道敏感性驱动的篡改检测机制，在标准基准上达到91%准确率。

Conclusion: 这项工作为基于CLIP的多模态系统中的特征错位建立了实用途径，同时提出了有效的防御方法，为CLIP模型的安全性和鲁棒性提供了重要见解。

Abstract: The well-aligned attribute of CLIP-based models enables its effective
application like CLIPscore as a widely adopted image quality assessment metric.
However, such a CLIP-based metric is vulnerable for its delicate multimodal
alignment. In this work, we propose \textbf{FoCLIP}, a feature-space
misalignment framework for fooling CLIP-based image quality metric. Based on
the stochastic gradient descent technique, FoCLIP integrates three key
components to construct fooling examples: feature alignment as the core module
to reduce image-text modality gaps, the score distribution balance module and
pixel-guard regularization, which collectively optimize multimodal output
equilibrium between CLIPscore performance and image quality. Such a design can
be engineered to maximize the CLIPscore predictions across diverse input
prompts, despite exhibiting either visual unrecognizability or semantic
incongruence with the corresponding adversarial prompts from human perceptual
perspectives. Experiments on ten artistic masterpiece prompts and ImageNet
subsets demonstrate that optimized images can achieve significant improvement
in CLIPscore while preserving high visual fidelity. In addition, we found that
grayscale conversion induces significant feature degradation in fooling images,
exhibiting noticeable CLIPscore reduction while preserving statistical
consistency with original images. Inspired by this phenomenon, we propose a
color channel sensitivity-driven tampering detection mechanism that achieves
91% accuracy on standard benchmarks. In conclusion, this work establishes a
practical pathway for feature misalignment in CLIP-based multimodal systems and
the corresponding defense method.

</details>


### [112] [PointCubeNet: 3D Part-level Reasoning with 3x3x3 Point Cloud Blocks](https://arxiv.org/abs/2511.06744)
*Da-Yeong Kim,Yeong-Jun Cho*

Main category: cs.CV

TL;DR: PointCubeNet是一种新颖的无监督多模态3D理解框架，能够在无需部件标注的情况下实现部件级推理，通过3x3x3局部块结构和伪标签方法有效训练。


<details>
  <summary>Details</summary>
Motivation: 现有的3D理解方法通常需要大量的部件级标注数据，这限制了其在实际应用中的扩展性。本文旨在开发一种无需人工标注即可实现部件级推理的3D理解框架。

Method: PointCubeNet采用双分支架构：全局分支处理整体3D对象，局部分支结构化为3x3x3局部块进行部件级分析。通过创新的伪标签方法和局部损失函数实现无监督训练，结合点云子区域和对应的局部文本标签进行学习。

Result: 实验证明PointCubeNet能够有效进行无监督3D部件级推理，理解3D对象部件显著增强了对整体对象的理解能力，这是首个在该领域取得可靠和有意义结果的方法。

Conclusion: PointCubeNet成功实现了首个无监督3D部件级推理框架，为3D对象理解开辟了新路径，证明了部件级分析对整体3D理解的重要价值，为未来的3D计算机视觉研究提供了重要参考。

Abstract: In this paper, we propose PointCubeNet, a novel multi-modal 3D understanding
framework that achieves part-level reasoning without requiring any part
annotations. PointCubeNet comprises global and local branches. The proposed
local branch, structured into 3x3x3 local blocks, enables part-level analysis
of point cloud sub-regions with the corresponding local text labels. Leveraging
the proposed pseudo-labeling method and local loss function, PointCubeNet is
effectively trained in an unsupervised manner. The experimental results
demonstrate that understanding 3D object parts enhances the understanding of
the overall 3D object. In addition, this is the first attempt to perform
unsupervised 3D part-level reasoning and achieves reliable and meaningful
results.

</details>


### [113] [TrueCity: Real and Simulated Urban Data for Cross-Domain 3D Scene Understanding](https://arxiv.org/abs/2511.07007)
*Duc Nguyen,Yan-Ling Lai,Qilin Zhang,Prabin Gyawali,Benedikt Schwab,Olaf Wysocki,Thomas H. Kolbe*

Main category: cs.CV

TL;DR: TrueCity是首个具有厘米级精度标注真实点云、语义3D城市模型和标注模拟点云的城市语义分割基准，用于解决合成到真实的域差问题


<details>
  <summary>Details</summary>
Motivation: 3D语义场景理解面临真实标注数据不足的挑战。虽然合成数据集提供可扩展性和完美标签，但设计者制作的场景无法捕捉真实世界复杂性和传感器噪声，导致合成到真实域差，且缺乏同步真实和模拟点云的分割域移分析基准

Method: 构建TrueCity基准，包含厘米级精度真实世界点云标注、语义3D城市模型和相同城市的标注模拟点云，提出与国际3D城市建模标准一致的分割类别，实现合成到真实域差的一致性评估

Result: 通过常见基线的广泛实验量化了域移，突出了利用合成数据增强真实世界3D场景理解的有效策略

Conclusion: TrueCity数据集将推动合成到真实域差量化的发展，支持泛化数据驱动模型的构建，数据、代码和3D模型已公开可用

Abstract: 3D semantic scene understanding remains a long-standing challenge in the 3D
computer vision community. One of the key issues pertains to limited real-world
annotated data to facilitate generalizable models. The common practice to
tackle this issue is to simulate new data. Although synthetic datasets offer
scalability and perfect labels, their designer-crafted scenes fail to capture
real-world complexity and sensor noise, resulting in a synthetic-to-real domain
gap. Moreover, no benchmark provides synchronized real and simulated point
clouds for segmentation-oriented domain shift analysis. We introduce TrueCity,
the first urban semantic segmentation benchmark with cm-accurate annotated
real-world point clouds, semantic 3D city models, and annotated simulated point
clouds representing the same city. TrueCity proposes segmentation classes
aligned with international 3D city modeling standards, enabling consistent
evaluation of synthetic-to-real gap. Our extensive experiments on common
baselines quantify domain shift and highlight strategies for exploiting
synthetic data to enhance real-world 3D scene understanding. We are convinced
that the TrueCity dataset will foster further development of sim-to-real gap
quantification and enable generalizable data-driven models. The data, code, and
3D models are available online: https://tum-gis.github.io/TrueCity/

</details>


### [114] [Image Restoration via Primal Dual Hybrid Gradient and Flow Generative Model](https://arxiv.org/abs/2511.06748)
*Ji Li,Chao Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种基于原始-对偶混合梯度(PDHG)方法的通用Plug-and-Play算法，将流匹配生成模型作为正则化先验集成到图像恢复框架中，支持多种数据保真项(包括ℓ₁和ℓ₂范数)，在非高斯噪声下的图像恢复任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的Plug-and-Play方法主要针对高斯噪声(平滑ℓ₂数据保真项)取得了成功，但在更一般的数据保真项(如非高斯噪声)方面应用有限，需要开发一种通用且高效的PnP算法来处理更广泛的噪声类型和数据保真项。

Method: 提出基于原始-对偶混合梯度(PDHG)方法的PnP算法框架，将正则化器的近端算子替换为从生成模型导出的时变去噪器，支持ℓ₁和ℓ₂范数损失，实现对泊松噪声和脉冲噪声等非高斯噪声的鲁棒性处理。

Result: 在图像去噪、超分辨率、去模糊和修复等多个图像恢复任务上验证了方法有效性，实验结果表明在存在非高斯噪声时，ℓ₁和ℓ₂保真项的表现优于传统的平方ℓ₂损失。

Conclusion: 所提出的PnP-PDHG算法是一种计算效率高、内存友好的通用方法，能够有效处理多种数据保真项，在非高斯噪声环境下的图像恢复任务中展现出优越性能，扩展了PnP方法在复杂噪声场景中的应用范围。

Abstract: Regularized optimization has been a classical approach to solving imaging
inverse problems, where the regularization term enforces desirable properties
of the unknown image. Recently, the integration of flow matching generative
models into image restoration has garnered significant attention, owing to
their powerful prior modeling capabilities. In this work, we incorporate such
generative priors into a Plug-and-Play (PnP) framework based on proximal
splitting, where the proximal operator associated with the regularizer is
replaced by a time-dependent denoiser derived from the generative model. While
existing PnP methods have achieved notable success in inverse problems with
smooth squared $\ell_2$ data fidelity--typically associated with Gaussian
noise--their applicability to more general data fidelity terms remains
underexplored. To address this, we propose a general and efficient PnP
algorithm inspired by the primal-dual hybrid gradient (PDHG) method. Our
approach is computationally efficient, memory-friendly, and accommodates a wide
range of fidelity terms. In particular, it supports both $\ell_1$ and $\ell_2$
norm-based losses, enabling robustness to non-Gaussian noise types such as
Poisson and impulse noise. We validate our method on several image restoration
tasks, including denoising, super-resolution, deblurring, and inpainting, and
demonstrate that $\ell_1$ and $\ell_2$ fidelity terms outperform the
conventional squared $\ell_2$ loss in the presence of non-Gaussian noise.

</details>


### [115] [Med-SORA: Symptom to Organ Reasoning in Abdomen CT Images](https://arxiv.org/abs/2511.06752)
*You-Kyoung Na,Yeong-Jun Cho*

Main category: cs.CV

TL;DR: 本研究提出Med-SORA框架，首次实现腹部CT图像中症状到器官的推理，通过RAG数据构建、软标签学习和2D-3D融合架构，准确捕捉多器官关联和解剖上下文。


<details>
  <summary>Details</summary>
Motivation: 现有医学多模态模型存在两个主要局限：1) 使用简单的一对一硬标签，无法反映临床现实中症状与多器官的复杂关联；2) 仅使用单层2D特征，缺乏3D解剖信息，限制了临床推理能力。

Method: Med-SORA框架包含三个关键技术：1) 基于RAG的数据集构建；2) 可学习器官锚点的软标签机制，捕捉一对多症状-器官关系；3) 2D-3D交叉注意力架构，融合局部和全局图像特征。

Result: 实验结果表明，Med-SORA在症状到器官推理任务上显著优于现有医学多模态模型，能够实现准确的3D临床推理。

Conclusion: 这是首个专门针对医学多模态学习中症状到器官推理的工作，为临床诊断提供了更精确的器官定位和症状关联分析能力。

Abstract: Understanding symptom-image associations is crucial for clinical reasoning.
However, existing medical multimodal models often rely on simple one-to-one
hard labeling, oversimplifying clinical reality where symptoms relate to
multiple organs. In addition, they mainly use single-slice 2D features without
incorporating 3D information, limiting their ability to capture full anatomical
context. In this study, we propose Med-SORA, a framework for symptom-to-organ
reasoning in abdominal CT images. Med-SORA introduces RAG-based dataset
construction, soft labeling with learnable organ anchors to capture one-to-many
symptom-organ relationships, and a 2D-3D cross-attention architecture to fuse
local and global image features. To our knowledge, this is the first work to
address symptom-to-organ reasoning in medical multimodal learning. Experimental
results show that Med-SORA outperforms existing medical multimodal models and
enables accurate 3D clinical reasoning.

</details>


### [116] [Pandar128 dataset for lane line detection](https://arxiv.org/abs/2511.07084)
*Filip Beránek,Václav Diviš,Ivan Gruber*

Main category: cs.CV

TL;DR: 论文提出了Pandar128数据集（最大公开128束激光雷达车道线检测数据集）、SimpleLidarLane轻量级基线方法和IAM-F1评估指标，解决了激光雷达车道线检测缺乏标准化数据和评估的问题。


<details>
  <summary>Details</summary>
Motivation: 解决激光雷达车道线检测领域存在的两个关键问题：缺乏大规模公开数据集和缺乏标准化评估指标，阻碍了该领域的可重复性研究发展。

Method: 1) 构建Pandar128数据集：包含52,000个相机帧和34,000个激光雷达扫描，提供完整传感器标定和同步里程计；2) 提出SimpleLidarLane方法：结合BEV分割、聚类和折线拟合的轻量级管道；3) 设计IAM-F1评估指标：基于插值感知的横向匹配的折线评估方法。

Result: SimpleLidarLane方法在各种挑战条件下（雨天、稀疏返回等）表现优异，证明模块化管道配合高质量数据和原则性评估能够与更复杂的竞争方法相媲美。

Conclusion: 所有数据和代码公开发布，为激光雷达车道线检测研究提供了高质量基准，支持该领域的可重复性研究和发展。

Abstract: We present Pandar128, the largest public dataset for lane line detection
using a 128-beam LiDAR. It contains over 52,000 camera frames and 34,000 LiDAR
scans, captured in diverse real-world conditions in Germany. The dataset
includes full sensor calibration (intrinsics, extrinsics) and synchronized
odometry, supporting tasks such as projection, fusion, and temporal modeling.
  To complement the dataset, we also introduce SimpleLidarLane, a light-weight
baseline method for lane line reconstruction that combines BEV segmentation,
clustering, and polyline fitting. Despite its simplicity, our method achieves
strong performance under challenging various conditions (e.g., rain, sparse
returns), showing that modular pipelines paired with high-quality data and
principled evaluation can compete with more complex approaches.
  Furthermore, to address the lack of standardized evaluation, we propose a
novel polyline-based metric - Interpolation-Aware Matching F1 (IAM-F1) - that
employs interpolation-aware lateral matching in BEV space.
  All data and code are publicly released to support reproducibility in
LiDAR-based lane detection.

</details>


### [117] [How Bias Binds: Measuring Hidden Associations for Bias Control in Text-to-Image Compositions](https://arxiv.org/abs/2511.07091)
*Jeng-Lin Li,Ming-Ching Chang,Wei-Chao Chen*

Main category: cs.CV

TL;DR: 本文研究了文本到图像生成模型中的语义绑定偏见问题，提出了偏见依从性分数和无需训练的上下文偏见控制框架，在组合生成任务中实现超过10%的去偏见改进，但揭示了在保持语义关系的同时减少偏见的根本挑战。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成模型的偏见研究主要局限于单物体提示词，忽视了实际应用中物体与属性间语义绑定效应导致的偏见放大问题。例如"戴粉色帽子的助理"提示词可能反映与粉色帽子相关的女性倾向偏见，现有去偏见方法因忽视这种联合效应而失效。

Method: 引入偏见依从性分数量化特定物体-属性绑定如何激活偏见，开发无需训练的上下文偏见控制框架，通过探索标记解耦促进语义绑定的去偏见处理。

Result: 提出的框架在组合生成任务中实现超过10%的去偏见改进，对各种属性-物体绑定的偏见分数和标记去相关分析表明，在不破坏基本语义关系的前提下减少偏见是一个根本性挑战。

Conclusion: 现有去偏见方法在语义绑定上下文中存在关键局限性，暴露了当前偏见缓解策略的不足，强调需要重新评估主流偏见减轻方法，特别是在保持必要语义关系的同时有效处理偏见的问题。

Abstract: Text-to-image generative models often exhibit bias related to sensitive
attributes. However, current research tends to focus narrowly on single-object
prompts with limited contextual diversity. In reality, each object or attribute
within a prompt can contribute to bias. For example, the prompt "an assistant
wearing a pink hat" may reflect female-inclined biases associated with a pink
hat. The neglected joint effects of the semantic binding in the prompts cause
significant failures in current debiasing approaches. This work initiates a
preliminary investigation on how bias manifests under semantic binding, where
contextual associations between objects and attributes influence generative
outcomes. We demonstrate that the underlying bias distribution can be amplified
based on these associations. Therefore, we introduce a bias adherence score
that quantifies how specific object-attribute bindings activate bias. To delve
deeper, we develop a training-free context-bias control framework to explore
how token decoupling can facilitate the debiasing of semantic bindings. This
framework achieves over 10% debiasing improvement in compositional generation
tasks. Our analysis of bias scores across various attribute-object bindings and
token decorrelation highlights a fundamental challenge: reducing bias without
disrupting essential semantic relationships. These findings expose critical
limitations in current debiasing approaches when applied to semantically bound
contexts, underscoring the need to reassess prevailing bias mitigation
strategies.

</details>


### [118] [Robust and High-Fidelity 3D Gaussian Splatting: Fusing Pose Priors and Geometry Constraints for Texture-Deficient Outdoor Scenes](https://arxiv.org/abs/2511.06765)
*Meijun Guo,Yongliang Shi,Caiyun Liu,Yixiao Feng,Ming Ma,Tinghai Yan,Weining Lu,Bin Liang*

Main category: cs.CV

TL;DR: 本文提出了一种改进的3D高斯泼溅方法，通过结合LiDAR-IMU里程计进行位姿估计优化，并引入法向量约束和有效秩正则化来增强场景表示，显著提升了大规模弱纹理户外场景的重建效果。


<details>
  <summary>Details</summary>
Motivation: 针对大规模户外场景中弱纹理或重复纹理导致的几何纹理不一致问题，传统3D高斯泼溅方法存在位姿估计不稳定和场景表示失真的挑战。

Method: 在位姿估计方面，利用LiDAR-IMU里程计为相机提供先验位姿，将其约束集成到COLMAP三角测量过程中，并通过束调整优化；在场景表示方面，引入法向量约束和有效秩正则化来强制高斯基元方向和形状的一致性，与现有光度损失联合优化。

Result: 在公开和自采集数据集上，位姿优化仅需传统方法三分之一的时间并保持准确性和鲁棒性；场景表示方面显著优于传统3DGS流程，在弱纹理场景中展现出增强的可视化能力和优越的整体性能。

Conclusion: 该方法有效解决了大规模户外场景3D重建中的关键技术难题，为弱纹理环境下的高质量数字资产创建提供了高效可靠的解决方案，代码和数据将公开发布。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a key rendering pipeline for
digital asset creation due to its balance between efficiency and visual
quality. To address the issues of unstable pose estimation and scene
representation distortion caused by geometric texture inconsistency in large
outdoor scenes with weak or repetitive textures, we approach the problem from
two aspects: pose estimation and scene representation. For pose estimation, we
leverage LiDAR-IMU Odometry to provide prior poses for cameras in large-scale
environments. These prior pose constraints are incorporated into COLMAP's
triangulation process, with pose optimization performed via bundle adjustment.
Ensuring consistency between pixel data association and prior poses helps
maintain both robustness and accuracy. For scene representation, we introduce
normal vector constraints and effective rank regularization to enforce
consistency in the direction and shape of Gaussian primitives. These
constraints are jointly optimized with the existing photometric loss to enhance
the map quality. We evaluate our approach using both public and self-collected
datasets. In terms of pose optimization, our method requires only one-third of
the time while maintaining accuracy and robustness across both datasets. In
terms of scene representation, the results show that our method significantly
outperforms conventional 3DGS pipelines. Notably, on self-collected datasets
characterized by weak or repetitive textures, our approach demonstrates
enhanced visualization capabilities and achieves superior overall performance.
Codes and data will be publicly available at
https://github.com/justinyeah/normal_shape.git.

</details>


### [119] [GEWDiff: Geometric Enhanced Wavelet-based Diffusion Model for Hyperspectral Image Super-resolution](https://arxiv.org/abs/2511.07103)
*Sirui Wang,Jiang He,Natàlia Blasco Andreo,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 本文提出了GEWDiff，一种用于高光谱图像4倍超分辨率重建的几何增强小波扩散模型，通过小波编码压缩和几何增强扩散过程，有效解决了传统扩散模型处理高维HSI的内存和几何结构理解问题。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像超分辨率是重要研究领域，但面临三个主要挑战：1)高光谱维度高，传统扩散模型内存消耗过大；2)通用生成模型缺乏对遥感图像地物拓扑几何结构的理解；3)多数扩散模型在噪声级别优化损失函数导致收敛行为不直观和生成质量不佳。

Method: 提出GEWDiff框架，包含三个核心组件：1)小波编码器-解码器，高效压缩HSI到潜在空间同时保留光谱-空间信息；2)几何增强扩散过程，在生成过程中保留几何特征；3)多级损失函数设计，指导扩散过程实现稳定收敛和提高重建保真度。

Result: 模型在保真度、光谱精度、视觉真实感和清晰度等多个维度上均展现出最先进的性能表现，验证了该方法的有效性。

Conclusion: GEWDiff成功解决了高光谱图像超分辨率中的内存效率、几何结构理解和优化收敛等关键问题，为HSI生成建模提供了新的技术路径，在多个评估指标上达到SOTA水平，具有重要的应用价值。

Abstract: Improving the quality of hyperspectral images (HSIs), such as through
super-resolution, is a crucial research area. However, generative modeling for
HSIs presents several challenges. Due to their high spectral dimensionality,
HSIs are too memory-intensive for direct input into conventional diffusion
models. Furthermore, general generative models lack an understanding of the
topological and geometric structures of ground objects in remote sensing
imagery. In addition, most diffusion models optimize loss functions at the
noise level, leading to a non-intuitive convergence behavior and suboptimal
generation quality for complex data. To address these challenges, we propose a
Geometric Enhanced Wavelet-based Diffusion Model (GEWDiff), a novel framework
for reconstructing hyperspectral images at 4-times super-resolution. A
wavelet-based encoder-decoder is introduced that efficiently compresses HSIs
into a latent space while preserving spectral-spatial information. To avoid
distortion during generation, we incorporate a geometry-enhanced diffusion
process that preserves the geometric features. Furthermore, a multi-level loss
function was designed to guide the diffusion process, promoting stable
convergence and improved reconstruction fidelity. Our model demonstrated
state-of-the-art results across multiple dimensions, including fidelity,
spectral accuracy, visual realism, and clarity.

</details>


### [120] [Integrating Reweighted Least Squares with Plug-and-Play Diffusion Priors for Noisy Image Restoration](https://arxiv.org/abs/2511.06823)
*Ji Li,Chao Wang*

Main category: cs.CV

TL;DR: 提出基于生成扩散先验的即插即用图像恢复框架，使用广义高斯尺度混合损失处理非高斯噪声，通过IRLS方法优化，在去除脉冲噪声上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有即插即用图像恢复方法主要针对高斯噪声设计，使用高斯去噪器作为近端算子，但对非高斯噪声（如脉冲噪声）的应用仍缺乏探索，需要开发能处理一般噪声类型的鲁棒图像恢复方法。

Method: 在最大后验估计框架下，引入基于广义高斯尺度混合的损失函数近似多种噪声分布，形成ℓ_q范数保真项，采用迭代重加权最小二乘方法优化，利用扩散去噪器高效执行生成先验的近端步骤。

Result: 在基准数据集上的实验结果表明，所提方法能有效去除非高斯脉冲噪声，相比现有方法实现了更优异的图像恢复性能。

Conclusion: 成功将生成扩散先验扩展到非高斯噪声场景，提出的框架具有良好的通用性和鲁棒性，为处理复杂噪声分布的图像恢复问题提供了有效解决方案。

Abstract: Existing plug-and-play image restoration methods typically employ
off-the-shelf Gaussian denoisers as proximal operators within classical
optimization frameworks based on variable splitting. Recently, denoisers
induced by generative priors have been successfully integrated into regularized
optimization methods for image restoration under Gaussian noise. However, their
application to non-Gaussian noise--such as impulse noise--remains largely
unexplored. In this paper, we propose a plug-and-play image restoration
framework based on generative diffusion priors for robust removal of general
noise types, including impulse noise. Within the maximum a posteriori (MAP)
estimation framework, the data fidelity term is adapted to the specific noise
model. Departing from the conventional least-squares loss used for Gaussian
noise, we introduce a generalized Gaussian scale mixture-based loss, which
approximates a wide range of noise distributions and leads to an $\ell_q$-norm
($0<q\leq2$) fidelity term. This optimization problem is addressed using an
iteratively reweighted least squares (IRLS) approach, wherein the proximal step
involving the generative prior is efficiently performed via a diffusion-based
denoiser. Experimental results on benchmark datasets demonstrate that the
proposed method effectively removes non-Gaussian impulse noise and achieves
superior restoration performance.

</details>


### [121] [MUGSQA: Novel Multi-Uncertainty-Based Gaussian Splatting Quality Assessment Method, Dataset, and Benchmarks](https://arxiv.org/abs/2511.06830)
*Tianang Chen,Jian Jin,Shilv Cai,Zhuangzi Li,Weisi Lin*

Main category: cs.CV

TL;DR: 本文提出了一个统一的多距离主观质量评估方法，并构建了MUGSQA数据集和两个基准测试，用于评估高斯泼洒重建3D对象的感知质量和算法鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着高斯泼洒(GS)技术的快速发展，如何客观评估不同GS方法重建的3D对象的感知质量成为了一个挑战，缺乏统一的评估标准和数据集。

Method: 提出模拟人类真实观看行为的多距离主观质量评估方法，构建考虑输入数据多种不确定性的MUGSQA数据集（包括输入视图数量和分辨率、视角距离、初始点云精度），并建立两个基准测试。

Result: 创建了MUGSQA数据集和相应的基准测试框架，用于评估GS重建方法的鲁棒性和现有质量评估指标的性能，相关代码和数据集将公开发布。

Conclusion: 该研究为GS技术建立了系统性的质量评估体系，填补了该领域缺乏标准化评估工具的空白，将促进GS重建方法的进一步发展。

Abstract: Gaussian Splatting (GS) has recently emerged as a promising technique for 3D
object reconstruction, delivering high-quality rendering results with
significantly improved reconstruction speed. As variants continue to appear,
assessing the perceptual quality of 3D objects reconstructed with different
GS-based methods remains an open challenge. To address this issue, we first
propose a unified multi-distance subjective quality assessment method that
closely mimics human viewing behavior for objects reconstructed with GS-based
methods in actual applications, thereby better collecting perceptual
experiences. Based on it, we also construct a novel GS quality assessment
dataset named MUGSQA, which is constructed considering multiple uncertainties
of the input data. These uncertainties include the quantity and resolution of
input views, the view distance, and the accuracy of the initial point cloud.
Moreover, we construct two benchmarks: one to evaluate the robustness of
various GS-based reconstruction methods under multiple uncertainties, and the
other to evaluate the performance of existing quality assessment metrics. Our
dataset and benchmark code will be released soon.

</details>


### [122] [ConsistTalk: Intensity Controllable Temporally Consistent Talking Head Generation with Diffusion Noise Search](https://arxiv.org/abs/2511.06833)
*Zhenjie Liu,Jianzhang Lu,Renjie Lu,Cong Liang,Shangfei Wang*

Main category: cs.CV

TL;DR: ConsistTalk是一个新颖的强度可控且时间一致的头像生成框架，通过扩散噪声搜索推理来解决当前音频驱动肖像动画中的闪烁、身份漂移和视听同步问题。


<details>
  <summary>Details</summary>
Motivation: 当前视频扩散模型在音频驱动肖像动画方面取得了显著进展，但仍存在闪烁、身份漂移和视听同步不佳等问题。这些问题主要源于纠缠的外观-运动表示和不稳定的推理策略。

Method: ConsistTalk包含三个核心组件：1) 光流引导的时间模块(OFT)，通过利用面部光流将运动特征从静态外观中解耦，减少视觉闪烁；2) 通过多模态师生知识蒸馏获得的音频到强度(A2I)模型，将音频和面部速度特征转换为逐帧强度序列；3) 扩散噪声初始化策略(IC-Init)，通过在推理时噪声搜索过程中强制背景连贯性和运动连续性约束，实现更好的身份保持。

Result: 大量实验表明，ConsistTalk在减少闪烁、保持身份以及生成时间稳定、高保真度的说话人头像视频方面显著优于先前方法。

Conclusion: ConsistTalk通过改进的表示解耦和推理策略，成功解决了当前说话人头像生成方法中的关键问题，为音频驱动的肖像动画提供了更稳定和高质量的解决方案。

Abstract: Recent advancements in video diffusion models have significantly enhanced
audio-driven portrait animation. However, current methods still suffer from
flickering, identity drift, and poor audio-visual synchronization. These issues
primarily stem from entangled appearance-motion representations and unstable
inference strategies. In this paper, we introduce \textbf{ConsistTalk}, a novel
intensity-controllable and temporally consistent talking head generation
framework with diffusion noise search inference. First, we propose \textbf{an
optical flow-guided temporal module (OFT)} that decouples motion features from
static appearance by leveraging facial optical flow, thereby reducing visual
flicker and improving temporal consistency. Second, we present an
\textbf{Audio-to-Intensity (A2I) model} obtained through multimodal
teacher-student knowledge distillation. By transforming audio and facial
velocity features into a frame-wise intensity sequence, the A2I model enables
joint modeling of audio and visual motion, resulting in more natural dynamics.
This further enables fine-grained, frame-wise control of motion dynamics while
maintaining tight audio-visual synchronization. Third, we introduce a
\textbf{diffusion noise initialization strategy (IC-Init)}. By enforcing
explicit constraints on background coherence and motion continuity during
inference-time noise search, we achieve better identity preservation and refine
motion dynamics compared to the current autoregressive strategy. Extensive
experiments demonstrate that ConsistTalk significantly outperforms prior
methods in reducing flicker, preserving identity, and delivering temporally
stable, high-fidelity talking head videos.

</details>


### [123] [Federated Learning for Video Violence Detection: Complementary Roles of Lightweight CNNs and Vision-Language Models for Energy-Efficient Use](https://arxiv.org/abs/2511.07171)
*Sébastien Thuau,Siba Haidar,Rachid Chelouah*

Main category: cs.CV

TL;DR: 比较了联邦学习中三种暴力检测策略：预训练VLM零样本推理、LoRA微调LLaVA和个性化3D CNN，发现3D CNN在能效和校准方面更优，而VLM提供更丰富的多模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 深度学习视频监控需要保护隐私且低计算开销的架构，联邦学习虽保护隐私但部署大型视觉语言模型带来巨大能耗和可持续性挑战，需要寻找能效与性能的平衡点。

Method: 在RWF-2000和RLVS数据集的非IID分割上对比三种策略：预训练VLM零样本推理、LoRA微调LLaVA-NeXT-Video-7B、以及65.8M参数3D CNN的个性化联邦学习，并在UCF-Crime上测试层次分组优化。

Result: 所有方法在二元暴力检测上均超90%准确率；3D CNN实现更优校准(ROC AUC 92.59%)且能耗约为联邦LoRA的一半(240 Wh vs 570 Wh)；VLM提供更丰富多模态推理；层次分组将VLM多类准确率从65.31%提升至81%。

Conclusion: 首次对LoRA调优VLM和个性化CNN进行联邦暴力检测对比研究，明确量化能耗和CO2排放，结果支持混合部署策略：常规推理使用高效CNN，复杂情境推理选择性启用VLM。

Abstract: Deep learning-based video surveillance increasingly demands
privacy-preserving architectures with low computational and environmental
overhead. Federated learning preserves privacy but deploying large
vision-language models (VLMs) introduces major energy and sustainability
challenges. We compare three strategies for federated violence detection under
realistic non-IID splits on the RWF-2000 and RLVS datasets: zero-shot inference
with pretrained VLMs, LoRA-based fine-tuning of LLaVA-NeXT-Video-7B, and
personalized federated learning of a 65.8M-parameter 3D CNN. All methods exceed
90% accuracy in binary violence detection. The 3D CNN achieves superior
calibration (ROC AUC 92.59%) at roughly half the energy cost (240 Wh vs. 570
Wh) of federated LoRA, while VLMs provide richer multimodal reasoning.
Hierarchical category grouping (based on semantic similarity and class
exclusion) boosts VLM multiclass accuracy from 65.31% to 81% on the UCF-Crime
dataset. To our knowledge, this is the first comparative simulation study of
LoRA-tuned VLMs and personalized CNNs for federated violence detection, with
explicit energy and CO2e quantification. Our results inform hybrid deployment
strategies that default to efficient CNNs for routine inference and selectively
engage VLMs for complex contextual reasoning.

</details>


### [124] [PanoNav: Mapless Zero-Shot Object Navigation with Panoramic Scene Parsing and Dynamic Memory](https://arxiv.org/abs/2511.06840)
*Qunchao Jin,Yilin Wu,Changhao Chen*

Main category: cs.CV

TL;DR: PanoNav是一个完全基于RGB、无需地图的零样本物体导航框架，通过全景场景解析和记忆引导决策机制，在导航性能上显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本物体导航方法存在两大局限：依赖深度传感器或预构建地图限制了多模态大语言模型的空间推理能力；无地图方法通常做出短视决策，缺乏历史上下文导致局部死锁。

Method: 提出PanoNav框架，包含两个核心组件：1）全景场景解析模块，从全景RGB输入中释放MLLMs的空间解析潜力；2）记忆引导决策机制，通过动态有界记忆队列整合探索历史，避免局部死锁。

Result: 在公开导航基准测试上，PanoNav在成功率(SR)和加权路径长度成功率(SPL)两个指标上均显著优于代表性基线方法。

Conclusion: PanoNav成功解决了现有方法的局限性，实现了纯RGB、无地图的导航，通过增强的空间推理和历史上下文整合能力，为家庭机器人的零样本物体导航提供了更有效的解决方案。

Abstract: Zero-shot object navigation (ZSON) in unseen environments remains a
challenging problem for household robots, requiring strong perceptual
understanding and decision-making capabilities. While recent methods leverage
metric maps and Large Language Models (LLMs), they often depend on depth
sensors or prebuilt maps, limiting the spatial reasoning ability of Multimodal
Large Language Models (MLLMs). Mapless ZSON approaches have emerged to address
this, but they typically make short-sighted decisions, leading to local
deadlocks due to a lack of historical context. We propose PanoNav, a fully
RGB-only, mapless ZSON framework that integrates a Panoramic Scene Parsing
module to unlock the spatial parsing potential of MLLMs from panoramic RGB
inputs, and a Memory-guided Decision-Making mechanism enhanced by a Dynamic
Bounded Memory Queue to incorporate exploration history and avoid local
deadlocks. Experiments on the public navigation benchmark show that PanoNav
significantly outperforms representative baselines in both SR and SPL metrics.

</details>


### [125] [Leveraging Text-Driven Semantic Variation for Robust OOD Segmentation](https://arxiv.org/abs/2511.07238)
*Seungheon Song,Jaekoo Lee*

Main category: cs.CV

TL;DR: 提出了一种基于视觉语言模型的文本驱动OOD分割方法，在多个公开数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶和机器人技术中，确保道路安全和可靠决策关键依赖于分布外（OOD）分割。尽管已有许多方法被提出来检测道路上的异常物体，但利用视觉语言空间中丰富语言知识的方法仍然是一个未被充分探索的领域。假设在真实世界自动驾驶场景的复杂背景下，结合这些语言提示可能特别有益。

Method: 提出了一种新颖的方法，训练文本驱动OOD分割模型来学习视觉语言空间中语义多样化的物体集合。具体而言，该方法将视觉语言模型的编码器与transformer解码器相结合，使用位于与分布内（ID）类不同语义距离的基于距离的OOD提示，并利用OOD语义增强来获得OOD表示。通过对齐视觉和文本信息，该方法有效泛化到未见物体并提供强大的OOD分割。

Result: 在Fishyscapes、Segment-Me-If-You-Can和Road Anomaly等多个公开OOD分割数据集上进行了大量实验，证明该方法在像素级和物体级评估中都实现了最先进的性能。

Conclusion: 这一结果突显了基于视觉语言的OOD分割在增强未来自动驾驶系统安全性和可靠性方面的潜力。

Abstract: In autonomous driving and robotics, ensuring road safety and reliable
decision-making critically depends on out-of-distribution (OOD) segmentation.
While numerous methods have been proposed to detect anomalous objects on the
road, leveraging the vision-language space-which provides rich linguistic
knowledge-remains an underexplored field. We hypothesize that incorporating
these linguistic cues can be especially beneficial in the complex contexts
found in real-world autonomous driving scenarios.
  To this end, we present a novel approach that trains a Text-Driven OOD
Segmentation model to learn a semantically diverse set of objects in the
vision-language space. Concretely, our approach combines a vision-language
model's encoder with a transformer decoder, employs Distance-Based OOD prompts
located at varying semantic distances from in-distribution (ID) classes, and
utilizes OOD Semantic Augmentation for OOD representations. By aligning visual
and textual information, our approach effectively generalizes to unseen objects
and provides robust OOD segmentation in diverse driving environments.
  We conduct extensive experiments on publicly available OOD segmentation
datasets such as Fishyscapes, Segment-Me-If-You-Can, and Road Anomaly datasets,
demonstrating that our approach achieves state-of-the-art performance across
both pixel-level and object-level evaluations. This result underscores the
potential of vision-language-based OOD segmentation to bolster the safety and
reliability of future autonomous driving systems.

</details>


### [126] [Aerial Image Stitching Using IMU Data from a UAV](https://arxiv.org/abs/2511.06841)
*Selim Ahmet Iz,Mustafa Unel*

Main category: cs.CV

TL;DR: 本文提出了一种结合IMU数据和计算机视觉技术的无人机图像拼接新方法，通过估计位移、旋转和透视校正来提高拼接精度，在挑战性场景下优于传统特征基算法。


<details>
  <summary>Details</summary>
Motivation: 无人机航拍和遥感应用中，传统基于特征的图像拼接算法在特征检测和匹配过程中存在错误和歧义，特别是在大位移、旋转和相机姿态变化等挑战性场景下效果不佳。

Method: 结合IMU数据和计算机视觉技术，通过估计连续图像间的位移和旋转、透视畸变校正、计算单应矩阵等步骤，最后使用标准图像拼接算法对齐和融合图像。

Result: 实验证明该方法在精度和可靠性方面优于现有的基于特征的图像拼接算法，特别是在大位移、旋转和相机姿态变化等挑战性场景下表现更佳。

Conclusion: 该方法充分利用IMU数据提供的额外信息，有效校正各种畸变，可轻松集成到现有无人机工作流程中，为无人机图像拼接提供了一种更鲁棒和准确的解决方案。

Abstract: Unmanned Aerial Vehicles (UAVs) are widely used for aerial photography and
remote sensing applications. One of the main challenges is to stitch together
multiple images into a single high-resolution image that covers a large area.
Featurebased image stitching algorithms are commonly used but can suffer from
errors and ambiguities in feature detection and matching. To address this,
several approaches have been proposed, including using bundle adjustment
techniques or direct image alignment. In this paper, we present a novel method
that uses a combination of IMU data and computer vision techniques for
stitching images captured by a UAV. Our method involves several steps such as
estimating the displacement and rotation of the UAV between consecutive images,
correcting for perspective distortion, and computing a homography matrix. We
then use a standard image stitching algorithm to align and blend the images
together. Our proposed method leverages the additional information provided by
the IMU data, corrects for various sources of distortion, and can be easily
integrated into existing UAV workflows. Our experiments demonstrate the
effectiveness and robustness of our method, outperforming some of the existing
feature-based image stitching algorithms in terms of accuracy and reliability,
particularly in challenging scenarios such as large displacements, rotations,
and variations in camera pose.

</details>


### [127] [MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal LLMs](https://arxiv.org/abs/2511.07250)
*Tianhao Peng,Haochen Wang,Yuanxing Zhang,Zekun Wang,Zili Wang,Ge Zhang,Jian Yang,Shihao Li,Yanghai Wang,Xintao Wang,Houyi Li,Wei Ji,Pengfei Wan,Wenhao Huang,Zhaoxiang Zhang,Jiaheng Liu*

Main category: cs.CV

TL;DR: 研究推出了首个多视频理解评估基准MVU-Eval，包含1,824个问答对覆盖4,959个视频，揭示了现有MLLM模型在多视频理解方面的显著性能差距和局限性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型评估基准仅限于单视频理解，而现实场景（如体育分析、自动驾驶）迫切需要多视频理解能力，存在显著的评估空白。

Method: 构建MVU-Eval评估基准，通过8个核心能力维度评估，包含1,824个精心策划的问答对，涵盖4,959个来自不同领域的视频，涉及基础感知和高阶推理任务。

Result: 对最先进的开源和闭源模型进行广泛评估，揭示了当前MLLM在跨视频理解方面存在显著性能差异和局限性。

Conclusion: MVU-Eval填补了多视频理解评估的空白，将公开提供以促进未来研究，为提升MLLM在现实多视频场景中的表现奠定基础。

Abstract: The advent of Multimodal Large Language Models (MLLMs) has expanded AI
capabilities to visual modalities, yet existing evaluation benchmarks remain
limited to single-video understanding, overlooking the critical need for
multi-video understanding in real-world scenarios (e.g., sports analytics and
autonomous driving). To address this significant gap, we introduce MVU-Eval,
the first comprehensive benchmark for evaluating Multi-Video Understanding for
MLLMs. Specifically, our MVU-Eval mainly assesses eight core competencies
through 1,824 meticulously curated question-answer pairs spanning 4,959 videos
from diverse domains, addressing both fundamental perception tasks and
high-order reasoning tasks. These capabilities are rigorously aligned with
real-world applications such as multi-sensor synthesis in autonomous systems
and cross-angle sports analytics. Through extensive evaluation of
state-of-the-art open-source and closed-source models, we reveal significant
performance discrepancies and limitations in current MLLMs' ability to perform
understanding across multiple videos. The benchmark will be made publicly
available to foster future research.

</details>


### [128] [Gaussian-Augmented Physics Simulation and System Identification with Complex Colliders](https://arxiv.org/abs/2511.06846)
*Federico Vasile,Ri-Zhao Qiu,Lorenzo Natale,Xiaolong Wang*

Main category: cs.CV

TL;DR: 提出AS-DiffMPM，一个可处理任意形状碰撞体的可微分材质点方法框架，用于从视频中进行物理属性估计。


<details>
  <summary>Details</summary>
Motivation: 现有的可微分MPM方法局限于平面碰撞体，无法处理物体与非平面表面碰撞等更具挑战性的场景，限制了系统识别在复杂环境中的应用。

Method: 通过引入可微分碰撞处理机制扩展现有方法，使目标物体能够与复杂刚体交互，同时保持端到端优化能力，并可轻松集成到各种新视图合成方法中。

Result: AS-DiffMPM成功实现了任意形状碰撞体的物理属性估计，为从视觉观测进行系统识别提供了一个灵活的框架，扩展了可微分MPM在复杂物体-环境交互中的应用能力。

Conclusion: 该框架显著提升了可微分MPM处理复杂几何形状的能力，为机器人学和图形学中的系统识别任务提供了更强大和通用的解决方案。

Abstract: System identification involving the geometry, appearance, and physical
properties from video observations is a challenging task with applications in
robotics and graphics. Recent approaches have relied on fully differentiable
Material Point Method (MPM) and rendering for simultaneous optimization of
these properties. However, they are limited to simplified object-environment
interactions with planar colliders and fail in more challenging scenarios where
objects collide with non-planar surfaces. We propose AS-DiffMPM, a
differentiable MPM framework that enables physical property estimation with
arbitrarily shaped colliders. Our approach extends existing methods by
incorporating a differentiable collision handling mechanism, allowing the
target object to interact with complex rigid bodies while maintaining
end-to-end optimization. We show AS-DiffMPM can be easily interfaced with
various novel view synthesis methods as a framework for system identification
from visual observations.

</details>


### [129] [Glioma C6: A Novel Dataset for Training and Benchmarking Cell Segmentation](https://arxiv.org/abs/2511.07286)
*Roman Malashin,Svetlana Pashkevich,Daniil Ilyukhin,Arseniy Volkov,Valeria Yachnaya,Andrey Denisov,Maria Mikhalkova*

Main category: cs.CV

TL;DR: 本研究提出了Glioma C6数据集，这是一个新的开源数据集，专门用于胶质瘤C6细胞的实例分割，可作为深度学习模型的基准测试和训练资源。


<details>
  <summary>Details</summary>
Motivation: 为生物医学图像分析领域提供 realistic 的测试平台，解决现有数据集不足的问题，促进癌细胞研究的发展，特别是在胶质瘤细胞的形态学分析方面。

Method: 构建包含75张高分辨率相差显微镜图像的数据集，涵盖超过12,000个标注细胞。数据集包含生物学家提供的体细胞注释和形态学细胞分类。数据集分为两部分：第一部分使用受控参数进行基准测试，第二部分在不同条件下支持泛化测试。评估了多个通用分割模型在该数据集上的性能。

Result: 实验表明，在Glioma C6数据集上训练能显著提升分割性能，突出了该数据集对于开发和评估深度学习模型的价值。通用分割模型在该数据集上表现出局限性，证明了专门训练的必要性。

Conclusion: Glioma C6数据集为开发稳健且可泛化的模型提供了重要资源，其双重设计（基准测试+泛化测试）和详细的形态学分类使其成为癌症细胞研究的重要工具，已向研究人员公开提供。

Abstract: We present Glioma C6, a new open dataset for instance segmentation of glioma
C6 cells, designed as both a benchmark and a training resource for deep
learning models. The dataset comprises 75 high-resolution phase-contrast
microscopy images with over 12,000 annotated cells, providing a realistic
testbed for biomedical image analysis. It includes soma annotations and
morphological cell categorization provided by biologists. Additional
categorization of cells, based on morphology, aims to enhance the utilization
of image data for cancer cell research. Glioma C6 consists of two parts: the
first is curated with controlled parameters for benchmarking, while the second
supports generalization testing under varying conditions. We evaluate the
performance of several generalist segmentation models, highlighting their
limitations on our dataset. Our experiments demonstrate that training on Glioma
C6 significantly enhances segmentation performance, reinforcing its value for
developing robust and generalizable models. The dataset is publicly available
for researchers.

</details>


### [130] [Distillation Dynamics: Towards Understanding Feature-Based Distillation in Vision Transformers](https://arxiv.org/abs/2511.06848)
*Huiyuan Tian,Bonan Xu Shijian Li*

Main category: cs.CV

TL;DR: 传统基于特征的知识蒸馏在Vision Transformers上意外失败，表现甚至不如简单的基于logit的蒸馏。研究发现ViTs具有独特的U型信息处理模式，师生模型之间存在根本性的表征范式不匹配。


<details>
  <summary>Details</summary>
Motivation: 尽管基于特征的知识蒸馏在压缩CNNs方面非常有效，但当应用于Vision Transformers时却意外失败，这个现象需要深入分析以解决ViTs的有效压缩问题。

Method: 提出"蒸馏动力学"分析框架，结合频谱分析、信息熵度量和激活幅度追踪来研究现象。通过频域分析揭示教师模型在后期层采用分布式高维编码策略。

Result: 发现ViTs表现出独特的U型信息处理模式（初始压缩后扩张），识别出特征蒸馏负迁移的根本原因是师生模型间的表征范式不匹配，后期层特征对齐会损害学生模型性能。

Conclusion: 成功的ViTs知识转移需要超越简单的特征模仿，采用尊重这些基本表征约束的方法，为设计有效的ViTs压缩策略提供了重要的理论指导。

Abstract: While feature-based knowledge distillation has proven highly effective for
compressing CNNs, these techniques unexpectedly fail when applied to Vision
Transformers (ViTs), often performing worse than simple logit-based
distillation. We provide the first comprehensive analysis of this phenomenon
through a novel analytical framework termed as ``distillation dynamics",
combining frequency spectrum analysis, information entropy metrics, and
activation magnitude tracking. Our investigation reveals that ViTs exhibit a
distinctive U-shaped information processing pattern: initial compression
followed by expansion. We identify the root cause of negative transfer in
feature distillation: a fundamental representational paradigm mismatch between
teacher and student models. Through frequency-domain analysis, we show that
teacher models employ distributed, high-dimensional encoding strategies in
later layers that smaller student models cannot replicate due to limited
channel capacity. This mismatch causes late-layer feature alignment to actively
harm student performance. Our findings reveal that successful knowledge
transfer in ViTs requires moving beyond naive feature mimicry to methods that
respect these fundamental representational constraints, providing essential
theoretical guidance for designing effective ViTs compression strategies. All
source code and experimental logs are provided in the supplementary material.

</details>


### [131] [LMM-IQA: Image Quality Assessment for Low-Dose CT Imaging](https://arxiv.org/abs/2511.07298)
*Kagan Celik,Mehmet Ozan Unal,Metin Ertas,Isa Yildirim*

Main category: cs.CV

TL;DR: 本文提出了一种基于大语言模型(LLM)的低剂量CT图像质量评估系统，能够生成数值评分和噪声、模糊、对比度损失等退化描述。


<details>
  <summary>Details</summary>
Motivation: 低剂量CT降低了辐射剂量但导致噪声增加、模糊和对比度损失，影响诊断质量，因此需要一致且鲁棒的图像质量评估方法。

Method: 采用LLM构建质量评估系统，系统研究从零样本到元数据整合和错误反馈等多种推理策略，逐步提升性能。

Result: 系统产生高度相关且可解释的评估结果，包括数值评分和文本描述，为临床工作流程增加了价值。

Conclusion: 基于LLM的低剂量CT质量评估系统有效且实用，多种推理策略的组合优化了性能，代码已开源供研究使用。

Abstract: Low-dose computed tomography (CT) represents a significant improvement in
patient safety through lower radiation doses, but increased noise, blur, and
contrast loss can diminish diagnostic quality. Therefore, consistency and
robustness in image quality assessment become essential for clinical
applications. In this study, we propose an LLM-based quality assessment system
that generates both numerical scores and textual descriptions of degradations
such as noise, blur, and contrast loss. Furthermore, various inference
strategies - from the zero-shot approach to metadata integration and error
feedback - are systematically examined, demonstrating the progressive
contribution of each method to overall performance. The resultant assessments
yield not only highly correlated scores but also interpretable output, thereby
adding value to clinical workflows. The source codes of our study are available
at https://github.com/itu-biai/lmms_ldct_iqa.

</details>


### [132] [Ambiguity-aware Truncated Flow Matching for Ambiguous Medical Image Segmentation](https://arxiv.org/abs/2511.06857)
*Fanding Li,Xiangyu Li,Xianghe Su,Xingyu Qiu,Suyu Dong,Wei Wang,Kuanquan Wang,Gongning Luo,Shuo Li*

Main category: cs.CV

TL;DR: 本文提出Ambiguity-aware Truncated Flow Matching (ATFM)方法，通过数据层次推理、高斯截断表示和分割流匹配三个核心组件，解决了模糊医学图像分割中准确性与多样性难以兼顾的挑战，实现了同时提升预测精度和多样性的效果。


<details>
  <summary>Details</summary>
Motivation: 在模糊医学图像分割(AMIS)中，同时提高预测的准确性和多样性是一个重大挑战，存在固有权衡问题。现有的截断扩散概率模型(TDPMs)虽然具有潜力，但存在准确性和多样性纠缠的问题，且预测的保真度和合理性不足，无法满足实际临床需求。

Method: ATFM包含三个核心创新：1) 数据层次推理(Data-Hierarchical Inference) - 重新定义AMIS专用推理范式，在数据分布层面增强准确性，在数据样本层面增强多样性，实现有效解耦；2) 高斯截断表示(GTR) - 将截断分布明确建模为高斯分布而非采样近似，提升预测保真度和截断分布可靠性；3) 分割流匹配(SFM) - 扩展流匹配中的语义感知流变换，增强多样性预测的合理性。

Result: 在LIDC和ISIC3数据集上的综合评估表明，ATFM显著优于现有最先进方法，GED指标提升高达12%，HM-IoU指标提升7.3%，同时实现了更高效的推理过程。

Conclusion: ATFM通过创新的推理范式和专用模型组件，成功解决了模糊医学图像分割中准确性与多样性的权衡问题，在保持高保真度的同时实现了更好的多样性预测，为医学图像分析领域提供了新的技术思路。

Abstract: A simultaneous enhancement of accuracy and diversity of predictions remains a
challenge in ambiguous medical image segmentation (AMIS) due to the inherent
trade-offs. While truncated diffusion probabilistic models (TDPMs) hold strong
potential with a paradigm optimization, existing TDPMs suffer from entangled
accuracy and diversity of predictions with insufficient fidelity and
plausibility. To address the aforementioned challenges, we propose
Ambiguity-aware Truncated Flow Matching (ATFM), which introduces a novel
inference paradigm and dedicated model components. Firstly, we propose
Data-Hierarchical Inference, a redefinition of AMIS-specific inference
paradigm, which enhances accuracy and diversity at data-distribution and
data-sample level, respectively, for an effective disentanglement. Secondly,
Gaussian Truncation Representation (GTR) is introduced to enhance both fidelity
of predictions and reliability of truncation distribution, by explicitly
modeling it as a Gaussian distribution at $T_{\text{trunc}}$ instead of using
sampling-based approximations.Thirdly, Segmentation Flow Matching (SFM) is
proposed to enhance the plausibility of diverse predictions by extending
semantic-aware flow transformation in Flow Matching (FM). Comprehensive
evaluations on LIDC and ISIC3 datasets demonstrate that ATFM outperforms SOTA
methods and simultaneously achieves a more efficient inference. ATFM improves
GED and HM-IoU by up to $12\%$ and $7.3\%$ compared to advanced methods.

</details>


### [133] [Beyond Boundaries: Leveraging Vision Foundation Models for Source-Free Object Detection](https://arxiv.org/abs/2511.07301)
*Huizai Yao,Sicheng Zhao,Pengteng Li,Yi Cui,Shuo Lu,Weiyu Guo,Yunfan Lu,Yijie Xu,Hui Xiong*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的无源数据目标检测框架，通过利用视觉基础模型作为外部知识源，设计了PGFA、PIFA和DEPF三个模块来提升特征对齐和标签质量，在六个基准测试上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有无源数据目标检测方法主要依赖源模型内部知识，限制跨域泛化能力且产生有偏伪标签，影响迁移性和判别性；而视觉基础模型虽具有强大感知能力和广泛泛化性，在无源数据目标检测场景中潜力尚未被充分挖掘。

Method: 设计三个基于视觉基础模型的模块：(1)基于块加权的全局特征对齐(PGFA)使用块相似性加权提取全局特征增强迁移性；(2)基于原型的实例特征对齐(PIFA)通过动量更新的视觉基础模型原型引导实例级对比学习；(3)双源增强伪标签融合(DEPF)通过熵感知策略融合检测视觉基础模型和教师模型的预测以生成更可靠的监督信号。

Result: 在六个基准数据集上的大量实验表明，该方法实现了无源数据目标检测的当前最先进性能，验证了集成视觉基础模型同时提升迁移性和判别性的有效性。

Conclusion: 通过充分利用视觉基础模型作为外部知识源，所提框架有效解决了传统无源数据目标检测中存在的内部知识局限和伪标签偏差问题，为无源域自适应目标检测提供了新的解决思路。

Abstract: Source-Free Object Detection (SFOD) aims to adapt a source-pretrained object
detector to a target domain without access to source data. However, existing
SFOD methods predominantly rely on internal knowledge from the source model,
which limits their capacity to generalize across domains and often results in
biased pseudo-labels, thereby hindering both transferability and
discriminability. In contrast, Vision Foundation Models (VFMs), pretrained on
massive and diverse data, exhibit strong perception capabilities and broad
generalization, yet their potential remains largely untapped in the SFOD
setting. In this paper, we propose a novel SFOD framework that leverages VFMs
as external knowledge sources to jointly enhance feature alignment and label
quality. Specifically, we design three VFM-based modules: (1) Patch-weighted
Global Feature Alignment (PGFA) distills global features from VFMs using
patch-similarity-based weighting to enhance global feature transferability; (2)
Prototype-based Instance Feature Alignment (PIFA) performs instance-level
contrastive learning guided by momentum-updated VFM prototypes; and (3)
Dual-source Enhanced Pseudo-label Fusion (DEPF) fuses predictions from
detection VFMs and teacher models via an entropy-aware strategy to yield more
reliable supervision. Extensive experiments on six benchmarks demonstrate that
our method achieves state-of-the-art SFOD performance, validating the
effectiveness of integrating VFMs to simultaneously improve transferability and
discriminability.

</details>


### [134] [VAEVQ: Enhancing Discrete Visual Tokenization through Variational Modeling](https://arxiv.org/abs/2511.06863)
*Sicheng Yang,Xing Hu,Qiang Wu,Dawei Yang*

Main category: cs.CV

TL;DR: VAEVQ是一种改进的向量量化方法，通过变分自编码器、表示一致性策略和分布一致性正则化三个组件解决传统VQ框架的问题，提升图像重建和生成性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于向量量化的框架存在三大问题：潜在空间非平滑、量化前后表示对齐弱、连续与离散域一致性差，导致码字学习不稳定和码本利用率不足，影响重建和下游生成任务性能。

Method: VAEVQ包含三个关键组件：(1)变分潜在量化(VLQ)：用VAE替换AE进行量化，利用结构化平滑潜在空间促进码字激活；(2)表示一致性策略(RCS)：自适应调节量化前后特征对齐强度，增强一致性防止过拟合；(3)分布一致性正则化(DCR)：对齐码本分布与连续潜在分布以提高利用率。

Result: 在两个基准数据集上的大量实验证明，VAEVQ在重建质量和下游生成任务上都优于现有最先进方法。

Conclusion: VAEVQ通过解决VQ框架的核心缺陷，实现了更稳定的码字学习和更好的码本利用，显著提升了重建和生成任务的性能表现。

Abstract: Vector quantization (VQ) transforms continuous image features into discrete
representations, providing compressed, tokenized inputs for generative models.
However, VQ-based frameworks suffer from several issues, such as non-smooth
latent spaces, weak alignment between representations before and after
quantization, and poor coherence between the continuous and discrete domains.
These issues lead to unstable codeword learning and underutilized codebooks,
ultimately degrading the performance of both reconstruction and downstream
generation tasks. To this end, we propose VAEVQ, which comprises three key
components: (1) Variational Latent Quantization (VLQ), replacing the AE with a
VAE for quantization to leverage its structured and smooth latent space,
thereby facilitating more effective codeword activation; (2) Representation
Coherence Strategy (RCS), adaptively modulating the alignment strength between
pre- and post-quantization features to enhance consistency and prevent
overfitting to noise; and (3) Distribution Consistency Regularization (DCR),
aligning the entire codebook distribution with the continuous latent
distribution to improve utilization. Extensive experiments on two benchmark
datasets demonstrate that VAEVQ outperforms state-of-the-art methods.

</details>


### [135] [Generating an Image From 1,000 Words: Enhancing Text-to-Image With Structured Captions](https://arxiv.org/abs/2511.06876)
*Eyal Gutflaish,Eliran Kachlon,Hezi Zisman,Tal Hacham,Nimrod Sarid,Alexander Visheratin,Saar Huberman,Gal Davidi,Guy Bukchin,Kfir Goldberg,Ron Mokady*

Main category: cs.CV

TL;DR: 该论文提出了首个在长结构化标注上训练的开源文本到图像模型FIBO，通过DimFusion机制和TaBR评估协议，解决了现有模型在稀疏输入与丰富输出间的可控性问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型主要处理短提示词，导致稀疏文本输入与丰富视觉输出之间存在匹配差距，降低了模型的可控性，模型会随意填充缺失细节，偏向平均用户偏好，限制了专业应用的精确性。

Method: 1) 在相同细粒度属性集标注的长结构化标注上训练模型；2) 提出DimFusion融合机制，高效集成轻量级LLM的中间令牌而不增加令牌长度；3) 引入Text-as-a-Bottleneck Reconstruction (TaBR)评估协议，通过图像-标注生成循环测量可控性和表达力。

Result: 训练的大规模模型FIBO在开源模型中实现了最先进的提示词对齐性能，能够处理长标注并提供解耦的视觉因子控制，模型权重已公开。

Conclusion: 通过长结构化标注训练和创新的融合机制，成功提升了文本到图像模型的可控性和表达力，为专业应用提供了更精确的生成控制，并通过新的评估方法验证了改进效果。

Abstract: Text-to-image models have rapidly evolved from casual creative tools to
professional-grade systems, achieving unprecedented levels of image quality and
realism. Yet, most models are trained to map short prompts into detailed
images, creating a gap between sparse textual input and rich visual outputs.
This mismatch reduces controllability, as models often fill in missing details
arbitrarily, biasing toward average user preferences and limiting precision for
professional use. We address this limitation by training the first open-source
text-to-image model on long structured captions, where every training sample is
annotated with the same set of fine-grained attributes. This design maximizes
expressive coverage and enables disentangled control over visual factors. To
process long captions efficiently, we propose DimFusion, a fusion mechanism
that integrates intermediate tokens from a lightweight LLM without increasing
token length. We also introduce the Text-as-a-Bottleneck Reconstruction (TaBR)
evaluation protocol. By assessing how well real images can be reconstructed
through a captioning-generation loop, TaBR directly measures controllability
and expressiveness, even for very long captions where existing evaluation
methods fail. Finally, we demonstrate our contributions by training the
large-scale model FIBO, achieving state-of-the-art prompt alignment among
open-source models. Model weights are publicly available at
https://huggingface.co/briaai/FIBO

</details>


### [136] [A Two-Stage System for Layout-Controlled Image Generation using Large Language Models and Diffusion Models](https://arxiv.org/abs/2511.06888)
*Jan-Hendrik Koch,Jonas Krumme,Konrad Gadzicki*

Main category: cs.CV

TL;DR: 本文提出了一种两阶段系统来解决文本到图像扩散模型在物体数量和空间布局控制方面的局限性。第一阶段使用LLM生成结构化布局，第二阶段通过布局条件扩散模型合成符合布局的真实感图像。研究发现任务分解至关重要，并通过比较ControlNet和GLIGEN方法，实现了物体99.9%的召回率和精确的布局控制。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型虽然具有出色的生成能力，但在精确控制物体数量和空间布局方面存在明显不足。现有模型无法可靠地生成包含特定数量物体的图像，也难以准确控制物体在场景中的空间关系，这限制了其在需要精确构图的合成任务中的应用。

Method: 采用两阶段解耦方法：第一阶段使用大语言模型从物体列表生成结构化布局，通过任务分解策略简化初始生成并采用基于规则的插入完善布局；第二阶段使用布局条件扩散模型进行图像合成，比较了ControlNet和GLIGEN两种主要条件方法，并在餐桌摆设数据集上进行领域特定微调。

Result: 任务分解策略显著改善了复杂场景的物体召回率，从57.2%提升到99.9%。在图像合成方面发现关键权衡：ControlNet保持文本风格控制但存在物体幻觉问题，GLIGEN提供更优的布局保真度但牺牲了基于提示的控制能力。端到端系统成功生成具有指定物体数量和合理空间布局的图像。

Conclusion: 证明了基于解耦方法的组合控制合成在技术上是可行的，通过将空间规划与图像生成分离，有效解决了现有文本到图像模型在精确构图控制方面的根本限制。这种方法为实现更可控和可靠的图像生成系统提供了新的方向。

Abstract: Text-to-image diffusion models exhibit remarkable generative capabilities,
but lack precise control over object counts and spatial arrangements. This work
introduces a two-stage system to address these compositional limitations. The
first stage employs a Large Language Model (LLM) to generate a structured
layout from a list of objects. The second stage uses a layout-conditioned
diffusion model to synthesize a photorealistic image adhering to this layout.
We find that task decomposition is critical for LLM-based spatial planning; by
simplifying the initial generation to core objects and completing the layout
with rule-based insertion, we improve object recall from 57.2% to 99.9% for
complex scenes. For image synthesis, we compare two leading conditioning
methods: ControlNet and GLIGEN. After domain-specific finetuning on
table-setting datasets, we identify a key trade-off: ControlNet preserves
text-based stylistic control but suffers from object hallucination, while
GLIGEN provides superior layout fidelity at the cost of reduced prompt-based
controllability. Our end-to-end system successfully generates images with
specified object counts and plausible spatial arrangements, demonstrating the
viability of a decoupled approach for compositionally controlled synthesis.

</details>


### [137] [Inference-Time Scaling of Diffusion Models for Infrared Data Generation](https://arxiv.org/abs/2511.07362)
*Kai A. Horstmann,Maxim Clouser,Kia Khezeli*

Main category: cs.CV

TL;DR: 该研究通过推理时缩放方法，使用领域自适应CLIP验证器改进红外图像生成质量，在KAIST数据集上将FID分数降低10%。


<details>
  <summary>Details</summary>
Motivation: 红外图像在低可见度条件下具有温度感知优势，但缺乏高质量标注数据阻碍了红外视觉模型的发展。训练基础级扩散模型在红外领域面临数据限制挑战。

Method: 采用参数高效技术微调FLUX.1-dev模型，在少量红外图像样本上训练领域自适应CLIP验证器，在推理时引导扩散采样过程生成更高质量的红外图像。

Result: 在KAIST多光谱行人检测基准测试中，该方法将FID分数降低10%，相比无引导基线实现了生成质量的持续改进。

Conclusion: 推理时指导技术为弥合低数据红外设置中的领域差异提供了有前景的方向，有效解决了红外图像生成中的数据稀缺问题。

Abstract: Infrared imagery enables temperature-based scene understanding using passive
sensors, particularly under conditions of low visibility where traditional RGB
imaging fails. Yet, developing downstream vision models for infrared
applications is hindered by the scarcity of high-quality annotated data, due to
the specialized expertise required for infrared annotation. While synthetic
infrared image generation has the potential to accelerate model development by
providing large-scale, diverse training data, training foundation-level
generative diffusion models in the infrared domain has remained elusive due to
limited datasets. In light of such data constraints, we explore an
inference-time scaling approach using a domain-adapted CLIP-based verifier for
enhanced infrared image generation quality. We adapt FLUX.1-dev, a
state-of-the-art text-to-image diffusion model, to the infrared domain by
finetuning it on a small sample of infrared images using parameter-efficient
techniques. The trained verifier is then employed during inference to guide the
diffusion sampling process toward higher quality infrared generations that
better align with input text prompts. Empirically, we find that our approach
leads to consistent improvements in generation quality, reducing FID scores on
the KAIST Multispectral Pedestrian Detection Benchmark dataset by 10% compared
to unguided baseline samples. Our results suggest that inference-time guidance
offers a promising direction for bridging the domain gap in low-data infrared
settings.

</details>


### [138] [Adaptive Morph-Patch Transformer for Arotic Vessel Segmentation](https://arxiv.org/abs/2511.06897)
*Zhenxi Zhang,Fuchen Zheng,Adnan Iltaf,Yifei Han,Zhenyu Cheng,Yue Du,Bin Li,Tianyong Liu,Shoujun Zhou*

Main category: cs.CV

TL;DR: 提出了自适应形态块Transformer (MPT)，用于主动脉血管分割，通过自适应块划分策略和语义聚类注意力方法改善复杂血管结构的分割精度


<details>
  <summary>Details</summary>
Motivation: 传统Transformer模型使用固定尺寸矩形块处理复杂血管结构时，会影响血管结构的完整性，导致分割精度不够理想，而主动脉血管的精确分割对心血管疾病诊断和治疗至关重要

Method: MPT架构引入两个关键创新：1）自适应块划分策略，动态生成与复杂血管结构对齐的形态感知块，保持血管结构的语义完整性；2）语义聚类注意力(SCA)方法，动态聚合具有相似语义特征的不同块的特征，增强对不同大小血管的分割能力

Result: 在三个开源数据集(AVT、AortaSeg24和TBAD)上的广泛实验证明，MPT达到了最先进的性能，在复杂血管结构分割方面有显著改善

Conclusion: MPT通过自适应形态块划分和语义聚类注意力，有效保持血管结构完整性，提升了主动脉血管分割的准确性，为心血管疾病诊断提供了更好的技术支持

Abstract: Accurate segmentation of aortic vascular structures is critical for
diagnosing and treating cardiovascular diseases.Traditional Transformer-based
models have shown promise in this domain by capturing long-range dependencies
between vascular features. However, their reliance on fixed-size rectangular
patches often influences the integrity of complex vascular structures, leading
to suboptimal segmentation accuracy. To address this challenge, we propose the
adaptive Morph Patch Transformer (MPT), a novel architecture specifically
designed for aortic vascular segmentation. Specifically, MPT introduces an
adaptive patch partitioning strategy that dynamically generates
morphology-aware patches aligned with complex vascular structures. This
strategy can preserve semantic integrity of complex vascular structures within
individual patches. Moreover, a Semantic Clustering Attention (SCA) method is
proposed to dynamically aggregate features from various patches with similar
semantic characteristics. This method enhances the model's capability to
segment vessels of varying sizes, preserving the integrity of vascular
structures. Extensive experiments on three open-source dataset(AVT, AortaSeg24
and TBAD) demonstrate that MPT achieves state-of-the-art performance, with
improvements in segmenting intricate vascular structures.

</details>


### [139] [Real-Time LiDAR Super-Resolution via Frequency-Aware Multi-Scale Fusion](https://arxiv.org/abs/2511.07377)
*June Moh Goo,Zichao Zeng,Jan Boehm*

Main category: cs.CV

TL;DR: FLASH是一个新颖的LiDAR超分辨率框架，通过双域处理（空间域+频域）和自适应多尺度融合，在保持单次推理效率的同时实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于transformer的LiDAR超分辨率方法（如TULIP）仅限于空间域处理且接收野受限，无法充分利用低分辨率LiDAR数据的频域特征，亟需更有效的处理框架来从经济传感器实现高质量3D感知。

Method: FLASH框架包含两个核心创新：(1) Frequency-Aware Window Attention将局部空间注意力与全局频域分析(FFT)结合，以对数线性复杂度捕获细粒度几何和周期性扫描模式；(2) Adaptive Multi-Scale Fusion用学习的位置特定特征聚合替代传统跳跃连接，并引入CBAM注意力实现动态特征选择。

Result: 在KITTI数据集上，FLASH在所有评估指标上均达到最先进性能，超越了需要多次前向传播的不确定性增强基线，在保持单次推理效率的同时优于TULIP+Monte Carlo Dropout。

Conclusion: FLASH通过双域架构设计有效处理不确定性，避免了计算昂贵的随机推理，实现了实时部署能力，在各距离范围的一致性优势验证了其在自动驾驶系统中的实用价值。

Abstract: LiDAR super-resolution addresses the challenge of achieving high-quality 3D
perception from cost-effective, low-resolution sensors. While recent
transformer-based approaches like TULIP show promise, they remain limited to
spatial-domain processing with restricted receptive fields. We introduce FLASH
(Frequency-aware LiDAR Adaptive Super-resolution with Hierarchical fusion), a
novel framework that overcomes these limitations through dual-domain
processing. FLASH integrates two key innovations: (i) Frequency-Aware Window
Attention that combines local spatial attention with global frequency-domain
analysis via FFT, capturing both fine-grained geometry and periodic scanning
patterns at log-linear complexity. (ii) Adaptive Multi-Scale Fusion that
replaces conventional skip connections with learned position-specific feature
aggregation, enhanced by CBAM attention for dynamic feature selection.
Extensive experiments on KITTI demonstrate that FLASH achieves state-of-the-art
performance across all evaluation metrics, surpassing even uncertainty-enhanced
baselines that require multiple forward passes. Notably, FLASH outperforms
TULIP with Monte Carlo Dropout while maintaining single-pass efficiency, which
enables real-time deployment. The consistent superiority across all distance
ranges validates that our dual-domain approach effectively handles uncertainty
through architectural design rather than computationally expensive stochastic
inference, making it practical for autonomous systems.

</details>


### [140] [Mono3DVG-EnSD: Enhanced Spatial-aware and Dimension-decoupled Text Encoding for Monocular 3D Visual Grounding](https://arxiv.org/abs/2511.06908)
*Yuzhen Li,Min Liu,Zhaoyang Li,Yuan Bian,Xueping Wang,Erbo Zhai,Yaonan Wang*

Main category: cs.CV

TL;DR: 本文提出Mono3DVG-EnSD框架，通过CLIP-LCA动态掩蔽高确定性关键词和D2M维度解耦模块解决单目3D视觉定位中的文本依赖和跨维度干扰问题，在Mono3DRefer数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有单目3D视觉定位方法存在两大问题：1)过度依赖明确标识目标的高确定性关键词，忽视关键的空间描述信息；2)通用文本特征同时包含2D和3D描述信息，与单一维度视觉特征对齐时产生跨维度干扰，影响定位精度。

Method: 提出Mono3DVG-EnSD框架，包含两个核心组件：1)CLIP-Guided Lexical Certainty Adapter(CLIP-LCA)动态掩蔽高确定性关键词，强制模型关注空间关系描述；2)Dimension-Decoupled Module(D2M)解构维度特定文本特征，实现维度一致性的跨模态交互。

Result: 在Mono3DRefer数据集上的全面对比和消融实验表明，该方法在所有指标上均达到SOTA性能，特别在具有挑战性的Far(Acc@0.5)场景上显著提升13.54%，验证了方法的有效性。

Conclusion: Mono3DVG-EnSD通过动态词汇掩蔽和维度解耦策略有效解决了文本依赖和跨维度干扰问题，为单目3D视觉定位任务提供了新的解决方案，在复杂场景下表现优异。

Abstract: Monocular 3D Visual Grounding (Mono3DVG) is an emerging task that locates 3D
objects in RGB images using text descriptions with geometric cues. However,
existing methods face two key limitations. Firstly, they often over-rely on
high-certainty keywords that explicitly identify the target object while
neglecting critical spatial descriptions. Secondly, generalized textual
features contain both 2D and 3D descriptive information, thereby capturing an
additional dimension of details compared to singular 2D or 3D visual features.
This characteristic leads to cross-dimensional interference when refining
visual features under text guidance. To overcome these challenges, we propose
Mono3DVG-EnSD, a novel framework that integrates two key components: the
CLIP-Guided Lexical Certainty Adapter (CLIP-LCA) and the Dimension-Decoupled
Module (D2M). The CLIP-LCA dynamically masks high-certainty keywords while
retaining low-certainty implicit spatial descriptions, thereby forcing the
model to develop a deeper understanding of spatial relationships in captions
for object localization. Meanwhile, the D2M decouples dimension-specific
(2D/3D) textual features from generalized textual features to guide
corresponding visual features at same dimension, which mitigates
cross-dimensional interference by ensuring dimensionally-consistent cross-modal
interactions. Through comprehensive comparisons and ablation studies on the
Mono3DRefer dataset, our method achieves state-of-the-art (SOTA) performance
across all metrics. Notably, it improves the challenging Far(Acc@0.5) scenario
by a significant +13.54%.

</details>


### [141] [PADM: A Physics-aware Diffusion Model for Attenuation Correction](https://arxiv.org/abs/2511.06948)
*Trung Kien Pham,Hoang Minh Vu,Anh Duc Chu,Dac Thai Nguyen,Trung Thanh Nguyen,Thao Nguyen Truong,Mai Hong Son,Thanh Trung Nguyen,Phi Le Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的物理感知衰减校正方法(PADM)，通过师生蒸馏机制融入物理先验，无需CT即可实现心脏SPECT图像的衰减校正，并发布了包含424例患者数据的新数据集CardiAC。


<details>
  <summary>Details</summary>
Motivation: 心脏SPECT心肌灌注成像中的衰减伪影严重影响诊断准确性，而传统的混合SPECT/CT系统虽然能提供衰减校正但成本高昂、可及性有限且增加辐射暴露，需要开发无需CT的衰减校正解决方案。

Method: 提出物理感知衰减校正扩散模型(PADM)，采用基于扩散的生成方法，通过师生蒸馏机制融入显式物理先验，仅使用非衰减校正(NAC)输入即可实现衰减伪影校正，训练过程中受益于物理信息的监督。

Result: 实验表明PADM在定量指标和视觉评估上均优于最先进的生成模型，提供了更优的重建保真度，同时构建了包含424例患者研究的CardiAC数据集，包含配对的NAC/AC重建和基于CT的衰减图。

Conclusion: PADM作为一种无需CT的衰减校正解决方案，有效解决了心脏SPECT中的衰减伪影问题，在保持高诊断质量的同时克服了传统方法的局限性，具有重要的临床应用价值。

Abstract: Attenuation artifacts remain a significant challenge in cardiac Myocardial
Perfusion Imaging (MPI) using Single-Photon Emission Computed Tomography
(SPECT), often compromising diagnostic accuracy and reducing clinical
interpretability. While hybrid SPECT/CT systems mitigate these artifacts
through CT-derived attenuation maps, their high cost, limited accessibility,
and added radiation exposure hinder widespread clinical adoption. In this
study, we propose a novel CT-free solution to attenuation correction in cardiac
SPECT. Specifically, we introduce Physics-aware Attenuation Correction
Diffusion Model (PADM), a diffusion-based generative method that incorporates
explicit physics priors via a teacher--student distillation mechanism. This
approach enables attenuation artifact correction using only
Non-Attenuation-Corrected (NAC) input, while still benefiting from
physics-informed supervision during training. To support this work, we also
introduce CardiAC, a comprehensive dataset comprising 424 patient studies with
paired NAC and Attenuation-Corrected (AC) reconstructions, alongside
high-resolution CT-based attenuation maps. Extensive experiments demonstrate
that PADM outperforms state-of-the-art generative models, delivering superior
reconstruction fidelity across both quantitative metrics and visual assessment.

</details>


### [142] [Learning from the Right Patches: A Two-Stage Wavelet-Driven Masked Autoencoder for Histopathology Representation Learning](https://arxiv.org/abs/2511.06958)
*Raneen Younis,Louay Hamdi,Lukas Chavez,Zahra Ahmadi*

Main category: cs.CV

TL;DR: WISE-MAE是一种基于小波信息选择的自监督学习框架，通过粗到精的两步策略提升数字病理学中组织模式学习的效果。


<details>
  <summary>Details</summary>
Motivation: 全切片图像是数字病理学的核心，但其超大尺寸和标注稀缺性使得自监督学习至关重要。传统MAE的随机采样策略常包含无关或噪声区域，限制了模型捕捉有意义的组织模式的能力。

Method: 提出WISE-MAE框架，采用小波信息指导的补丁选择策略。通过两步粗到精过程：低倍镜下小波筛选定位结构丰富区域，然后高分辨率提取进行精细建模，模拟病理学家的诊断工作流程。

Result: 在肺部、肾脏和结直肠癌组织的多个数据集上评估显示，WISE-MAE在弱监督条件下实现了竞争力的表征质量和下游分类性能，同时保持了高效率。

Conclusion: WISE-MAE成功将结构信息和生物学相关性引入基于MAE的学习中，通过领域自适应的方法显著提升了学习表征的质量，为数字病理学提供了有效且高效的解决方案。

Abstract: Whole-slide images are central to digital pathology, yet their extreme size
and scarce annotations make self-supervised learning essential. Masked
Autoencoders (MAEs) with Vision Transformer backbones have recently shown
strong potential for histopathology representation learning. However,
conventional random patch sampling during MAE pretraining often includes
irrelevant or noisy regions, limiting the model's ability to capture meaningful
tissue patterns. In this paper, we present a lightweight and domain-adapted
framework that brings structure and biological relevance into MAE-based
learning through a wavelet-informed patch selection strategy. WISE-MAE applies
a two-step coarse-to-fine process: wavelet-based screening at low magnification
to locate structurally rich regions, followed by high-resolution extraction for
detailed modeling. This approach mirrors the diagnostic workflow of
pathologists and improves the quality of learned representations. Evaluations
across multiple cancer datasets, including lung, renal, and colorectal tissues,
show that WISE-MAE achieves competitive representation quality and downstream
classification performance while maintaining efficiency under weak supervision.

</details>


### [143] [Exploring the "Great Unseen" in Medieval Manuscripts: Instance-Level Labeling of Legacy Image Collections with Zero-Shot Models](https://arxiv.org/abs/2511.07004)
*Christofer Meinecke,Estelle Guéville,David Joseph Wrisley*

Main category: cs.CV

TL;DR: 本研究旨在通过先进技术对中世纪手稿页面进行整体分割和描述，为计算机视觉技术创建更丰富的训练数据。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对中世纪手稿的计算机视觉训练数据，需要开发更全面的方法来处理手稿页面的复杂性。

Method: 采用最先进的技术对整个手稿叶进行分割和描述，构建实例分割和多模态模型的训练数据集。

Result: 创建了一套更丰富的训练数据，可支持中世纪特定视觉内容的计算机视觉任务。

Conclusion: 整体化理论框架结合先进技术为 medieval 手稿研究提供了新的数字化分析工具。

Abstract: We aim to theorize the medieval manuscript page and its contents more
holistically, using state-of-the-art techniques to segment and describe the
entire manuscript folio, for the purpose of creating richer training data for
computer vision techniques, namely instance segmentation, and multimodal models
for medieval-specific visual content.

</details>


### [144] [Performance Decay in Deepfake Detection: The Limitations of Training on Outdated Data](https://arxiv.org/abs/2511.07009)
*Jack Richings,Margaux Leblanc,Ian Groves,Victoria Nockles*

Main category: cs.CV

TL;DR: 本文提出了一个简单的两阶段深度伪造检测方法，在当代深度伪造数据上达到99.8%以上的AUROC，但模型性能随威胁演进而显著衰退（六个月后召回率下降30%+）。分析表明：持续性能需要大型多样化数据集的持续收集，检测能力主要来自静态帧级伪影而非时序不一致性。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术质量持续提升，使恶意生成的合成内容越来越难与真实内容区分，加剧了虚假信息、欺诈和骚扰威胁。现有检测方法在应对不断演进的威胁时面临性能快速衰退的问题。

Method: 提出了一个简单而有效的两阶段检测方法，通过分析模型在不同时期生成的深度伪造数据上的性能表现，揭示了检测机制的关键特征。

Result: 该方法在当代深度伪造数据上实现99.8%以上AUROC，但在六个月后生成的深度伪造数据上召回率下降超过30%。分析发现检测能力主要来自静态帧级伪影，而非时序不一致性。

Conclusion: 有效的深度伪造检测未来取决于快速数据收集和先进帧级特征检测器的开发，需要持续维护大型多样化数据集来应对不断演进的威胁。

Abstract: The continually advancing quality of deepfake technology exacerbates the
threats of disinformation, fraud, and harassment by making
maliciously-generated synthetic content increasingly difficult to distinguish
from reality. We introduce a simple yet effective two-stage detection method
that achieves an AUROC of over 99.8% on contemporary deepfakes. However, this
high performance is short-lived. We show that models trained on this data
suffer a recall drop of over 30% when evaluated on deepfakes created with
generation techniques from just six months later, demonstrating significant
decay as threats evolve. Our analysis reveals two key insights for robust
detection. Firstly, continued performance requires the ongoing curation of
large, diverse datasets. Second, predictive power comes primarily from static,
frame-level artifacts, not temporal inconsistencies. The future of effective
deepfake detection therefore depends on rapid data collection and the
development of advanced frame-level feature detectors.

</details>


### [145] [Certified L2-Norm Robustness of 3D Point Cloud Recognition in the Frequency Domain](https://arxiv.org/abs/2511.07029)
*Liang Zhou,Qiming Wang,Tianze Chen*

Main category: cs.CV

TL;DR: FreqCert是一个基于频域的3D点云分类认证框架，通过图傅里叶变换和频谱相似性子采样，提供更强的结构化对抗扰动认证鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有3D点云分类器容易受到结构化对抗扰动和几何损坏的影响，但现有认证防御仅限制点级扰动，忽略了保持单个点不变但改变整体结构的细微几何失真，这在自动驾驶等安全关键应用中存在风险。

Method: FreqCert将鲁棒性分析从空域转向频域，首先通过图傅里叶变换(GFT)转换输入点云，然后应用结构化的频感知子采样生成多个子点云；每个子云独立分类，通过多数投票获得最终预测；子云构建基于频谱相似性而非空间邻近性。

Result: 在ModelNet40和ScanObjectNN数据集上的广泛实验表明，FreqCert在强扰动下持续实现更高的认证准确率和经验准确率，推导出认证L2鲁棒半径的闭合形式下界并证明了其在最小可解释假设下的紧致性。

Conclusion: 频谱表示为实现3D点云识别中的可认证鲁棒性提供了有效途径，频域认证方法为解决结构化对抗扰动问题建立了理论基础。

Abstract: 3D point cloud classification is a fundamental task in safety-critical
applications such as autonomous driving, robotics, and augmented reality.
However, recent studies reveal that point cloud classifiers are vulnerable to
structured adversarial perturbations and geometric corruptions, posing risks to
their deployment in safety-critical scenarios. Existing certified defenses
limit point-wise perturbations but overlook subtle geometric distortions that
preserve individual points yet alter the overall structure, potentially leading
to misclassification. In this work, we propose FreqCert, a novel certification
framework that departs from conventional spatial domain defenses by shifting
robustness analysis to the frequency domain, enabling structured certification
against global L2-bounded perturbations. FreqCert first transforms the input
point cloud via the graph Fourier transform (GFT), then applies structured
frequency-aware subsampling to generate multiple sub-point clouds. Each
sub-cloud is independently classified by a standard model, and the final
prediction is obtained through majority voting, where sub-clouds are
constructed based on spectral similarity rather than spatial proximity, making
the partitioning more stable under L2 perturbations and better aligned with the
object's intrinsic structure. We derive a closed-form lower bound on the
certified L2 robustness radius and prove its tightness under minimal and
interpretable assumptions, establishing a theoretical foundation for frequency
domain certification. Extensive experiments on the ModelNet40 and ScanObjectNN
datasets demonstrate that FreqCert consistently achieves higher certified
accuracy and empirical accuracy under strong perturbations. Our results suggest
that spectral representations provide an effective pathway toward certifiable
robustness in 3D point cloud recognition.

</details>


### [146] [From Pretrain to Pain: Adversarial Vulnerability of Video Foundation Models Without Task Knowledge](https://arxiv.org/abs/2511.07049)
*Hui Lu,Yi Yu,Song Xia,Yiming Yang,Deepu Rajan,Boon Poh Ng,Alex Kot,Xudong Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种可迁移视频攻击(TVA)方法，能够直接利用视频基础模型(VFMs)的时序表征动态来攻击下游模型，无需访问目标任务、训练数据或模型架构，在24个视频任务上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着大规模视频基础模型(VFMs)的开放获取， adversaries可以利用对VFMs的完整知识发动强大攻击。现有攻击方法需要访问目标任务、训练数据、模型查询或架构，缺乏实际可行性。本文旨在探索一种更实用的攻击场景：直接从VFMs中挖掘对抗脆弱性来攻击微调后的下游模型或多模态大语言模型。

Method: 提出可迁移视频攻击(TVA)，一种时序感知的对抗攻击方法。TVA利用VFMs的时序表征动态来制作有效扰动，集成双向对比学习机制最大化干净特征与对抗特征之间的差异，引入时序一致性损失利用运动线索增强扰动的序列影响，避免训练昂贵的替代模型或访问领域特定数据。

Result: 在24个视频相关任务上的广泛实验证明TVA对下游模型和多模态大语言模型的有效性，揭示了视频模型部署中先前未充分探索的安全漏洞。

Conclusion: TVA提供了一种更实用、高效的攻击策略，无需训练替代模型或访问特定领域数据，成功证明了VFMs部署中存在的严重安全隐患，为视频模型安全性研究提供了新的视角和挑战。

Abstract: Large-scale Video Foundation Models (VFMs) has significantly advanced various
video-related tasks, either through task-specific models or Multi-modal Large
Language Models (MLLMs). However, the open accessibility of VFMs also
introduces critical security risks, as adversaries can exploit full knowledge
of the VFMs to launch potent attacks. This paper investigates a novel and
practical adversarial threat scenario: attacking downstream models or MLLMs
fine-tuned from open-source VFMs, without requiring access to the victim task,
training data, model query, and architecture. In contrast to conventional
transfer-based attacks that rely on task-aligned surrogate models, we
demonstrate that adversarial vulnerabilities can be exploited directly from the
VFMs. To this end, we propose the Transferable Video Attack (TVA), a
temporal-aware adversarial attack method that leverages the temporal
representation dynamics of VFMs to craft effective perturbations. TVA
integrates a bidirectional contrastive learning mechanism to maximize the
discrepancy between the clean and adversarial features, and introduces a
temporal consistency loss that exploits motion cues to enhance the sequential
impact of perturbations. TVA avoids the need to train expensive surrogate
models or access to domain-specific data, thereby offering a more practical and
efficient attack strategy. Extensive experiments across 24 video-related tasks
demonstrate the efficacy of TVA against downstream models and MLLMs, revealing
a previously underexplored security vulnerability in the deployment of video
models.

</details>


### [147] [Improving Deepfake Detection with Reinforcement Learning-Based Adaptive Data Augmentation](https://arxiv.org/abs/2511.07051)
*Yuxuan Zhou,Tao Yu,Wen Huang,Yuheng Zhang,Tao Dai,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 提出CRDA框架，通过强化学习和因果推理的动态数据增强策略，显著提升deepfake检测器的跨域泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有deepfake检测器泛化能力不足，当前SOTA方法依赖固定数据增强策略，无法充分模拟现实世界中日益复杂的伪造特征（如面部扭曲、表情操作），需要动态自适应的增强方法

Method: CRDA框架结合强化学习和因果推理：1）构建可配置的伪造操作池生成增强样本；2）RL智能体根据检测器性能动态选择增强操作，实现从简单到复杂的课程学习；3）通过因果推理引导动作空间变化，抑制虚假相关性，专注于因果不变特征

Result: 在多个跨域数据集上的广泛实验表明，该方法显著提升检测器泛化性，性能超越当前SOTA方法

Conclusion: 通过将强化学习的自适应探索能力与因果推理的特征选择机制相结合，CRDA有效解决了静态增强策略的局限性，为deepfake检测提供了更鲁棒的泛化解决方案

Abstract: The generalization capability of deepfake detectors is critical for
real-world use. Data augmentation via synthetic fake face generation
effectively enhances generalization, yet current SoTA methods rely on fixed
strategies-raising a key question: Is a single static augmentation sufficient,
or does the diversity of forgery features demand dynamic approaches? We argue
existing methods overlook the evolving complexity of real-world forgeries
(e.g., facial warping, expression manipulation), which fixed policies cannot
fully simulate. To address this, we propose CRDA (Curriculum
Reinforcement-Learning Data Augmentation), a novel framework guiding detectors
to progressively master multi-domain forgery features from simple to complex.
CRDA synthesizes augmented samples via a configurable pool of forgery
operations and dynamically generates adversarial samples tailored to the
detector's current learning state. Central to our approach is integrating
reinforcement learning (RL) and causal inference. An RL agent dynamically
selects augmentation actions based on detector performance to efficiently
explore the vast augmentation space, adapting to increasingly challenging
forgeries. Simultaneously, the agent introduces action space variations to
generate heterogeneous forgery patterns, guided by causal inference to mitigate
spurious correlations-suppressing task-irrelevant biases and focusing on
causally invariant features. This integration ensures robust generalization by
decoupling synthetic augmentation patterns from the model's learned
representations. Extensive experiments show our method significantly improves
detector generalizability, outperforming SOTA methods across multiple
cross-domain datasets.

</details>


### [148] [RaLD: Generating High-Resolution 3D Radar Point Clouds with Latent Diffusion](https://arxiv.org/abs/2511.07067)
*Ruijie Zhang,Bixin Zeng,Shengpeng Wang,Fuhui Zhou,Wei Wang*

Main category: cs.CV

TL;DR: RaLD是一个基于潜在扩散模型的框架，能够从稀疏的毫米波雷达频谱生成密集准确的3D点云，解决了雷达感知中的稀疏性和低分辨率问题。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达在自动驾驶中具有成本低、恶劣条件下鲁棒性强等优势，但其点云的稀疏性和低分辨率严重限制了在需要密集3D感知任务中的应用。现有生成方法依赖密集体素表示，效率低下且难以保持结构细节。

Method: 提出RaLD框架，通过整合场景级视锥LiDAR自编码、顺序不变的潜在表示和直接的雷达频谱条件策略，首次将潜在扩散模型有效应用于基于雷达的3D生成任务。

Result: 实验证明RaLD能够从原始雷达频谱生成密集且准确的3D点云，在保持结构细节的同时提供更紧凑和表达力更强的生成过程。

Conclusion: RaLD为在挑战性环境中实现鲁棒感知提供了有前景的解决方案，有效填补了毫米波雷达3D生成领域的技术空白。

Abstract: Millimeter-wave radar offers a promising sensing modality for autonomous
systems thanks to its robustness in adverse conditions and low cost. However,
its utility is significantly limited by the sparsity and low resolution of
radar point clouds, which poses challenges for tasks requiring dense and
accurate 3D perception. Despite that recent efforts have shown great potential
by exploring generative approaches to address this issue, they often rely on
dense voxel representations that are inefficient and struggle to preserve
structural detail. To fill this gap, we make the key observation that latent
diffusion models (LDMs), though successful in other modalities, have not been
effectively leveraged for radar-based 3D generation due to a lack of compatible
representations and conditioning strategies. We introduce RaLD, a framework
that bridges this gap by integrating scene-level frustum-based LiDAR
autoencoding, order-invariant latent representations, and direct radar spectrum
conditioning. These insights lead to a more compact and expressive generation
process. Experiments show that RaLD produces dense and accurate 3D point clouds
from raw radar spectrums, offering a promising solution for robust perception
in challenging environments.

</details>


### [149] [ClusterMine: Robust Label-Free Visual Out-Of-Distribution Detection via Concept Mining from Text Corpora](https://arxiv.org/abs/2511.07068)
*Nikolas Adaloglou,Diana Petrusheva,Mohamed Asker,Felix Michels,Markus Kollmann*

Main category: cs.CV

TL;DR: ClusterMine是一种无需预定义正标签的无监督视觉OOD检测方法，通过挖掘文本语料库并融合视觉聚类和零样本图像文本一致性，实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉OOD检测方法严重依赖预定义的ID标签名称，但这些标签在实际应用中可能不可用、不可靠或因分布变化而过时，阻碍了真正无监督OOD检测的发展。

Method: 提出ClusterMine方法，在通用概念挖掘范式下，通过结合视觉样本聚类的一致性和零样本图像文本一致性，从大规模文本语料库中自动挖掘正概念，无需人工定义标签。

Result: ClusterMine在多个CLIP模型上表现出良好的可扩展性，实现了SOTA的OOD检测性能，并对协变量ID分布变化具有最强鲁棒性。

Conclusion: ClusterMine成功实现了无需访问正标签的SOTA OOD检测，为大规模无监督视觉OOD检测提供了新的解决方案，具有实际应用价值。

Abstract: Large-scale visual out-of-distribution (OOD) detection has witnessed
remarkable progress by leveraging vision-language models such as CLIP. However,
a significant limitation of current methods is their reliance on a pre-defined
set of in-distribution (ID) ground-truth label names (positives). These fixed
label names can be unavailable, unreliable at scale, or become less relevant
due to in-distribution shifts after deployment. Towards truly unsupervised OOD
detection, we utilize widely available text corpora for positive label mining,
bypassing the need for positives. In this paper, we utilize widely available
text corpora for positive label mining under a general concept mining paradigm.
Within this framework, we propose ClusterMine, a novel positive label mining
method. ClusterMine is the first method to achieve state-of-the-art OOD
detection performance without access to positive labels. It extracts positive
concepts from a large text corpus by combining visual-only sample consistency
(via clustering) and zero-shot image-text consistency. Our experimental study
reveals that ClusterMine is scalable across a plethora of CLIP models and
achieves state-of-the-art robustness to covariate in-distribution shifts. The
code is available at https://github.com/HHU-MMBS/clustermine_wacv_official.

</details>


### [150] [Sparse4DGS: 4D Gaussian Splatting for Sparse-Frame Dynamic Scene Reconstruction](https://arxiv.org/abs/2511.07122)
*Changyue Shi,Chuxiao Yang,Xinyuan Hu,Minghao Chen,Wenwen Pan,Yan Yang,Jiajun Ding,Zhou Yu,Jun Yu*

Main category: cs.CV

TL;DR: Sparse4DGS是首个针对稀疏帧动态场景重建的方法，通过纹理感知的变形正则化和规范优化技术，解决了现有动态高斯分布在稀疏帧设置下的重建失败问题。


<details>
  <summary>Details</summary>
Motivation: 现有动态高斯分布方法依赖密集帧视频进行4D场景重建，但现实场景中因设备限制常只能获得稀疏帧。观察到动态重建方法在稀疏帧下，特别是纹理丰富区域，在规范空间和变形空间都会失败。

Method: 提出两个纹理感知技术：1) Texture-Aware Deformation Regularization - 引入基于纹理的深度对齐损失调节高斯变形；2) Texture-Aware Canonical Optimization - 将基于纹理的噪声融入规范高斯的梯度下降过程。

Result: 在NeRF-Synthetic、HyperNeRF、NeRF-DS和iPhone-4D数据集上的广泛实验表明，使用稀疏帧输入时，该方法优于现有的动态或少样本技术。

Conclusion: Sparse4DGS成功解决了稀疏帧动态场景重建的挑战，为实际应用中受限数据场景提供了有效的重建方案，特别是在纹理丰富区域的重建质量显著提升。

Abstract: Dynamic Gaussian Splatting approaches have achieved remarkable performance
for 4D scene reconstruction. However, these approaches rely on dense-frame
video sequences for photorealistic reconstruction. In real-world scenarios, due
to equipment constraints, sometimes only sparse frames are accessible. In this
paper, we propose Sparse4DGS, the first method for sparse-frame dynamic scene
reconstruction. We observe that dynamic reconstruction methods fail in both
canonical and deformed spaces under sparse-frame settings, especially in areas
with high texture richness. Sparse4DGS tackles this challenge by focusing on
texture-rich areas. For the deformation network, we propose Texture-Aware
Deformation Regularization, which introduces a texture-based depth alignment
loss to regulate Gaussian deformation. For the canonical Gaussian field, we
introduce Texture-Aware Canonical Optimization, which incorporates
texture-based noise into the gradient descent process of canonical Gaussians.
Extensive experiments show that when taking sparse frames as inputs, our method
outperforms existing dynamic or few-shot techniques on NeRF-Synthetic,
HyperNeRF, NeRF-DS, and our iPhone-4D datasets.

</details>


### [151] [MPJudge: Towards Perceptual Assessment of Music-Induced Paintings](https://arxiv.org/abs/2511.07137)
*Shiqi Jiang,Tianyi Liang,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: 本文提出了一种音乐激发绘画评估的新框架MPJudge，通过直接建模音乐与视觉艺术的感知一致性来解决传统基于情感识别方法的局限性。作者构建了首个大规模专家标注的音乐-绘画配对数据集MPD，并采用基于调制的融合机制和直接偏好优化训练，显著提升了评估准确性。


<details>
  <summary>Details</summary>
Motivation: 音乐激发绘画评估是评估绘画是否忠实反映激发音乐具有挑战性的感知评估任务。现有方法主要依赖情感识别模型评估音乐与绘画相似性，但存在两个关键问题：一是引入大量噪声，二是忽略了情感之外更广泛的感知线索。因此需要开发能够直接建模音乐与视觉艺术感知一致性的新方法。

Method: 方法包含三个核心组件：1) 构建MPD数据集，首个大规模音乐-绘画配对数据集，由领域专家基于感知一致性标注，并额外收集成对偏好标注处理模糊情况；2) 提出MPJudge模型，通过基于调制的融合机制将音乐特征集成到视觉编码器；3) 采用直接偏好优化(DPO)进行训练，有效学习模糊情况下的偏好信息。

Result: 大量实验表明所提方法显著优于现有方法。定性结果进一步显示模型能够更准确地识别绘画中与音乐相关的区域，验证了直接建模感知一致性的有效性，以及在处理模糊情况下采用偏好优化训练的优势。

Conclusion: 本研究成功解决了音乐激发绘画评估中传统情感识别方法的局限性，通过直接建模感知一致性、构建高质量数据集和创新的模型架构，为跨模态艺术评估提供了新的解决方案。该方法不仅在定量指标上表现优异，在定性分析中也展现出更强的语义理解能力。

Abstract: Music induced painting is a unique artistic practice, where visual artworks
are created under the influence of music. Evaluating whether a painting
faithfully reflects the music that inspired it poses a challenging perceptual
assessment task. Existing methods primarily rely on emotion recognition models
to assess the similarity between music and painting, but such models introduce
considerable noise and overlook broader perceptual cues beyond emotion. To
address these limitations, we propose a novel framework for music induced
painting assessment that directly models perceptual coherence between music and
visual art. We introduce MPD, the first large scale dataset of music painting
pairs annotated by domain experts based on perceptual coherence. To better
handle ambiguous cases, we further collect pairwise preference annotations.
Building on this dataset, we present MPJudge, a model that integrates music
features into a visual encoder via a modulation based fusion mechanism. To
effectively learn from ambiguous cases, we adopt Direct Preference Optimization
for training. Extensive experiments demonstrate that our method outperforms
existing approaches. Qualitative results further show that our model more
accurately identifies music relevant regions in paintings.

</details>


### [152] [ProcGen3D: Learning Neural Procedural Graph Representations for Image-to-3D Reconstruction](https://arxiv.org/abs/2511.07142)
*Xinyi Zhang,Daoyi Gao,Naiqi Li,Angela Dai*

Main category: cs.CV

TL;DR: ProcGen3D是一种基于程序化图的3D内容创建方法，通过从RGB图像生成程序化图抽象，然后解码为复杂的3D资产，结合了变换器预测和蒙特卡洛树搜索指导采样。


<details>
  <summary>Details</summary>
Motivation: 受到生产级3D应用程序中程序化生成器广泛使用的启发，现有3D生成方法在图像到3D重建方面存在局限性，需要一种能够生成可编辑、复杂3D资产的新方法。

Method: 提出序列化的基于图的程序化图表示，使用基于边的标记化编码程序化图，训练变换器先验来预测基于输入RGB图像的下一个标记，并集成蒙特卡洛树搜索(MCTS)指导采样来优化生成过程。

Result: 在仙人掌、树木和桥梁上的广泛实验表明，神经程序化图生成方法优于最先进的生成3D方法和领域特定建模技术，尽管只在合成数据上训练，但在真实世界输入图像上展现出改进的泛化能力。

Conclusion: ProcGen3D成功地将程序化生成与深度学习结合，为图像到3D重建提供了一种新的解决方案，能够生成高质量、可编辑的3D资产，并在多种对象类型上表现出良好的泛化性能。

Abstract: We introduce ProcGen3D, a new approach for 3D content creation by generating
procedural graph abstractions of 3D objects, which can then be decoded into
rich, complex 3D assets. Inspired by the prevalent use of procedural generators
in production 3D applications, we propose a sequentialized, graph-based
procedural graph representation for 3D assets. We use this to learn to
approximate the landscape of a procedural generator for image-based 3D
reconstruction. We employ edge-based tokenization to encode the procedural
graphs, and train a transformer prior to predict the next token conditioned on
an input RGB image. Crucially, to enable better alignment of our generated
outputs to an input image, we incorporate Monte Carlo Tree Search (MCTS) guided
sampling into our generation process, steering output procedural graphs towards
more image-faithful reconstructions. Our approach is applicable across a
variety of objects that can be synthesized with procedural generators.
Extensive experiments on cacti, trees, and bridges show that our neural
procedural graph generation outperforms both state-of-the-art generative 3D
methods and domain-specific modeling techniques. Furthermore, this enables
improved generalization on real-world input images, despite training only on
synthetic data.

</details>


### [153] [LiteUpdate: A Lightweight Framework for Updating AI-Generated Image Detectors](https://arxiv.org/abs/2511.07192)
*Jiajie Lu,Zhenkan Fu,Na Zhao,Long Xing,Kejiang Chen,Weiming Zhang,Nenghai Yu*

Main category: cs.CV

TL;DR: LiteUpdate是一个轻量级框架，通过边界样本选择和模型融合技术，高效更新AI生成图像检测器，解决新生成器带来的检测性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 生成式AI快速发展导致新的生成模型不断涌现，现有检测方法难以跟上步伐，检测性能显著下降，迫切需要持续更新AI生成图像检测器以适应新生成器。

Method: LiteUpdate框架包含两个核心模块：1）基于图像置信度和梯度判别特征的代表性样本选择模块，精确选择边界样本；2）模型融合模块，融合预训练、代表性更新和随机更新的权重轨迹。

Result: 在AIDE数据集上，LiteUpdate将Midjourney的平均检测准确率从87.63%提升到93.03%，相对增长6.16%，在各种检测器中都显著提升了检测性能。

Conclusion: LiteUpdate通过轻量级的边界样本选择和多轨迹权重融合策略，有效解决了检测器更新中的低效率和灾难性遗忘问题，为AI生成图像检测器的持续更新提供了高效解决方案。

Abstract: The rapid progress of generative AI has led to the emergence of new
generative models, while existing detection methods struggle to keep pace,
resulting in significant degradation in the detection performance. This
highlights the urgent need for continuously updating AI-generated image
detectors to adapt to new generators. To overcome low efficiency and
catastrophic forgetting in detector updates, we propose LiteUpdate, a
lightweight framework for updating AI-generated image detectors. LiteUpdate
employs a representative sample selection module that leverages image
confidence and gradient-based discriminative features to precisely select
boundary samples. This approach improves learning and detection accuracy on new
distributions with limited generated images, significantly enhancing detector
update efficiency. Additionally, LiteUpdate incorporates a model merging module
that fuses weights from multiple fine-tuning trajectories, including
pre-trained, representative, and random updates. This balances the adaptability
to new generators and mitigates the catastrophic forgetting of prior knowledge.
Experiments demonstrate that LiteUpdate substantially boosts detection
performance in various detectors. Specifically, on AIDE, the average detection
accuracy on Midjourney improved from 87.63% to 93.03%, a 6.16% relative
increase.

</details>


### [154] [Automated Estimation of Anatomical Risk Metrics for Endoscopic Sinus Surgery Using Deep Learning](https://arxiv.org/abs/2511.07199)
*Konrad Reuter,Lennart Thaysen,Bilkay Doruk,Sarah Latus,Brigitte Holst,Benjamin Becker,Dennis Eggert,Christian Betz,Anna-Sophie Hoffmann,Alexander Schlaefer*

Main category: cs.CV

TL;DR: 提出自动化深度学习流水线，通过热图回归定位关键解剖标志点，自动评估内镜鼻窦手术风险评分（Keros、Gera和TMS分数），在解剖测量上达到亚毫米级精度。


<details>
  <summary>Details</summary>
Motivation: 内镜鼻窦手术术前需要评估颅底解剖以降低脑脊液漏等风险，现有解剖风险评分需要手动测量冠状位CT/CBCT图像，耗时且容易产生人为误差。

Method: 开发自动化深度学习流水线，采用热图回归定位关键解剖标志点，比较直接方法与专门的全局到局部学习策略来估计Keros、Gera和TMS风险评分。

Result: 在相关解剖测量上获得较低的平均绝对误差：Keros评分0.506mm、Gera评分4.516°、TMS分类分别为0.802mm和0.777mm。

Conclusion: 该自动化方法能够准确估计内镜鼻窦手术的解剖风险评分，具有临床应用潜力，可减少手动测量的时间成本和人为误差。

Abstract: Endoscopic sinus surgery requires careful preoperative assessment of the
skull base anatomy to minimize risks such as cerebrospinal fluid leakage.
Anatomical risk scores like the Keros, Gera and Thailand-Malaysia-Singapore
score offer a standardized approach but require time-consuming manual
measurements on coronal CT or CBCT scans. We propose an automated deep learning
pipeline that estimates these risk scores by localizing key anatomical
landmarks via heatmap regression. We compare a direct approach to a specialized
global-to-local learning strategy and find mean absolute errors on the relevant
anatomical measurements of 0.506mm for the Keros, 4.516{\deg} for the Gera and
0.802mm / 0.777mm for the TMS classification.

</details>


### [155] [Geometric implicit neural representations for signed distance functions](https://arxiv.org/abs/2511.07206)
*Luiz Schirmer,Tiago Novello,Vinícius da Silva,Guilherme Schardong,Daniel Perazzo,Hélio Lopes,Nuno Gonçalves,Luiz Velho*

Main category: cs.CV

TL;DR: 这篇综述讨论了几何神经隐式表示（geometric INRs），这是一种专门用于逼近有符号距离函数（SDFs）的神经隐式表示方法，通过在损失函数中整合微分几何工具来实现3D表面重建。


<details>
  <summary>Details</summary>
Motivation: 神经隐式表示框架在低维空间信号表示方面具有前景，但需要专门针对有符号距离函数的INR方法来改进表面重建质量，现有方法需要更好的几何约束和正则化机制。

Method: 提出几何神经隐式表示（geometric INRs），在损失函数中包含微分几何工具（如法向量和曲率），添加正则化项确保INR满足SDF的全局特性（如单位梯度），从微分几何角度构建几何损失函数和采样方案。

Result: 几何INRs在从定向点云和位姿图像进行表面重建方面取得了显著进展，通过整合微分几何原理提升了重建质量。

Conclusion: 几何神经隐式表示是一种有效的3D重建方法，通过将微分几何原理整合到神经隐式表示框架中，能够实现更准确和几何一致性的表面重建。

Abstract: \textit{Implicit neural representations} (INRs) have emerged as a promising
framework for representing signals in low-dimensional spaces. This survey
reviews the existing literature on the specialized INR problem of approximating
\textit{signed distance functions} (SDFs) for surface scenes, using either
oriented point clouds or a set of posed images. We refer to neural SDFs that
incorporate differential geometry tools, such as normals and curvatures, in
their loss functions as \textit{geometric} INRs. The key idea behind this 3D
reconstruction approach is to include additional \textit{regularization} terms
in the loss function, ensuring that the INR satisfies certain global properties
that the function should hold -- such as having unit gradient in the case of
SDFs. We explore key methodological components, including the definition of
INR, the construction of geometric loss functions, and sampling schemes from a
differential geometry perspective. Our review highlights the significant
advancements enabled by geometric INRs in surface reconstruction from oriented
point clouds and posed images.

</details>


### [156] [Breaking the Stealth-Potency Trade-off in Clean-Image Backdoors with Generative Trigger Optimization](https://arxiv.org/abs/2511.07210)
*Binyan Xu,Fan Yang,Di Tang,Xilin Dai,Kehuan Zhang*

Main category: cs.CV

TL;DR: 本文提出GCB框架，使用条件InfoGAN识别自然图像特征作为隐蔽触发器，实现了CA下降小于1%的高隐蔽性干净图像后门攻击，在6个数据集、5个架构和4个任务上验证了其通用性。


<details>
  <summary>Details</summary>
Motivation: 现有干净图像后门攻击方法的关键缺陷是：成功攻击所需的毒化率会导致Clean Accuracy的明显下降，从而破坏攻击的隐蔽性。

Method: 提出Generative Clean-Image Backdoors (GCB)框架，使用条件InfoGAN来识别自然存在的图像特征作为触发器，确保这些触发器与良性任务特征易于分离，使受害者模型从极少量毒化样本中学习后门。

Result: 实现了CA下降小于1%的高隐蔽性攻击；成功适应6个数据集、5个架构和4个任务，首次在回归和分割任务中展示干净图像后门；对大多数现有后门防御表现出韧性。

Conclusion: GCB框架通过优化触发器本身而非毒化率，成功解决了现有方法的隐蔽性问题，利用自然特征作为触发器实现了高隐蔽性、高攻击成功率和强通用性的干净图像后门攻击。

Abstract: Clean-image backdoor attacks, which use only label manipulation in training
datasets to compromise deep neural networks, pose a significant threat to
security-critical applications. A critical flaw in existing methods is that the
poison rate required for a successful attack induces a proportional, and thus
noticeable, drop in Clean Accuracy (CA), undermining their stealthiness. This
paper presents a new paradigm for clean-image attacks that minimizes this
accuracy degradation by optimizing the trigger itself. We introduce Generative
Clean-Image Backdoors (GCB), a framework that uses a conditional InfoGAN to
identify naturally occurring image features that can serve as potent and
stealthy triggers. By ensuring these triggers are easily separable from benign
task-related features, GCB enables a victim model to learn the backdoor from an
extremely small set of poisoned examples, resulting in a CA drop of less than
1%. Our experiments demonstrate GCB's remarkable versatility, successfully
adapting to six datasets, five architectures, and four tasks, including the
first demonstration of clean-image backdoors in regression and segmentation.
GCB also exhibits resilience against most of the existing backdoor defenses.

</details>


### [157] [Omni-View: Unlocking How Generation Facilitates Understanding in Unified 3D Model based on Multiview images](https://arxiv.org/abs/2511.07222)
*JiaKui Hu,Shanshan Zhao,Qing-Guo Chen,Xuerui Qiu,Jialun Liu,Zhao Xu,Weihua Luo,Kaifu Zhang,Yanye Lu*

Main category: cs.CV

TL;DR: Omni-View是一个基于多视图图像的3D场景统一多模态理解与生成框架，通过联合建模场景理解、新视角合成和几何估计，实现了理解与生成的协同交互。


<details>
  <summary>Details</summary>
Motivation: 探索"生成促进理解"的原理，解决现有3D场景理解模型只能处理单一任务的局限性，实现多任务协同的统一框架。

Method: 包含理解模型、纹理模块和几何模块三部分；利用纹理模块的时空建模能力进行外观合成，几何模块提供显式几何约束；采用两阶段训练策略。

Result: 在VSI-Bench基准测试中达到55.4分的SOTA性能，超越现有专门3D理解模型，同时在新视角合成和3D场景生成任务上表现优异。

Conclusion: Omni-View成功实现了3D场景理解与生成的统一框架，验证了生成促进理解的有效性，为3D场景多模态处理提供了新的解决方案。

Abstract: This paper presents Omni-View, which extends the unified multimodal
understanding and generation to 3D scenes based on multiview images, exploring
the principle that "generation facilitates understanding". Consisting of
understanding model, texture module, and geometry module, Omni-View jointly
models scene understanding, novel view synthesis, and geometry estimation,
enabling synergistic interaction between 3D scene understanding and generation
tasks. By design, it leverages the spatiotemporal modeling capabilities of its
texture module responsible for appearance synthesis, alongside the explicit
geometric constraints provided by its dedicated geometry module, thereby
enriching the model's holistic understanding of 3D scenes. Trained with a
two-stage strategy, Omni-View achieves a state-of-the-art score of 55.4 on the
VSI-Bench benchmark, outperforming existing specialized 3D understanding
models, while simultaneously delivering strong performance in both novel view
synthesis and 3D scene generation.

</details>


### [158] [Mapping Reduced Accessibility to WASH Facilities in Rohingya Refugee Camps with Sub-Meter Imagery](https://arxiv.org/abs/2511.07231)
*Kyeongjin Ahn,YongHun Suh,Sungwon Han,Jeasurk Yang,Hannes Taubenböck,Meeyoung Cha*

Main category: cs.CV

TL;DR: 该研究利用遥感机器学习框架量化科克斯巴扎尔罗兴亚难民营的WASH设施可及性，发现随着人口增长，设施可及性呈下降趋势，特别是对女性和女童而言。


<details>
  <summary>Details</summary>
Motivation: 难民营的WASH服务是重大公共卫生关切。在人口稠密、空间配置密集且几何形状不规则的突发难民营中，检测难民住所和量化设施可及性面临巨大挑战。

Method: 使用亚米级卫星图像开发半监督分割框架，检测单个难民住所（F1分数76.4%），然后分析多年数据来量化水泵、厕所和淋浴间等WASH设施的可及性。

Result: 发现WASH可及性从2022年每设施服务25人下降到2025年29.4人；性别分层分析显示女性和女童在设施安全隔离不足的情况下面临更差的可及性。

Conclusion: 强调了需求响应式分配策略的重要性，以及高分辨率遥感和机器学习技术在检测不平等和指导复杂人道主义环境中公平资源规划方面的价值。

Abstract: Access to Water, Sanitation, and Hygiene (WASH) services remains a major
public health concern in refugee camps. This study introduces a remote
sensing-driven framework to quantify WASH accessibility-specifically to water
pumps, latrines, and bathing cubicles-in the Rohingya camps of Cox's Bazar, one
of the world's most densely populated displacement settings. Detecting refugee
shelters in such emergent camps presents substantial challenges, primarily due
to their dense spatial configuration and irregular geometric patterns. Using
sub-meter satellite images, we develop a semi-supervised segmentation framework
that achieves an F1-score of 76.4% in detecting individual refugee shelters.
Applying the framework across multi-year data reveals declining WASH
accessibility, driven by rapid refugee population growth and reduced facility
availability, rising from 25 people per facility in 2022 to 29.4 in 2025.
Gender-disaggregated analysis further shows that women and girls experience
reduced accessibility, in scenarios with inadequate safety-related segregation
in WASH facilities. These findings suggest the importance of demand-responsive
allocation strategies that can identify areas with under-served
populations-such as women and girls-and ensure that limited infrastructure
serves the greatest number of people in settings with fixed or shrinking
budgets. We also discuss the value of high-resolution remote sensing and
machine learning to detect inequality and inform equitable resource planning in
complex humanitarian environments.

</details>


### [159] [Noise & pattern: identity-anchored Tikhonov regularization for robust structural anomaly detection](https://arxiv.org/abs/2511.07233)
*Alexander Bauer,Klaus-Robert Müller*

Main category: cs.CV

TL;DR: 本文提出了一种基于自监督自编码器的结构异常检测方法，通过修复损坏的输入来学习检测工业缺陷，在MVTec AD基准测试中达到99.9/99.4的I/P-AUROC。


<details>
  <summary>Details</summary>
Motivation: 自动化工业检测中的异常检测至关重要，旨在识别统一视觉模式中的细微或罕见缺陷。由于收集所有可能异常的代表性样本是不可行的，需要新的方法来解决结构异常检测问题。

Method: 提出了一种自监督自编码器，学习修复损坏的输入。引入损坏模型向训练图像注入人工干扰来模拟结构缺陷。与去噪自编码器不同：1）应用结构化、空间连贯的扰动而非非结构化噪声，使任务成为分割和修复的混合体；2）在遮挡上添加并保留高斯噪声，作为Tikhonov正则化器，将重建函数的雅可比矩阵锚定在恒等映射上。

Result: 在MVTec AD基准测试中取得了最先进的结果（I/P-AUROC: 99.9/99.4），验证了理论框架的实用性。

Conclusion: 恒等锚定正则化稳定了重建过程，进一步提高了检测和分割准确性。该方法的理论框架得到了实验支持，在自动检测中具有实际应用价值。

Abstract: Anomaly detection plays a pivotal role in automated industrial inspection,
aiming to identify subtle or rare defects in otherwise uniform visual patterns.
As collecting representative examples of all possible anomalies is infeasible,
we tackle structural anomaly detection using a self-supervised autoencoder that
learns to repair corrupted inputs. To this end, we introduce a corruption model
that injects artificial disruptions into training images to mimic structural
defects. While reminiscent of denoising autoencoders, our approach differs in
two key aspects. First, instead of unstructured i.i.d.\ noise, we apply
structured, spatially coherent perturbations that make the task a hybrid of
segmentation and inpainting. Second, and counterintuitively, we add and
preserve Gaussian noise on top of the occlusions, which acts as a Tikhonov
regularizer anchoring the Jacobian of the reconstruction function toward
identity. This identity-anchored regularization stabilizes reconstruction and
further improves both detection and segmentation accuracy. On the MVTec AD
benchmark, our method achieves state-of-the-art results (I/P-AUROC: 99.9/99.4),
supporting our theoretical framework and demonstrating its practical relevance
for automatic inspection.

</details>


### [160] [StreamKV: Streaming Video Question-Answering with Segment-based KV Cache Retrieval and Compression](https://arxiv.org/abs/2511.07278)
*Yilong Chen,Xiang Bai,Zhibin Wang,Chengyu Bai,Yuhan Dai,Ming Lu,Shanghang Zhang*

Main category: cs.CV

TL;DR: StreamKV是一种无需训练的框架，通过动态语义分割、摘要向量和引导提示机制，统一处理Video-LLMs的KV缓存检索与压缩，显著提升长视频问答的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有Video-LLMs在处理长视频时面临挑战，虽然已有KV缓存检索方法，但缓存压缩和检索机制仍未充分探索，需要更有效的解决方案。

Method: StreamKV采用动态语义分割替代均匀分割，为每个片段计算摘要向量进行检索，引入引导提示捕获关键语义元素进行压缩，并将检索和压缩统一在单个模块中以层自适应方式执行。

Result: 在StreamingVQA基准测试上，StreamKV显著超越现有在线Video-LLMs，在实现更高准确率的同时大幅提升了内存效率和计算延迟。

Conclusion: StreamKV成功解决了Video-LLMs处理长视频的关键挑战，通过创新的KV缓存检索与压缩统一框架，为流式视频问答提供了高效准确的解决方案。

Abstract: Video Large Language Models (Video-LLMs) have demonstrated significant
potential in the areas of video captioning, search, and summarization. However,
current Video-LLMs still face challenges with long real-world videos. Recent
methods have introduced a retrieval mechanism that retrieves query-relevant KV
caches for question answering, enhancing the efficiency and accuracy of long
real-world videos. However, the compression and retrieval of KV caches are
still not fully explored. In this paper, we propose \textbf{StreamKV}, a
training-free framework that seamlessly equips Video-LLMs with advanced KV
cache retrieval and compression. Compared to previous methods that used uniform
partitioning, StreamKV dynamically partitions video streams into semantic
segments, which better preserves semantic information. For KV cache retrieval,
StreamKV calculates a summary vector for each segment to retain segment-level
information essential for retrieval. For KV cache compression, StreamKV
introduces a guidance prompt designed to capture the key semantic elements
within each segment, ensuring only the most informative KV caches are retained
for answering questions. Moreover, StreamKV unifies KV cache retrieval and
compression within a single module, performing both in a layer-adaptive manner,
thereby further improving the effectiveness of streaming video question
answering. Extensive experiments on public StreamingVQA benchmarks demonstrate
that StreamKV significantly outperforms existing Online Video-LLMs, achieving
superior accuracy while substantially improving both memory efficiency and
computational latency. The code has been released at
https://github.com/sou1p0wer/StreamKV.

</details>


### [161] [Segmentation of Ischemic Stroke Lesions using Transfer Learning on Multi-sequence MRI](https://arxiv.org/abs/2511.07281)
*R. P. Chowdhury,T. Rahman*

Main category: cs.CV

TL;DR: 本研究提出了一种基于Res-Unet架构和迁移学习的缺血性脑卒中病变自动分割方法，在多种MRI序列上实现了80.5%的Dice分数，有效解决了人工分割的耗时和不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 缺血性脑卒中病变的准确理解对治疗和预后至关重要，但传统人工分割方法存在耗时、繁琐和观察者不一致等问题，而现有自动方法依赖手工特征，难以捕获病变的不规则复杂形状。

Method: 提出基于Res-Unet架构的分割框架，分别在有预训练权重和无权重条件下训练模型，探索迁移学习优势，最后集成多数投票分类器融合各轴分割结果。在ISLES 2015数据集上验证，涵盖T1、T2、DWI和FLAIR多种MRI序列。

Result: 在3D体积评估中，该方法实现了80.5%的Dice分数和74.03%的准确率，有效提升了缺血性脑卒中病变的自动分割性能。

Conclusion: 通过结合迁移学习和Res-Unet架构，以及多数投票分类器的集成策略，所提出的自动分割方法能够准确、快速地分割缺血性脑卒中病变，为临床诊断提供了有效的辅助工具。

Abstract: The accurate understanding of ischemic stroke lesions is critical for
efficient therapy and prognosis of stroke patients. Magnetic resonance imaging
(MRI) is sensitive to acute ischemic stroke and is a common diagnostic method
for stroke. However, manual lesion segmentation performed by experts is
tedious, time-consuming, and prone to observer inconsistency. Automatic medical
image analysis methods have been proposed to overcome this challenge. However,
previous approaches have relied on hand-crafted features that may not capture
the irregular and physiologically complex shapes of ischemic stroke lesions. In
this study, we present a novel framework for quickly and automatically
segmenting ischemic stroke lesions on various MRI sequences, including
T1-weighted, T2-weighted, DWI, and FLAIR. The proposed methodology is validated
on the ISLES 2015 Brain Stroke sequence dataset, where we trained our model
using the Res-Unet architecture twice: first, with pre-existing weights, and
then without, to explore the benefits of transfer learning. Evaluation metrics,
including the Dice score and sensitivity, were computed across 3D volumes.
Finally, a Majority Voting Classifier was integrated to amalgamate the outcomes
from each axis, resulting in a comprehensive segmentation method. Our efforts
culminated in achieving a Dice score of 80.5\% and an accuracy of 74.03\%,
showcasing the efficacy of our segmentation approach.

</details>


### [162] [VADER: Towards Causal Video Anomaly Understanding with Relation-Aware Large Language Models](https://arxiv.org/abs/2511.07299)
*Ying Cheng,Yu-Ho Lin,Min-Hung Chen,Fu-En Yang,Shang-Hong Lai*

Main category: cs.CV

TL;DR: VADER是一个基于大语言模型的视频异常理解框架，通过集成关键帧对象关系特征与视觉线索，实现对视频中异常事件的深度理解、因果推理和详细描述生成。


<details>
  <summary>Details</summary>
Motivation: 传统视频异常检测方法仅关注检测和定位异常，缺乏对异常事件的深层语义理解。现有方法忽视了对象间的因果关系和交互作用，而这些对于理解异常行为至关重要，因此需要能够提供详细解释和因果分析的框架。

Method: VADER框架包含四个核心组件：1)异常评分器为每帧分配异常分数；2)上下文感知采样策略(CAES)捕获异常事件的因果上下文；3)关系特征提取器和对比关系编码器(CORE)联合建模动态对象交互，生成紧凑的关系表示；4)将视觉和关系线索与大语言模型集成，支持因果描述生成和问答任务。

Result: 在多个真实世界视频异常理解基准测试中，VADER在异常描述、解释和因果推理任务上均取得了优异表现，证明了其在可解释视频异常分析方面的有效性。

Conclusion: VADER框架通过结合视觉特征、对象关系建模和大语言模型推理能力，成功推进了视频异常理解领域的发展，为异常事件提供了更深入、更可解释的分析，为未来的可解释视频分析研究奠定了基础。

Abstract: Video anomaly understanding (VAU) aims to provide detailed interpretation and
semantic comprehension of anomalous events within videos, addressing
limitations of traditional methods that focus solely on detecting and
localizing anomalies. However, existing approaches often neglect the deeper
causal relationships and interactions between objects, which are critical for
understanding anomalous behaviors. In this paper, we propose VADER, an
LLM-driven framework for Video Anomaly unDErstanding, which integrates keyframe
object Relation features with visual cues to enhance anomaly comprehension from
video. Specifically, VADER first applies an Anomaly Scorer to assign per-frame
anomaly scores, followed by a Context-AwarE Sampling (CAES) strategy to capture
the causal context of each anomalous event. A Relation Feature Extractor and a
COntrastive Relation Encoder (CORE) jointly model dynamic object interactions,
producing compact relational representations for downstream reasoning. These
visual and relational cues are integrated with LLMs to generate detailed,
causally grounded descriptions and support robust anomaly-related question
answering. Experiments on multiple real-world VAU benchmarks demonstrate that
VADER achieves strong results across anomaly description, explanation, and
causal reasoning tasks, advancing the frontier of explainable video anomaly
analysis.

</details>


### [163] [Garbage Vulnerable Point Monitoring using IoT and Computer Vision](https://arxiv.org/abs/2511.07325)
*R. Kumar,A. Lall,S. Chaudhari,M. Kale,A. Vattem*

Main category: cs.CV

TL;DR: 本文提出了一种基于物联网和计算机视觉的智能城市固体废物管理系统，通过街景摄像头和目标检测算法监控垃圾易倾倒点的非法倾倒行为，其中YOLO11m模型在废物检测中达到92.39%的准确率。


<details>
  <summary>Details</summary>
Motivation: 城市地区存在大量垃圾易倾倒点的非法倾倒问题，需要开发一种有效的监控系统来快速检测和追踪这些倾倒事件，以改善城市环境卫生管理。

Method: 构建基于物联网和计算机视觉的监控系统，使用街景摄像头采集数据，评估多种目标检测模型(YOLOv8, YOLOv10, YOLO11m, RT-DETR)的性能，在印度特伦甘纳邦Sangareddy区收集真实数据进行实验验证。

Result: YOLO11m模型表现最佳，废物检测准确率达92.39%，mAP@50为0.91。系统能够有效捕捉垃圾倾倒的时间模式，包括小时、日、周的趋势，实现全天候监控。

Conclusion: 研究证明了基于计算机视觉的智能监控系统在垃圾易倾倒点监测中的有效性，为城市废物管理提供了可靠的技术解决方案，能够及时发现和处理非法倾倒行为。

Abstract: This paper proposes a smart way to manage municipal solid waste by using the
Internet of Things (IoT) and computer vision (CV) to monitor illegal waste
dumping at garbage vulnerable points (GVPs) in urban areas. The system can
quickly detect and monitor dumped waste using a street-level camera and object
detection algorithm. Data was collected from the Sangareddy district in
Telangana, India. A series of comprehensive experiments was carried out using
the proposed dataset to assess the accuracy and overall performance of various
object detection models. Specifically, we performed an in-depth evaluation of
YOLOv8, YOLOv10, YOLO11m, and RT-DETR on our dataset. Among these models,
YOLO11m achieved the highest accuracy of 92.39\% in waste detection,
demonstrating its effectiveness in detecting waste. Additionally, it attains an
mAP@50 of 0.91, highlighting its high precision. These findings confirm that
the object detection model is well-suited for monitoring and tracking waste
dumping events at GVP locations. Furthermore, the system effectively captures
waste disposal patterns, including hourly, daily, and weekly dumping trends,
ensuring comprehensive daily and nightly monitoring.

</details>


### [164] [TwinOR: Photorealistic Digital Twins of Dynamic Operating Rooms for Embodied AI Research](https://arxiv.org/abs/2511.07412)
*Han Zhang,Yiqing Shen,Roger D. Soberanis-Mukul,Ankita Ghosh,Hao Ding,Lalithkumar Seenivasan,Jose L. Porras,Zhekai Mao,Chenjia Li,Wenjie Xiao,Lonny Yarmus,Angela Christine Argento,Masaru Ishii,Mathias Unberath*

Main category: cs.CV

TL;DR: TwinOR是一个构建手术室动态光照片级数字孪生的框架，通过融合静态几何重建和动态行为建模，为具身AI提供安全可控的训练和评估环境，实现了从仿真到现实的有效部署。


<details>
  <summary>Details</summary>
Motivation: 手术室的安全法规和操作限制了具身AI在真实环境中的自由感知和交互，而现有方法难以创建能捕捉手术室空间、视觉和行为复杂性的动态、光照片级数字表示，迫切需要构建高保真、无风险的数字孪生环境。

Method: 提出TwinOR框架，从预扫描视频中重建静态几何结构，通过多视角感知持续建模人员和设备运动，将静态和动态组件融合到支持可控仿真和具身探索的沉浸式3D环境中。

Result: 系统以厘米级精度重建完整手术室几何结构，保持手术工作流程中的动态交互，在TwinOR合成数据上测试的FoundationStereo和ORB-SLAM3模型性能达到真实室内数据集的相当水平，证明了传感器级的真实感。

Conclusion: TwinOR建立了从真实到仿真的管道，为动态、光照片级手术室数字孪生提供了安全、可扩展、数据高效的具身AI开发和基准测试平台，最终加速了具身AI从仿真到现实的部署进程。

Abstract: Developing embodied AI for intelligent surgical systems requires safe,
controllable environments for continual learning and evaluation. However,
safety regulations and operational constraints in operating rooms (ORs) limit
embodied agents from freely perceiving and interacting in realistic settings.
Digital twins provide high-fidelity, risk-free environments for exploration and
training. How we may create photorealistic and dynamic digital representations
of ORs that capture relevant spatial, visual, and behavioral complexity remains
unclear. We introduce TwinOR, a framework for constructing photorealistic,
dynamic digital twins of ORs for embodied AI research. The system reconstructs
static geometry from pre-scan videos and continuously models human and
equipment motion through multi-view perception of OR activities. The static and
dynamic components are fused into an immersive 3D environment that supports
controllable simulation and embodied exploration. The proposed framework
reconstructs complete OR geometry with centimeter level accuracy while
preserving dynamic interaction across surgical workflows, enabling realistic
renderings and a virtual playground for embodied AI systems. In our
experiments, TwinOR simulates stereo and monocular sensor streams for geometry
understanding and visual localization tasks. Models such as FoundationStereo
and ORB-SLAM3 on TwinOR-synthesized data achieve performance within their
reported accuracy on real indoor datasets, demonstrating that TwinOR provides
sensor-level realism sufficient for perception and localization challenges. By
establishing a real-to-sim pipeline for constructing dynamic, photorealistic
digital twins of OR environments, TwinOR enables the safe, scalable, and
data-efficient development and benchmarking of embodied AI, ultimately
accelerating the deployment of embodied AI from sim-to-real.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [165] [Ming-UniAudio: Speech LLM for Joint Understanding, Generation and Editing with Unified Representation](https://arxiv.org/abs/2511.05516)
*Canxiang Yan,Chunxiang Jin,Dawei Huang,Haibing Yu,Han Peng,Hui Zhan,Jie Gao,Jing Peng,Jingdong Chen,Jun Zhou,Kaimeng Ren,Ming Yang,Mingxue Yang,Qiang Xu,Qin Zhao,Ruijie Xiong,Shaoxiong Lin,Xuezhi Wang,Yi Yuan,Yifei Wu,Yongjie Lyu,Zhengyu He,Zhihao Qiu,Zhiqiang Fang,Ziyuan Huang*

Main category: cs.CL

TL;DR: 本文提出了统一语音理解、生成和编辑的创新框架，核心是首个整合语义和声学特征的连续语音tokenizer MingTok-Audio，基于此开发了Ming-UniAudio模型，并进一步实现了首个支持自然语言指令的自由形式语音编辑模型Ming-UniAudio-Edit。


<details>
  <summary>Details</summary>
Motivation: 现有语音模型在理解和生成任务中对token表示存在竞争性需求，这种表示差异阻碍了语音语言模型进行基于指令的自由形式编辑。

Method: 提出了统一框架：1) 开发MingTok-Audio连续语音tokenizer，首次有效整合语义和声学特征；2) 基于此构建Ming-UniAudio基础模型，平衡生成与理解能力；3) 进一步训练Ming-UniAudio-Edit专用编辑模型，实现仅通过自然语言指令的通用自由形式语音编辑；4) 建立Ming-Freeform-Audio-Edit评估基准。

Result: Ming-UniAudio在ContextASR基准12个指标中8个达到SOTA，中文语音克隆Seed-TTS-WER为0.95；Ming-UniAudio-Edit成为首个支持纯自然语言指令的自由形式语音编辑模型，可处理语义和声学修改；开源了所有模型组件。

Conclusion: 该框架成功解决了语音理解与生成任务的表示冲突问题，实现了真正的统一语音处理能力，为未来音频理解、生成和操作的协同发展奠定了基础。

Abstract: Existing speech models suffer from competing requirements on token
representations by understanding and generation tasks. This discrepancy in
representation prevents speech language models from performing
instruction-based free-form editing. To solve this challenge, we introduce a
novel framework that unifies speech understanding, generation, and editing. The
core of our unified model is a unified continuous speech tokenizer
MingTok-Audio, the first continuous tokenizer to effectively integrate semantic
and acoustic features, which makes it suitable for both understanding and
generation tasks. Based on this unified continuous audio tokenizer, we
developed the speech language model Ming-UniAudio, which achieved a balance
between generation and understanding capabilities. Ming-UniAudio sets new
state-of-the-art (SOTA) records on 8 out of 12 metrics on the ContextASR
benchmark. Notably, for Chinese voice cloning, it achieves a highly competitive
Seed-TTS-WER of 0.95. Leveraging this foundational model, we further trained a
dedicated speech editing model Ming-UniAudio-Edit, the first speech language
model that enables universal, free-form speech editing guided solely by natural
language instructions, handling both semantic and acoustic modifications
without timestamp condition. To rigorously assess the editing capability and
establish a foundation for future research, we introduce
Ming-Freeform-Audio-Edit, the first comprehensive benchmark tailored for
instruction-based free-form speech editing, featuring diverse scenarios and
evaluation dimensions spanning semantic correctness, acoustic quality, and
instruction alignment. We open-sourced the continuous audio tokenizer, the
unified foundational model, and the free-form instruction-based editing model
to facilitate the development of unified audio understanding, generation, and
manipulation.

</details>


### [166] [Retracing the Past: LLMs Emit Training Data When They Get Lost](https://arxiv.org/abs/2511.05518)
*Myeongseob Ko,Nikhil Reddy Billa,Adam Nguyen,Charles Fleming,Ming Jin,Ruoxi Jia*

Main category: cs.CL

TL;DR: 本文提出困惑诱导攻击(CIA)框架，通过系统性地最大化模型不确定性来提取大语言模型中的记忆数据，并为对齐模型提出不匹配监督微调方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据提取方法成功率有限且缺乏对记忆泄漏根本机制的洞察，大语言模型对训练数据的记忆引发了严重的隐私和版权问题。

Method: 发现记忆文本释放前会出现持续的分词预测熵峰值；通过优化输入片段故意诱导高熵状态；针对对齐模型提出不匹配监督微调以削弱对齐并诱导目标困惑。

Result: 在多种对齐和非对齐大语言模型上的实验表明，所提攻击方法在提取逐字或近逐字训练数据方面优于现有基线，且无需训练数据先验知识。

Conclusion: 研究结果突显了大语言模型普遍存在的记忆风险，为评估这些漏洞提供了更系统的方法，揭示了不同模型架构中持续的记忆泄漏问题。

Abstract: The memorization of training data in large language models (LLMs) poses
significant privacy and copyright concerns. Existing data extraction methods,
particularly heuristic-based divergence attacks, often exhibit limited success
and offer limited insight into the fundamental drivers of memorization leakage.
This paper introduces Confusion-Inducing Attacks (CIA), a principled framework
for extracting memorized data by systematically maximizing model uncertainty.
We empirically demonstrate that the emission of memorized text during
divergence is preceded by a sustained spike in token-level prediction entropy.
CIA leverages this insight by optimizing input snippets to deliberately induce
this consecutive high-entropy state. For aligned LLMs, we further propose
Mismatched Supervised Fine-tuning (SFT) to simultaneously weaken their
alignment and induce targeted confusion, thereby increasing susceptibility to
our attacks. Experiments on various unaligned and aligned LLMs demonstrate that
our proposed attacks outperform existing baselines in extracting verbatim and
near-verbatim training data without requiring prior knowledge of the training
data. Our findings highlight persistent memorization risks across various LLMs
and offer a more systematic method for assessing these vulnerabilities.

</details>


### [167] [MCP4IFC: IFC-Based Building Design Using Large Language Models](https://arxiv.org/abs/2511.05533)
*Bharathi Kannan Nithyanantham,Tobias Sesterhenn,Ashwin Nedungadi,Sergio Peral Garijo,Janis Zenkner,Christian Bartelt,Stefan Lüdtke*

Main category: cs.CL

TL;DR: MCP4IFC是一个开源框架，使大语言模型能够通过模型上下文协议直接操作建筑行业的IFC数据，实现自然语言到建筑模型操作的转换。


<details>
  <summary>Details</summary>
Motivation: 将生成式AI引入建筑、工程和施工(AEC)领域需要能够将自然语言指令转换为标准化数据模型操作的系统。当前缺乏这样的系统来支持AI驱动的BIM设计工作流程。

Method: 提出MCP4IFC框架，提供BIM工具集包括场景查询工具、创建和修改建筑元素的预定义函数，以及结合上下文学习和检索增强生成的动态代码生成系统来处理超出预定义工具集的任务。

Result: 实验表明使用该框架的LLM能够成功执行复杂任务，从建造简单房屋到查询和编辑现有IFC数据。框架已开源发布。

Conclusion: MCP4IFC为LLM驱动的BIM设计研究提供了基础，并为AI辅助建模工作流程奠定了基础，促进了生成式AI在AEC领域的应用。

Abstract: Bringing generative AI into the architecture, engineering and construction
(AEC) field requires systems that can translate natural language instructions
into actions on standardized data models. We present MCP4IFC, a comprehensive
open-source framework that enables Large Language Models (LLMs) to directly
manipulate Industry Foundation Classes (IFC) data through the Model Context
Protocol (MCP). The framework provides a set of BIM tools, including scene
querying tools for information retrieval, predefined functions for creating and
modifying common building elements, and a dynamic code-generation system that
combines in-context learning with retrieval-augmented generation (RAG) to
handle tasks beyond the predefined toolset. Experiments demonstrate that an LLM
using our framework can successfully perform complex tasks, from building a
simple house to querying and editing existing IFC data. Our framework is
released as open-source to encourage research in LLM-driven BIM design and
provide a foundation for AI-assisted modeling workflows. Our code is available
at https://show2instruct.github.io/mcp4ifc/.

</details>


### [168] [FlowMM: Cross-Modal Information Flow Guided KV Cache Merging for Efficient Multimodal Context Inference](https://arxiv.org/abs/2511.05534)
*Kunxi Li,Yufan Xiong,Zhonghua Jiang,Yiyun Zhou,Zhaode Wang,Chengfei Lv,Shengyu Zhang*

Main category: cs.CL

TL;DR: FlowMM提出了一种跨模态信息流引导的多模态KV缓存合并框架，通过自适应层特定合并策略和敏感度感知的令牌匹配机制，在保持任务性能的同时显著减少内存使用和延迟。


<details>
  <summary>Details</summary>
Motivation: 传统KV缓存驱逐策略基于注意力分数丢弃不太重要的KV对，常导致生成质量下降、上下文丢失或幻觉。近期转向KV合并的方法在多模态场景下效果受限，因为模态令牌间的分布偏差和跨模态交互中的注意力偏差限制了其有效性。

Method: FlowMM采用跨模态信息流动态应用层特定的合并策略，捕获模态特定模式同时保持上下文完整性。引入敏感度自适应令牌匹配机制，联合评估令牌相似性和任务关键敏感度，合并低风险令牌同时保护高敏感度令牌。

Result: 在多个领先的多模态大模型上的广泛实验表明，FlowMM可将KV缓存内存减少80%至95%，解码延迟降低1.3-1.8倍，同时保持竞争性的任务性能。

Conclusion: FlowMM通过智能的跨模态信息流引导和敏感度感知的令牌合并策略，有效解决了多模态场景下传统KV缓存管理的问题，为大规模多模态模型的高效推理提供了实用解决方案。

Abstract: Traditional KV cache eviction strategies, which discard less critical
KV-pairs based on attention scores, often degrade generation quality, causing
context loss or hallucinations. Recent efforts shift toward KV merging, merging
eviction tokens with retention tokens based on similarity. However, in
multimodal scenarios, distributional biases across modality tokens and
attentional biases in cross-modal interactions limit its effectiveness. This
work introduces FlowMM, an adaptive framework for cross-modal information
flow-guided multimodal KV cache merging. FlowMM leverages cross-modal
information flow to dynamically apply layer-specific merging strategies,
capturing modality-specific patterns while preserving contextual integrity.
Furthermore, we introduce a sensitivity-adaptive token matching mechanism that
jointly evaluates token similarity and task-critical sensitivity, merging
low-risk tokens while safeguarding high-sensitivity ones. Extensive experiments
across diverse leading MLLMs show that FlowMM reduces KV cache memory by 80% to
95% and decoding latency by 1.3-1.8x, while maintaining competitive task
performance.

</details>


### [169] [Temporal Sparse Autoencoders: Leveraging the Sequential Nature of Language for Interpretability](https://arxiv.org/abs/2511.05541)
*Usha Bhalla,Alex Oesterling,Claudio Mayrink Verdun,Himabindu Lakkaraju,Flavio P. Calmon*

Main category: cs.CL

TL;DR: 本文提出时序稀疏自编码器(T-SAEs)，通过引入对比损失鼓励高级特征在相邻token上保持一致性激活，从而解决传统SAE偏向浅层特征的问题，实现语义与语法特征的自监督解耦。


<details>
  <summary>Details</summary>
Motivation: 现有字典学习方法如稀疏自编码器在发现人类可理解特征时存在系统性缺陷：无法捕获丰富的语言概念信息，偏向浅层token特定或噪声特征。根本原因在于当前无监督方法忽略了语言的结构知识，导致特征发现偏向表层模式而非有意义概念。

Method: 基于语义内容具有长程依赖且序列平滑、语法信息更局部的语言特性，提出T-SAEs。核心创新是引入新颖的对比损失函数，鼓励高级特征在相邻token上保持一致的激活，使SAE能够自监督地分离语义和语法特征。

Result: 在多个数据集和模型上，T-SAEs恢复出更平滑、更连贯的语义概念，且不牺牲重建质量。显著的是，尽管没有明确的语义信号训练，模型仍展现出清晰的语义结构。

Conclusion: T-SAEs为语言模型的无监督可解释性提供了新途径，通过简单而有效的修改克服了传统方法的局限，证明了利用语言结构知识进行特征发现的可行性。

Abstract: Translating the internal representations and computations of models into
concepts that humans can understand is a key goal of interpretability. While
recent dictionary learning methods such as Sparse Autoencoders (SAEs) provide a
promising route to discover human-interpretable features, they suffer from a
variety of problems, including a systematic failure to capture the rich
conceptual information that drives linguistic understanding. Instead, they
exhibit a bias towards shallow, token-specific, or noisy features, such as "the
phrase 'The' at the start of sentences". In this work, we propose that this is
due to a fundamental issue with how dictionary learning methods for LLMs are
trained. Language itself has a rich, well-studied structure spanning syntax,
semantics, and pragmatics; however, current unsupervised methods largely ignore
this linguistic knowledge, leading to poor feature discovery that favors
superficial patterns over meaningful concepts. We focus on a simple but
important aspect of language: semantic content has long-range dependencies and
tends to be smooth over a sequence, whereas syntactic information is much more
local. Building on this insight, we introduce Temporal Sparse Autoencoders
(T-SAEs), which incorporate a novel contrastive loss encouraging consistent
activations of high-level features over adjacent tokens. This simple yet
powerful modification enables SAEs to disentangle semantic from syntactic
features in a self-supervised manner. Across multiple datasets and models,
T-SAEs recover smoother, more coherent semantic concepts without sacrificing
reconstruction quality. Strikingly, they exhibit clear semantic structure
despite being trained without explicit semantic signal, offering a new pathway
for unsupervised interpretability in language models.

</details>


### [170] [UTF-8 Plumbing: Byte-level Tokenizers Unavoidably Enable LLMs to Generate Ill-formed UTF-8](https://arxiv.org/abs/2511.05578)
*Preston Firestone,Shubham Ugare,Gagandeep Singh,Sasa Misailovic*

Main category: cs.CL

TL;DR: 该论文形式化分析了子词分词中基于字节方法的UTF-8无效问题，证明了包含畸形UTF-8词汇的分词器总是会产生无效UTF-8序列，并提供了实际案例和缓解方案。


<details>
  <summary>Details</summary>
Motivation: 基于字节的子词分词方法虽然解决了词汇表外错误问题，但会产生无效的UTF-8序列，导致假设输入为有效UTF-8的代码出错，需要系统性地分析并解决这一问题。

Method: 使用幺半群理论形式化分词过程，数学上证明了基于字节分词器的性质，并评估了多种缓解方法的有效性，同时分析了主要基础模型、服务引擎和受限生成系统的实际案例。

Result: 严格证明了包含畸形UTF-8词汇的分词器总能产生畸形UTF-8序列，并揭示了逐个token转换与整体序列转换的不同结果，预测了实际环境中的bug模式。

Conclusion: 基于字节的子词分词存在根本性的UTF-8有效性问题，需要应用程序额外处理，论文为理解和缓解这一问题提供了理论框架和实践指导。

Abstract: Subword tokenization segments input text according to a pre-defined
vocabulary to feed it into a language model; the language model, in turn,
generates a sequence made from this same vocabulary. The members of the
vocabulary can be built of code points or bytes. Using code points means that
all members of the vocabulary are valid UTF-8 characters. However, it also
requires thousands of initial members to achieve acceptable coverage of inputs.
Beginning with bytes, on the contrary, avoids out-of-vocabulary errors with
only 256 initial members of the vocabulary, but the members of the vocabulary
and sequences of them are not guaranteed to be valid UTF-8. Sequences that are
not valid UTF-8 break code that assumes its input to be valid UTF-8.
Applications of language models must account for the breakage thereby
introduced. In this paper, we formalize tokenization using monoid theory and
prove that tokenizers whose vocabularies contain tokens that are ill-formed
UTF-8 can always produce sequences that are ill-formed UTF-8. We demonstrate
formally that attempting to incrementally convert tokens back to a string and
interpret the results as UTF-8 gives different results than converting the
whole sequence of tokens at once. This formal result predicts real-world bugs:
we evaluate mitigations for the problem identified and provide case studies of
major foundation models, serving engines, and constrained generation systems.

</details>


### [171] [Optimizing Diversity and Quality through Base-Aligned Model Collaboration](https://arxiv.org/abs/2511.05650)
*Yichen Wang,Chenghao Yang,Tenghao Huang,Muhao Chen,Jonathan May,Mina Lee*

Main category: cs.CL

TL;DR: BACo是一种推理时模型协作框架，通过动态结合基础LLM和其对齐模型，在单次生成中同时提高输出多样性和质量，相比现有方法在多样性和质量联合指标上提升21.3%。


<details>
  <summary>Details</summary>
Motivation: 现有对齐技术提高了LLM输出质量但牺牲了多样性，导致输出高度相似；传统多样性提升方法如重新训练、提示工程和多采样等方法会降低质量或需要昂贵解码。

Method: 提出Base-Aligned Model Collaboration (BACo)框架，在推理时基于下一token预测的不确定性和语义角色进行路由决策，动态选择从基础模型还是对齐模型解码每个token。

Result: 在三个开放生成任务和13个指标上，BACo一致超越现有推理时基线方法；最佳路由器实现21.3%的多样性和质量联合改进；人工评估验证了这些改进。

Conclusion: 基础模型和对齐模型间的协作可以有效优化和控制输出的多样性与质量，BACo在单次生成中实现了高质量与高多样性的平衡。

Abstract: Alignment has greatly improved large language models (LLMs)' output quality
at the cost of diversity, yielding highly similar outputs across generations.
We propose Base-Aligned Model Collaboration (BACo), an inference-time
token-level model collaboration framework that dynamically combines a base LLM
with its aligned counterpart to optimize diversity and quality. Inspired by
prior work (Fei et al., 2025), BACo employs routing strategies that determine,
at each token, from which model to decode based on next-token prediction
uncertainty and predicted contents' semantic role. Prior diversity-promoting
methods, such as retraining, prompt engineering, and multi-sampling methods,
improve diversity but often degrade quality or require costly decoding or
post-training. In contrast, BACo achieves both high diversity and quality post
hoc within a single pass, while offering strong controllability. We explore a
family of routing strategies, across three open-ended generation tasks and 13
metrics covering diversity and quality, BACo consistently surpasses
state-of-the-art inference-time baselines. With our best router, BACo achieves
a 21.3% joint improvement in diversity and quality. Human evaluations also
mirror these improvements. The results suggest that collaboration between base
and aligned models can optimize and control diversity and quality.

</details>


### [172] [OckBench: Measuring the Efficiency of LLM Reasoning](https://arxiv.org/abs/2511.05722)
*Zheng Du,Hao Kang,Song Han,Tushar Krishna,Ligeng Zhu*

Main category: cs.CL

TL;DR: OckBench是一个新的基准测试，同时评估语言模型在推理和编码任务中的准确性和token效率，揭示了模型间存在显著的效率差异。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注准确性和输出质量，忽略了token解码效率这一重要因素。在现实系统中，生成10,000个token与100,000个token在延迟、成本和能耗上有巨大差异。

Method: 引入OckBench，一个模型无关和硬件无关的基准测试，同时评估推理和编码任务的准确性和token数量。通过实验比较多个开源和闭源模型，分析效率差异并绘制准确率-效率平面上的帕累托前沿。

Result: 发现许多准确性相近的模型在token消耗上差异巨大，揭示了效率方差是一个被忽视但重要的差异化维度。实验展示了准确率-效率平面的帕累托前沿。

Conclusion: 呼吁评估范式转变：不应再将token视为可以"免费"生成的资源。OckBench为测量、比较和指导高效token推理研究提供了统一平台。

Abstract: Large language models such as GPT-4, Claude 3, and the Gemini series have
improved automated reasoning and code generation. However, existing benchmarks
mainly focus on accuracy and output quality, and they ignore an important
factor: decoding token efficiency. In real systems, generating 10,000 tokens
versus 100,000 tokens leads to large differences in latency, cost, and energy.
In this work, we introduce OckBench, a model-agnostic and hardware-agnostic
benchmark that evaluates both accuracy and token count for reasoning and coding
tasks. Through experiments comparing multiple open- and closed-source models,
we uncover that many models with comparable accuracy differ wildly in token
consumption, revealing that efficiency variance is a neglected but significant
axis of differentiation. We further demonstrate Pareto frontiers over the
accuracy-efficiency plane and argue for an evaluation paradigm shift: we should
no longer treat tokens as "free" to multiply. OckBench provides a unified
platform for measuring, comparing, and guiding research in token-efficient
reasoning. Our benchmarks are available at https://ockbench.github.io/ .

</details>


### [173] [In-Context Learning Without Copying](https://arxiv.org/abs/2511.05743)
*Kerem Sahin,Sheridan Feucht,Adam Belfki,Jannik Brinkmann,Aaron Mueller,David Bau,Chris Wendler*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Induction heads are attention heads that perform inductive copying by
matching patterns from earlier context and copying their continuations
verbatim. As models develop induction heads, they often experience a sharp drop
in training loss, a phenomenon cited as evidence that induction heads may serve
as a prerequisite for more complex in-context learning (ICL) capabilities. In
this work, we ask whether transformers can still acquire ICL capabilities when
inductive copying is suppressed. We propose Hapax, a setting where we omit the
loss contribution of any token that can be correctly predicted by induction
heads. Despite a significant reduction in inductive copying, performance on
abstractive ICL tasks (i.e., tasks where the answer is not contained in the
input context) remains comparable and surpasses the vanilla model on 13 of 21
tasks, even though 31.7\% of tokens are omitted from the loss. Furthermore, our
model achieves lower loss values on token positions that cannot be predicted
correctly by induction heads. Mechanistic analysis further shows that models
trained with Hapax develop fewer and weaker induction heads but still preserve
ICL capabilities. Taken together, our findings indicate that inductive copying
is not essential for learning abstractive ICL mechanisms.

</details>


### [174] [Language Generation: Complexity Barriers and Implications for Learning](https://arxiv.org/abs/2511.05759)
*Marcelo Arenas,Pablo Barceló,Luis Cofré,Alexander Kozachinskiy*

Main category: cs.CL

TL;DR: 本研究证明了即使对于简单的语言族（如正则语言和上下文无关语言），语言生成所需的样本数量也可能极其庞大，甚至不可计算，揭示了理论可能性与实际可学习性之间的巨大差距。


<details>
  <summary>Details</summary>
Motivation: Kleinberg和Mullainathan的理论证明了语言生成的可能性，但未考虑实际可行性。现代语言模型在实践中的成功需要一个更细致的解释框架，需要研究实际生成效率与理论保证之间的差距。

Method: 通过分析简单且研究充分的语言族（正则语言和上下文无关语言），量化语言生成所需的样本数量，证明了其可能达到不可计算的上界。

Result: 研究发现成功语言生成所需的示例数量可能极其庞大，在某些情况下甚至不被任何可计算函数所限制，这表明理论可能性与实际效率之间存在实质性差距。

Conclusion: 解释现代语言模型的实际成功需要考虑自然语言的结构特性，这些特性使得实际中的有效生成成为可能，超越了单纯的理论可能性保证。

Abstract: Kleinberg and Mullainathan showed that, in principle, language generation is
always possible: with sufficiently many positive examples, a learner can
eventually produce sentences indistinguishable from those of a target language.
However, the existence of such a guarantee does not speak to its practical
feasibility. In this work, we show that even for simple and well-studied
language families -- such as regular and context-free languages -- the number
of examples required for successful generation can be extraordinarily large,
and in some cases not bounded by any computable function. These results reveal
a substantial gap between theoretical possibility and efficient learnability.
They suggest that explaining the empirical success of modern language models
requires a refined perspective -- one that takes into account structural
properties of natural language that make effective generation possible in
practice.

</details>


### [175] [DRAGON: Guard LLM Unlearning in Context via Negative Detection and Reasoning](https://arxiv.org/abs/2511.05784)
*Yaxuan Wang,Chris Yuhao Liu,Quan Liu,Jinglong Pang,Wei Wei,Yujia Bao,Yang Liu*

Main category: cs.CL

TL;DR: 本文提出DRAGON框架，通过轻量级检测模块和思维链推理技术，在不修改模型权重的情况下实现大语言模型的高效遗忘，解决了传统方法依赖保留数据的问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型需要遗忘私有数据和有害知识来保护隐私和安全。现有方法依赖微调，通常需要保留数据进行训练，而实际场景中这些数据往往不可用，限制了方法的实用性。

Method: 提出DRAGON（检测-推理增强生成）框架，包含轻量级检测模块识别需要遗忘的提示词，然后通过专门的CoT守卫模型进行安全准确的上下文干预。该方法利用LLM固有的指令跟随能力，无需访问保留数据或修改基础模型权重。

Result: 在三个代表性遗忘任务上的广泛实验验证了DRAGON的有效性，展现出强大的遗忘能力、可扩展性和实用性。同时提出了新的遗忘性能评估指标和连续遗忘设置。

Conclusion: DRAGON为实际部署的大语言模型提供了一种实用的遗忘解决方案，通过推理机制而非模型修改，实现了在数据受限场景下的高效遗忘，具有重要的实际应用价值。

Abstract: Unlearning in Large Language Models (LLMs) is crucial for protecting private
data and removing harmful knowledge. Most existing approaches rely on
fine-tuning to balance unlearning efficiency with general language
capabilities. However, these methods typically require training or access to
retain data, which is often unavailable in real world scenarios. Although these
methods can perform well when both forget and retain data are available, few
works have demonstrated equivalent capability in more practical, data-limited
scenarios. To overcome these limitations, we propose Detect-Reasoning Augmented
GeneratiON (DRAGON), a systematic, reasoning-based framework that utilizes
in-context chain-of-thought (CoT) instructions to guard deployed LLMs before
inference. Instead of modifying the base model, DRAGON leverages the inherent
instruction-following ability of LLMs and introduces a lightweight detection
module to identify forget-worthy prompts without any retain data. These are
then routed through a dedicated CoT guard model to enforce safe and accurate
in-context intervention. To robustly evaluate unlearning performance, we
introduce novel metrics for unlearning performance and the continual unlearning
setting. Extensive experiments across three representative unlearning tasks
validate the effectiveness of DRAGON, demonstrating its strong unlearning
capability, scalability, and applicability in practical scenarios.

</details>


### [176] [Quantifying Edits Decay in Fine-tuned LLMs](https://arxiv.org/abs/2511.05852)
*Yinjie Cheng,Paul Youssef,Christin Seifert,Jörg Schlötterer,Zhixue Zhao*

Main category: cs.CL

TL;DR: 该研究发现大语言模型中的知识编辑在微调后会衰减，不同配置下编辑的存活率不同，AlphaEdit编辑比MEMIT编辑衰减更多。选择性层微调能有效移除编辑，但会对下游性能造成轻微损失。


<details>
  <summary>Details</summary>
Motivation: 知识编辑和微调作为两种广泛采用的后期训练干预方法，一直被孤立研究。这引发了一个关键问题：如果我们对编辑后的模型进行微调，编辑内容是否能够保留？这个问题的研究出于两个实际场景需求：移除隐蔽或恶意编辑，以及保留有益编辑。当前知识编辑方法的实用性和安全性存在风险。

Method: 研究者系统量化了微调后的编辑衰减情况。实验评估了两种最先进的编辑方法（MEMIT、AlphaEdit）和三种微调方法（全参数、LoRA、DoRA），在五个大语言模型和三个数据集上进行了232种实验配置的测试，并提出了选择性层微调策略。

Result: 编辑在微调后会衰减，不同配置下存活率不同，AlphaEdit编辑比MEMIT编辑衰减更多。选择性层微调仅微调编辑层可有效移除编辑，但会轻微影响下游性能。令人意外的是，微调非编辑层比全微调会破坏更多编辑。

Conclusion: 该研究为知识编辑与微调的整合建立了经验基线和可操作策略，强调评估模型编辑时需要考虑完整的大语言模型应用流程，为实际应用提供了重要指导。

Abstract: Knowledge editing has emerged as a lightweight alternative to retraining for
correcting or injecting specific facts in large language models (LLMs).
Meanwhile, fine-tuning remains the default operation for adapting LLMs to new
domains and tasks. Despite their widespread adoption, these two post-training
interventions have been studied in isolation, leaving open a crucial question:
if we fine-tune an edited model, do the edits survive? This question is
motivated by two practical scenarios: removing covert or malicious edits, and
preserving beneficial edits. If fine-tuning impairs edits as shown in Figure 1,
current KE methods become less useful, as every fine-tuned model would require
re-editing, which significantly increases the cost; if edits persist,
fine-tuned models risk propagating hidden malicious edits, raising serious
safety concerns. To this end, we systematically quantify edits decay after
fine-tuning, investigating how fine-tuning affects knowledge editing. We
evaluate two state-of-the-art editing methods (MEMIT, AlphaEdit) and three
fine-tuning approaches (full-parameter, LoRA, DoRA) across five LLMs and three
datasets, yielding 232 experimental configurations. Our results show that edits
decay after fine-tuning, with survival varying across configurations, e.g.,
AlphaEdit edits decay more than MEMIT edits. Further, we propose
selective-layer fine-tuning and find that fine-tuning edited layers only can
effectively remove edits, though at a slight cost to downstream performance.
Surprisingly, fine-tuning non-edited layers impairs more edits than full
fine-tuning. Overall, our study establishes empirical baselines and actionable
strategies for integrating knowledge editing with fine-tuning, and underscores
that evaluating model editing requires considering the full LLM application
pipeline.

</details>


### [177] [Retrieval-Augmented Generation in Medicine: A Scoping Review of Technical Implementations, Clinical Applications, and Ethical Considerations](https://arxiv.org/abs/2511.05901)
*Rui Yang,Matthew Yu Heng Wong,Huitao Li,Xin Li,Wentao Zhu,Jingchi Liao,Kunyu Yu,Jonathan Chong Kai Liew,Weihao Xuan,Yingjian Chen,Yuhe Ke,Jasmine Chiat Ling Ong,Douglas Teodoro,Chuan Hong,Daniel Shi Wei Ting,Nan Liu*

Main category: cs.CL

TL;DR: 医学检索增强生成（RAG）技术综述显示该领域仍处于早期阶段，主要依赖公开数据和通用模型，在临床验证、跨语言适应和安全性评估等方面存在局限，需要进一步发展以实现可信的全球应用。


<details>
  <summary>Details</summary>
Motivation: 医学知识快速增长和临床实践复杂性增加带来挑战，大语言模型虽有价值但存在固有局限性，需要RAG技术来增强其在临床实践中的适用性和可靠性。

Method: 采用综述研究方法，系统回顾了RAG在医学领域的应用现状，分析了数据使用、检索方法、模型选择、评估方式和应用场景等方面的特点。

Result: 研究发现医学RAG应用主要依赖公开数据而非私有数据；检索方法常用英语为中心的嵌入模型；大多使用通用LLMs而非医学专用模型；评估侧重生成质量和准确性，对偏见和安全性关注不足；应用集中在问答、报告生成、文本摘要和信息提取四个场景。

Conclusion: 医学RAG技术仍处于早期发展阶段，需要在临床验证、跨语言适应、低资源环境支持以及偏见和安全性评估等方面取得突破，才能实现可信和负责任的全球医疗应用。

Abstract: The rapid growth of medical knowledge and increasing complexity of clinical
practice pose challenges. In this context, large language models (LLMs) have
demonstrated value; however, inherent limitations remain. Retrieval-augmented
generation (RAG) technologies show potential to enhance their clinical
applicability. This study reviewed RAG applications in medicine. We found that
research primarily relied on publicly available data, with limited application
in private data. For retrieval, approaches commonly relied on English-centric
embedding models, while LLMs were mostly generic, with limited use of
medical-specific LLMs. For evaluation, automated metrics evaluated generation
quality and task performance, whereas human evaluation focused on accuracy,
completeness, relevance, and fluency, with insufficient attention to bias and
safety. RAG applications were concentrated on question answering, report
generation, text summarization, and information extraction. Overall, medical
RAG remains at an early stage, requiring advances in clinical validation,
cross-linguistic adaptation, and support for low-resource settings to enable
trustworthy and responsible global use.

</details>


### [178] [NILC: Discovering New Intents with LLM-assisted Clustering](https://arxiv.org/abs/2511.05913)
*Hongtao Wang,Renchi Yang,Wenqing Lin*

Main category: cs.CL

TL;DR: NILC提出了一种新颖的聚类框架，通过结合大语言模型进行语义质心生成和困难样本增强，实现了在新意图发现任务上的显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的新意图发现方法采用级联架构（嵌入编码+K-Means聚类），存在两个关键缺陷：1) 两个步骤之间缺乏相互反馈机制，无法实现联合优化；2) 仅基于嵌入向量的聚类忽略了细粒度的文本语义信息，导致性能次优。

Method: NILC采用迭代工作流程：首先利用LLM为聚类创建语义质心以增强欧几里得质心的上下文语义；然后识别聚类中的困难样本（模糊或简洁的utterance），通过LLM重写增强这些样本；进一步在半监督设置中通过种子技术和软必须链接注入监督信号，实现更准确的新意图发现。

Result: 在六个不同领域的基准数据集上，无论是在无监督还是半监督设置下，NILC相比多个最新基线方法都取得了显著的性能提升，证明了其有效性和泛化能力。

Conclusion: NILC通过LLM增强的迭代聚类框架，成功解决了传统级联方法中的缺陷，为新意图发现提供了一种更有效的解决方案，在多个实际应用场景中展现出优越性能。

Abstract: New intent discovery (NID) seeks to recognize both new and known intents from
unlabeled user utterances, which finds prevalent use in practical dialogue
systems. Existing works towards NID mainly adopt a cascaded architecture,
wherein the first stage focuses on encoding the utterances into informative
text embeddings beforehand, while the latter is to group similar embeddings
into clusters (i.e., intents), typically by K-Means. However, such a cascaded
pipeline fails to leverage the feedback from both steps for mutual refinement,
and, meanwhile, the embedding-only clustering overlooks nuanced textual
semantics, leading to suboptimal performance. To bridge this gap, this paper
proposes NILC, a novel clustering framework specially catered for effective
NID. Particularly, NILC follows an iterative workflow, in which clustering
assignments are judiciously updated by carefully refining cluster centroids and
text embeddings of uncertain utterances with the aid of large language models
(LLMs). Specifically, NILC first taps into LLMs to create additional semantic
centroids for clusters, thereby enriching the contextual semantics of the
Euclidean centroids of embeddings. Moreover, LLMs are then harnessed to augment
hard samples (ambiguous or terse utterances) identified from clusters via
rewriting for subsequent cluster correction. Further, we inject supervision
signals through non-trivial techniques seeding and soft must links for more
accurate NID in the semi-supervised setting. Extensive experiments comparing
NILC against multiple recent baselines under both unsupervised and
semi-supervised settings showcase that NILC can achieve significant performance
improvements over six benchmark datasets of diverse domains consistently.

</details>


### [179] [IDALC: A Semi-Supervised Framework for Intent Detection and Active Learning based Correction](https://arxiv.org/abs/2511.05921)
*Ankan Mullick,Sukannya Purkayastha,Saransh Sharma,Pawan Goyal,Niloy Ganguly*

Main category: cs.CL

TL;DR: IDALC是一个半监督框架，用于检测用户意图和纠正系统拒绝的话语，同时最小化人工标注需求。


<details>
  <summary>Details</summary>
Motivation: 现有的语音对话系统在处理已知意图时可能出现低置信度导致拒绝的情况，且需要不断添加新意图。手动标注所有拒绝话语和新兴意图成本高昂且不实际。

Method: 提出IDALC框架，结合意图检测和基于主动学习的校正机制，通过智能选择最有价值的数据进行标注来减少人工成本。

Result: 在基准数据集上比基线方法提高5-10%准确率和4-8%宏F1值，仅使用可用未标注数据的6-10%进行标注。

Conclusion: IDALC有效降低了对话系统的标注成本，同时提高了意图检测和拒绝话语校正的性能。

Abstract: Voice-controlled dialog systems have become immensely popular due to their
ability to perform a wide range of actions in response to diverse user queries.
These agents possess a predefined set of skills or intents to fulfill specific
user tasks. But every system has its own limitations. There are instances
where, even for known intents, if any model exhibits low confidence, it results
in rejection of utterances that necessitate manual annotation. Additionally, as
time progresses, there may be a need to retrain these agents with new intents
from the system-rejected queries to carry out additional tasks. Labeling all
these emerging intents and rejected utterances over time is impractical, thus
calling for an efficient mechanism to reduce annotation costs. In this paper,
we introduce IDALC (Intent Detection and Active Learning based Correction), a
semi-supervised framework designed to detect user intents and rectify
system-rejected utterances while minimizing the need for human annotation.
Empirical findings on various benchmark datasets demonstrate that our system
surpasses baseline methods, achieving a 5-10% higher accuracy and a 4-8%
improvement in macro-F1. Remarkably, we maintain the overall annotation cost at
just 6-10% of the unlabelled data available to the system. The overall
framework of IDALC is shown in Fig. 1

</details>


### [180] [Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs](https://arxiv.org/abs/2511.05933)
*Renfei Zhang,Manasa Kaniselvan,Niloofar Mireshghallah*

Main category: cs.CL

TL;DR: 强化学习增强的语言模型在知识回忆任务上表现更优，特别是结构化/层次化知识，这并非来自新数据，而是来自改进的导航现有知识层次的程序性技能。


<details>
  <summary>Details</summary>
Motivation: 挑战传统观点，即强化学习在提升语言模型推理和泛化能力的同时会损害记忆知识。观察到RL增强模型在纯知识回忆任务上持续超越基础模型和监督微调模型。

Method: 比较RL增强、基础和SFT模型在知识回忆任务上的表现；使用结构化提示引导SFT模型进行层次遍历；进行逐层内部激活分析来理解知识表示的变化。

Result: RL增强模型在知识回忆上表现更优；结构化提示将性能差距从24个百分点缩小到7个百分点；RL模型在深层检索任务中保持更优的程序路径回忆能力；事实表示在SFT和RL模型间保持相似，但查询表示明显不同。

Conclusion: 强化学习主要改变模型遍历知识的方式，而非知识表示本身。RL增强的是模型的程序性技能，使其能够更好地导航和搜索参数中现有的知识层次结构。

Abstract: Reinforcement learning (RL) is often credited with improving language model
reasoning and generalization at the expense of degrading memorized knowledge.
We challenge this narrative by observing that RL-enhanced models consistently
outperform their base and supervised fine-tuned (SFT) counterparts on pure
knowledge recall tasks, particularly those requiring traversal of hierarchical,
structured knowledge (e.g., medical codes). We hypothesize these gains stem not
from newly acquired data, but from improved procedural skills in navigating and
searching existing knowledge hierarchies within the model parameters. To
support this hypothesis, we show that structured prompting, which explicitly
guides SFTed models through hierarchical traversal, recovers most of the
performance gap (reducing 24pp to 7pp on MedConceptsQA for DeepSeek-V3/R1). We
further find that while prompting improves final-answer accuracy, RL-enhanced
models retain superior ability to recall correct procedural paths on
deep-retrieval tasks. Finally our layer-wise internal activation analysis
reveals that while factual representations (e.g., activations for the statement
"code 57.95 refers to urinary infection") maintain high cosine similarity
between SFT and RL models, query representations (e.g., "what is code 57.95")
diverge noticeably, indicating that RL primarily transforms how models traverse
knowledge rather than the knowledge representation itself.

</details>


### [181] [Interpretable Recognition of Cognitive Distortions in Natural Language Texts](https://arxiv.org/abs/2511.05969)
*Anton Kolonin,Anna Arinicheva*

Main category: cs.CL

TL;DR: 提出了一种基于加权结构化模式(N-grams)的多因素文本分类新方法，考虑了模式间的异层次关系，用于自动检测心理护理中的认知扭曲。


<details>
  <summary>Details</summary>
Motivation: 解决具有社会影响力的心理护理中认知扭曲自动检测问题，需要可解释、鲁棒且透明的人工智能模型来辅助临床工作。

Method: 使用加权结构化模式如N-grams，考虑模式间的异层次关系，提出新的识别和学习算法来构建分类模型。

Result: 在两个公开数据集上测试，显著提高了文献已知的F1分数，确定了最优超参数，代码和模型已开源供社区使用。

Conclusion: 该方法在认知扭曲检测任务上超越了当前技术水平，为研究和实践提供了有效且可复用的技术工具。

Abstract: We propose a new approach to multi-factor classification of natural language
texts based on weighted structured patterns such as N-grams, taking into
account the heterarchical relationships between them, applied to solve such a
socially impactful problem as the automation of detection of specific cognitive
distortions in psychological care, relying on an interpretable, robust and
transparent artificial intelligence model. The proposed recognition and
learning algorithms improve the current state of the art in this field. The
improvement is tested on two publicly available datasets, with significant
improvements over literature-known F1 scores for the task, with optimal
hyper-parameters determined, having code and models available for future use by
the community.

</details>


### [182] [Revisiting Entropy in Reinforcement Learning for Large Reasoning Models](https://arxiv.org/abs/2511.05993)
*Renren Jin,Pengzhi Gao,Yuqi Ren,Zhuowen Han,Tongxuan Zhang,Wuwei Huang,Wei Liu,Jian Luan,Deyi Xiong*

Main category: cs.CL

TL;DR: 本研究通过大规模实验分析了强化学习与可验证奖励(RLVR)训练中大语言模型的熵动力学，发现离策略更新次数、训练数据多样性和裁剪阈值是影响熵的关键因素，并提出通过调节正负优势token的损失权重来有效控制熵崩溃的方法。


<details>
  <summary>Details</summary>
Motivation: 尽管RLVR已成为增强LLM推理能力的主要方法，但训练过程中熵的崩溃会导致模型过早收敛到次优局部最小值，阻碍性能进一步提升。现有方法虽能缓解熵崩溃，但缺乏对RLVR中熵现象的系统性研究。

Method: 通过大规模实验研究RLVR训练的LLM熵动力学，分析模型熵与响应多样性、校准和性能的相关性，从理论和实证角度探究熵崩溃的根本原因，并识别影响熵的关键因素。

Result: 研究发现离策略更新次数、训练数据多样性和优化目标中的裁剪阈值是影响RLVR训练LLM熵的关键因素。理论和实证证明具有正优势的token是熵崩溃的主要贡献者，通过调整正负优势token的相对损失权重可以有效调节模型熵。

Conclusion: 本研究为理解RLVR中的熵现象提供了全面的视角，提出的基于token优势的熵调节方法为解决LLM训练中的熵崩溃问题提供了新思路，有助于提升RLVR训练的LLM性能。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a
predominant approach for enhancing the reasoning capabilities of large language
models (LLMs). However, the entropy of LLMs usually collapses during RLVR
training, causing premature convergence to suboptimal local minima and hinder
further performance improvement. Although various approaches have been proposed
to mitigate entropy collapse, a comprehensive study of entropy in RLVR remains
lacking. To address this gap, we conduct extensive experiments to investigate
the entropy dynamics of LLMs trained with RLVR and analyze how model entropy
correlates with response diversity, calibration, and performance across various
benchmarks. Our findings reveal that the number of off-policy updates, the
diversity of training data, and the clipping thresholds in the optimization
objective are critical factors influencing the entropy of LLMs trained with
RLVR. Moreover, we theoretically and empirically demonstrate that tokens with
positive advantages are the primary contributors to entropy collapse, and that
model entropy can be effectively regulated by adjusting the relative loss
weights of tokens with positive and negative advantages during training.

</details>


### [183] [LLMs Do Not See Age: Assessing Demographic Bias in Automated Systematic Review Synthesis](https://arxiv.org/abs/2511.06000)
*Favour Yahdii Aghaebe,Tanefa Apekey,Elizabeth Williams,Nafise Sadat Moosavi*

Main category: cs.CL

TL;DR: 研究发现语言模型在生成生物医学摘要时无法很好地保留年龄相关人口统计信息，存在年龄偏见和幻觉问题


<details>
  <summary>Details</summary>
Motivation: 随着语言模型在生物医学证据合成工作流程中的应用日益增加，需要评估这些系统是否能保留关键的人口统计信息（如年龄），因为医疗干预的安全性往往取决于患者年龄

Method: 构建了DemogSummary数据集（按年龄分层的系统性综述研究数据集），评估了三个LLM（Qwen、Longformer和GPT-4.1 Nano），使用标准指标和新提出的Demographic Salience Score (DSS)来量化年龄相关实体保留和幻觉情况

Result: 发现模型和年龄组间存在系统性差异：以成年人为重点的摘要人口统计保真度最低，代表性不足的人群（儿童和老年人）更容易出现幻觉

Conclusion: 当前LLM在忠实无偏见摘要方面存在局限性，需要在生物医学NLP中开发公平感知的评估框架和摘要管道

Abstract: Clinical interventions often hinge on age: medications and procedures safe
for adults may be harmful to children or ineffective for older adults. However,
as language models are increasingly integrated into biomedical evidence
synthesis workflows, it remains uncertain whether these systems preserve such
crucial demographic distinctions. To address this gap, we evaluate how well
state-of-the-art language models retain age-related information when generating
abstractive summaries of biomedical studies. We construct DemogSummary, a novel
age-stratified dataset of systematic review primary studies, covering child,
adult, and older adult populations. We evaluate three prominent
summarisation-capable LLMs, Qwen (open-source), Longformer (open-source) and
GPT-4.1 Nano (proprietary), using both standard metrics and a newly proposed
Demographic Salience Score (DSS), which quantifies age-related entity retention
and hallucination. Our results reveal systematic disparities across models and
age groups: demographic fidelity is lowest for adult-focused summaries, and
under-represented populations are more prone to hallucinations. These findings
highlight the limitations of current LLMs in faithful and bias-free
summarisation and point to the need for fairness-aware evaluation frameworks
and summarisation pipelines in biomedical NLP.

</details>


### [184] [Multi-Reward GRPO Fine-Tuning for De-biasing Large Language Models: A Study Based on Chinese-Context Discrimination Data](https://arxiv.org/abs/2511.06023)
*Deng Yixuan,Ji Xiaoqiang*

Main category: cs.CL

TL;DR: 本文提出多奖励组相对策略优化(GRPO)框架，通过构建中文语境歧视数据集和多维度奖励模型，有效降低大语言模型的偏见，提升伦理对齐。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型存在反映社会刻板印象的隐含偏见，传统对齐技术如RLHF和DPO在处理文化特定和多维度歧视方面效果有限。

Method: 构建基于中文语境歧视类别的合成英语数据集，训练基于DeBERTa-v3的多维度奖励模型，通过GRPO框架优化模型在公平性、中立性和语言质量方面的表现。

Result: 实验表明该方法显著降低偏见强度，提升与非歧视标准的对齐效果，同时保持回答的流畅性和信息量。

Conclusion: GRPO基础的多奖励优化是有效的LLM去偏见方法，为文化语境下的伦理对齐提供了可复制的框架。

Abstract: Large Language Models (LLMs) often exhibit implicit biases and discriminatory
tendencies that reflect underlying social stereotypes. While recent alignment
techniques such as RLHF and DPO have mitigated some of these issues, they
remain limited in addressing culturally specific and multi-dimensional forms of
discrimination. This paper proposes a Multi-Reward Group Relative Policy
Optimization (GRPO) framework to fine-tune LLMs toward ethical and bias-free
behavior. Our approach constructs a synthetic English-language dataset derived
from Chinese-context discrimination categories, including regional, ethnic, and
occupational biases. Each instance is paired with both neutral and biased
responses to train a reward model based on DeBERTa-v3, which provides
multi-dimensional reward signals capturing fairness, neutrality, and linguistic
quality. The trained reward model then guides GRPO fine-tuning to optimize
model outputs along these ethical dimensions. Experimental results demonstrate
significant reductions in bias intensity and improved alignment with
non-discriminatory standards without compromising fluency or informativeness.
This study highlights the effectiveness of GRPO-based multi-reward optimization
for de-biasing LLMs and offers a replicable framework for cultural-contextual
ethical alignment.

</details>


### [185] [Visual Exploration of Feature Relationships in Sparse Autoencoders with Curated Concepts](https://arxiv.org/abs/2511.06048)
*Xinyuan Yan,Shusen Liu,Kowshik Thopalli,Bei Wang*

Main category: cs.CL

TL;DR: 本文提出了一种针对稀疏自编码器（SAE）特征的交互式可视化框架，通过结合拓扑可视化和降维技术，实现有针对性的概念探索而非全局可视化，解决了高维特征空间中的探索难题。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器虽然能够从大型语言模型中提取可解释特征，但特征数量庞大使得全面探索变得不可行。传统的降维技术（如UMAP）存在高维压缩失真、过度绘制和邻域扭曲等局限性，无法有效支持对SAE特征的深入分析。

Method: 提出了一个重点探索框架，优先考虑策划概念及其对应的SAE特征，而不是尝试同时可视化所有特征。开发了结合拓扑视觉编码与降维的交互式可视化系统，能够忠实地表示选定特征之间的局部和全局关系。

Result: 该混合方法使用户能够通过有针对性的、可解释的子集来研究SAE行为，为潜在空间中的概念表示提供了更深入、更细致的分析能力，有效克服了传统可视化方法的局限性。

Conclusion: 通过采用重点探索策略和混合可视化技术，该方法为理解大型语言模型中的稀疏特征提供了一种实用工具，使用户能够更有效地探索和分析特定概念在潜在空间中的表示，推动了可解释AI研究的发展。

Abstract: Sparse autoencoders (SAEs) have emerged as a powerful tool for uncovering
interpretable features in large language models (LLMs) through the sparse
directions they learn. However, the sheer number of extracted directions makes
comprehensive exploration intractable. While conventional embedding techniques
such as UMAP can reveal global structure, they suffer from limitations
including high-dimensional compression artifacts, overplotting, and misleading
neighborhood distortions. In this work, we propose a focused exploration
framework that prioritizes curated concepts and their corresponding SAE
features over attempts to visualize all available features simultaneously. We
present an interactive visualization system that combines topology-based visual
encoding with dimensionality reduction to faithfully represent both local and
global relationships among selected features. This hybrid approach enables
users to investigate SAE behavior through targeted, interpretable subsets,
facilitating deeper and more nuanced analysis of concept representation in
latent space.

</details>


### [186] [Efficient Hate Speech Detection: A Three-Layer LoRA-Tuned BERTweet Framework](https://arxiv.org/abs/2511.06051)
*Mahmoud El-Bahnasawi*

Main category: cs.CL

TL;DR: 提出一种计算高效的仇恨言论检测系统，结合规则预过滤和LoRA微调的BERTweet模型，在仅用1%参数量的情况下达到94%的顶尖模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决开发计算高效的仇恨言论检测系统的关键挑战，需要在保持竞争力的同时满足实时部署的实际需求。

Method: 提出三层框架：1) 基于规则的预过滤；2) 参数高效的LoRA微调BERTweet模型；3) 持续学习能力。通过数据集统一策略和优化微调方法提升性能。

Result: 实现0.85的宏F1分数，达到SafePhi性能的94%，但模型规模小100倍（134M vs 14B参数）。仅需1.87M可训练参数（全微调的1.37%），在单张T4 GPU上约2小时完成训练。

Conclusion: 该系统在资源受限环境中实现了高效的仇恨言论检测，为实际部署提供了可行的解决方案，在计算成本和准确率之间实现了良好平衡。

Abstract: This paper addresses the critical challenge of developing computationally
efficient hate speech detection systems that maintain competitive performance
while being practical for real-time deployment. We propose a novel three-layer
framework that combines rule-based pre-filtering with a parameter-efficient
LoRA-tuned BERTweet model and continuous learning capabilities. Our approach
achieves 0.85 macro F1 score - representing 94% of the performance of
state-of-the-art large language models like SafePhi (Phi-4 based) while using a
base model that is 100x smaller (134M vs 14B parameters). Compared to
traditional BERT-based approaches with similar computational requirements, our
method demonstrates superior performance through strategic dataset unification
and optimized fine-tuning. The system requires only 1.87M trainable parameters
(1.37% of full fine-tuning) and trains in approximately 2 hours on a single T4
GPU, making robust hate speech detection accessible in resource-constrained
environments while maintaining competitive accuracy for real-world deployment.

</details>


### [187] [ReMoD: Rethinking Modality Contribution in Multimodal Stance Detection via Dual Reasoning](https://arxiv.org/abs/2511.06057)
*Bingbing Wang,Zhengda Jin,Bin Liang,Jing Li,Ruifeng Xu*

Main category: cs.CL

TL;DR: 本文提出了ReMoD框架，通过双推理范式重新思考多模态立场检测中的模态贡献，结合直觉推理和反思推理机制动态调整不同模态的表达权重。


<details>
  <summary>Details</summary>
Motivation: 现有多模态立场检测方法简单地融合各模态信息，忽视了不同模态在立场表达中的贡献差异，导致粗略的模态组合可能引入立场理解噪音，影响检测准确性。

Method: 受人类认知双过程理论启发，提出ReMoD框架：1）直觉推理阶段通过查询模态经验池(MEP)和语义经验池(SEP)形成初始立场假设；2）反思推理阶段通过两条推理链优化 - 模态CoT更新MEP以放大相关模态，语义CoT通过深层语境洞察完善SEP，动态调整模态权重。

Result: 在公开MMSD基准测试上，ReMoD显著优于大多数基线模型，展现出强大的泛化能力，证明了双推理机制在多模态立场检测中的有效性。

Conclusion: 通过模拟人类认知的双过程理论，ReMoD框架成功解决了多模态立场检测中的模态贡献不平衡问题，通过动态调整和持续优化的经验结构，实现了更鲁棒和上下文感知的立场决策。

Abstract: Multimodal Stance Detection (MSD) is a crucial task for understanding public
opinion on social media. Existing work simply fuses information from various
modalities to learn stance representations, overlooking the varying
contributions of stance expression from different modalities. Therefore, stance
misunderstanding noises may be drawn into the stance learning process due to
the risk of learning errors by rough modality combination. To address this, we
get inspiration from the dual-process theory of human cognition and propose
**ReMoD**, a framework that **Re**thinks **Mo**dality contribution of stance
expression through a **D**ual-reasoning paradigm. ReMoD integrates
*experience-driven intuitive reasoning* to capture initial stance cues with
*deliberate reflective reasoning* to adjust for modality biases, refine stance
judgments, and thereby dynamically weight modality contributions based on their
actual expressive power for the target stance. Specifically, the intuitive
stage queries the Modality Experience Pool (MEP) and Semantic Experience Pool
(SEP) to form an initial stance hypothesis, prioritizing historically impactful
modalities. This hypothesis is then refined in the reflective stage via two
reasoning chains: Modality-CoT updates MEP with adaptive fusion strategies to
amplify relevant modalities, while Semantic-CoT refines SEP with deeper
contextual insights of stance semantics. These dual experience structures are
continuously refined during training and recalled at inference to guide robust
and context-aware stance decisions. Extensive experiments on the public MMSD
benchmark demonstrate that our ReMoD significantly outperforms most baseline
models and exhibits strong generalization capabilities.

</details>


### [188] [Automating Hardware Design and Verification from Architectural Papers via a Neural-Symbolic Graph Framework](https://arxiv.org/abs/2511.06067)
*Haoyue Yang,Xuanle Zhao,Yujie Liu,Zhuojun Zou,Kailin Lyu,Changchun Zhou,Yao Zhu,Jie Hao*

Main category: cs.CL

TL;DR: ArchCraft是一个将学术论文中的架构描述转换为可综合Verilog代码的框架，包含ArchSynthBench基准测试，在RTL生成和验证方面表现优越。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏公开的源代码和硬件描述语言的复杂性，从学术论文复现硬件架构是一个重大挑战，需要自动化工具来解决这一问题。

Method: 提出ArchCraft框架，使用形式图捕获架构蓝图和符号定义功能规范，将非结构化学术论文转换为可验证的硬件设计，生成解耦的RTL和测试台代码。

Result: 在包含50个项目级电路和约600个电路块的ArchSynthBench基准上，ArchCraft在论文理解和代码完成方面优于直接生成方法和VerilogCoder框架。

Conclusion: 生成的可执行RTL代码满足所有时序约束，性能指标与原始论文报告一致，证明了框架的有效性和实用性。

Abstract: The reproduction of hardware architectures from academic papers remains a
significant challenge due to the lack of publicly available source code and the
complexity of hardware description languages (HDLs). To this end, we propose
\textbf{ArchCraft}, a Framework that converts abstract architectural
descriptions from academic papers into synthesizable Verilog projects with
register-transfer level (RTL) verification. ArchCraft introduces a structured
workflow, which uses formal graphs to capture the Architectural Blueprint and
symbols to define the Functional Specification, translating unstructured
academic papers into verifiable, hardware-aware designs. The framework then
generates RTL and testbench (TB) code decoupled via these symbols to facilitate
verification and debugging, ultimately reporting the circuit's Power, Area, and
Performance (PPA). Moreover, we propose the first benchmark,
\textbf{ArchSynthBench}, for synthesizing hardware from architectural
descriptions, with a complete set of evaluation indicators, 50 project-level
circuits, and around 600 circuit blocks. We systematically assess ArchCraft on
ArchSynthBench, where the experiment results demonstrate the superiority of our
proposed method, surpassing direct generation methods and the VerilogCoder
framework in both paper understanding and code completion. Furthermore,
evaluation and physical implementation of the generated executable RTL code
show that these implementations meet all timing constraints without violations,
and their performance metrics are consistent with those reported in the
original papers.

</details>


### [189] [Stemming Hallucination in Language Models Using a Licensing Oracle](https://arxiv.org/abs/2511.06073)
*Simeon Emanuilov,Richard Ackermann*

Main category: cs.CL

TL;DR: 提出Licensing Oracle架构解决语言模型幻觉问题，通过结构化知识图谱形式验证确保生成真实性


<details>
  <summary>Details</summary>
Motivation: 语言模型虽具备优秀生成能力但易产生幻觉，在语法连贯输出中生成事实错误信息，影响可靠性

Method: 引入Licensing Oracle架构方案，将确定性验证步骤嵌入模型生成过程，基于结构化知识图谱进行形式验证执行真实性约束

Result: 实现完美规避精度(AP=1.0)和零错误答案(FAR-NE=0.0)，确保仅生成有效声明，事实回答准确率达89.1%，优于RAG和微调方法

Conclusion: 架构创新为结构化知识领域幻觉问题提供必要充分解决方案，奠定真实性约束生成基础，为可靠AI模型提供新路径

Abstract: Language models exhibit remarkable natural language generation capabilities
but remain prone to hallucinations, generating factually incorrect information
despite producing syntactically coherent responses. This study introduces the
Licensing Oracle, an architectural solution designed to stem hallucinations in
LMs by enforcing truth constraints through formal validation against structured
knowledge graphs. Unlike statistical approaches that rely on data scaling or
fine-tuning, the Licensing Oracle embeds a deterministic validation step into
the model's generative process, ensuring that only factually accurate claims
are made. We evaluated the effectiveness of the Licensing Oracle through
experiments comparing it with several state-of-the-art methods, including
baseline language model generation, fine-tuning for factual recall, fine-tuning
for abstention behavior, and retrieval-augmented generation (RAG). Our results
demonstrate that although RAG and fine-tuning improve performance, they fail to
eliminate hallucinations. In contrast, the Licensing Oracle achieved perfect
abstention precision (AP = 1.0) and zero false answers (FAR-NE = 0.0), ensuring
that only valid claims were generated with 89.1% accuracy in factual responses.
This work shows that architectural innovations, such as the Licensing Oracle,
offer a necessary and sufficient solution for hallucinations in domains with
structured knowledge representations, offering guarantees that statistical
methods cannot match. Although the Licensing Oracle is specifically designed to
address hallucinations in fact-based domains, its framework lays the groundwork
for truth-constrained generation in future AI systems, providing a new path
toward reliable, epistemically grounded models.

</details>


### [190] [MuonAll: Muon Variant for Efficient Finetuning of Large Language Models](https://arxiv.org/abs/2511.06086)
*Saurabh Page,Advait Joshi,S. S. Sonawane*

Main category: cs.CL

TL;DR: 该论文提出了MuonAll优化器，通过将参数转换为2D矩阵来扩展Muon优化器，使其能够处理所有模型参数。实验表明Muon和MuonAll在微调任务中与AdamW表现相当，证明了它们作为替代优化器的有效性。


<details>
  <summary>Details</summary>
Motivation: Muon优化器在语言模型预训练中表现良好，但在微调现有公共预训练模型中的性能尚未探索。当前Muon与AdamW结合使用存在改进空间，需要将所有参数整合到Muon中以提高效率。

Method: 提出MuonAll优化器，将所有参数转换为2D矩阵形式，完全纳入Muon优化框架。对多个人工智能语言模型（最大达5亿参数）进行广泛的微调实验，比较Muon、MuonAll与AdamW的性能。

Result: 实验结果表明，Muon和MuonAll在主要基准测试中与AdamW表现相当，验证了它们作为替代优化器的有效性。同时开源了Muon和MuonAll的分布式实现代码。

Conclusion: MuonAll成功扩展了Muon优化器的应用范围，使其能够完整处理模型参数。Muon和MuonAll在微调任务中展现出与AdamW相当的性能，为语言模型优化提供了新的替代方案，并提供了可用的开源实现。

Abstract: Muon optimizer has demonstrated robust results in pretraining of language
models but its performance in finetuning of existing public pretrained models
is not yet explored. Currently, Muon is used along with AdamW introducing a
scope of improvement for adopting all parameters inside Muon. We introduce
MuonAll, which incorporates all the parameters inside Muon by transforming into
2D matrices. We conduct extensive finetuning experiments across publicly
available language models with model sizes upto half billion parameters. Muon
and MuonAll perform at par with AdamW across major benchmarks, highlighting
their effectiveness as alternative optimizers. We open-source the distributed
implementations of Muon and MuonAll, available at
https://github.com/Saurabh750/optimizer

</details>


### [191] [Evaluation of retrieval-based QA on QUEST-LOFT](https://arxiv.org/abs/2511.06125)
*Nathan Scales,Nathanael Schärli,Olivier Bousquet*

Main category: cs.CL

TL;DR: 该论文分析了RAG和长上下文模型在QUEST-LOFT基准上的性能问题，并证明通过结构化输出格式和答案重验证可以优化RAG，使其显著超越长上下文方法。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG在学术界和工业界都很流行，但现有方法在处理需要跨多个文档信息或需要复杂推理的问题时表现不佳。LOFT研究表明这一局限性也适用于长上下文语言模型，特别是在QUEST基准上存在巨大提升空间。

Method: 该研究深入分析了QUEST-LOFT上性能不佳的因素，通过彻底的人工评估发布了更新数据，并展示了RAG与包含推理和证据的结构化输出格式相结合的方法，可选择性地进行答案重验证。

Result: 研究结果表明，通过结构化输出格式和答案重验证优化的RAG方法能够显著超越长上下文语言模型的性能。

Conclusion: 该论文证明了RAG系统的局限性可以通过适当的优化策略来克服，特别是通过结构化输出和重验证机制，为处理复杂问答任务提供了更有效的解决方案。

Abstract: Despite the popularity of retrieval-augmented generation (RAG) as a solution
for grounded QA in both academia and industry, current RAG methods struggle
with questions where the necessary information is distributed across many
documents or where retrieval needs to be combined with complex reasoning.
Recently, the LOFT study has shown that this limitation also applies to
approaches based on long-context language models, with the QUEST benchmark
exhibiting particularly large headroom. In this paper, we provide an in-depth
analysis of the factors contributing to the poor performance on QUEST-LOFT,
publish updated numbers based on a thorough human evaluation, and demonstrate
that RAG can be optimized to significantly outperform long-context approaches
when combined with a structured output format containing reasoning and
evidence, optionally followed by answer re-verification.

</details>


### [192] [Referring Expressions as a Lens into Spatial Language Grounding in Vision-Language Models](https://arxiv.org/abs/2511.06146)
*Akshar Tumu,Varad Shinde,Parisa Kordjamshidi*

Main category: cs.CL

TL;DR: 提出使用指代表达理解任务作为评估视觉语言模型空间推理能力的新平台，发现所有模型在处理空间语义时都面临挑战，但表现因模型类型和空间语义类别而异。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在空间推理方面存在困难，现有的图像描述和视觉问答任务无法充分评估模型在处理复杂空间关系、对象检测歧义性和否定表达时的理解能力。

Method: 采用指代表达理解任务作为评估平台，专门测试模型在三种情况下的空间推理：1) 对象检测的歧义性，2) 包含长句结构和多重空间关系的复杂空间表达，3) 包含否定的表达。同时评估了任务特定架构和大型视觉语言模型的表现。

Result: 所有测试模型在该任务上都面临挑战，但相对行为表现取决于底层模型架构和特定的空间语义类别（拓扑、方向、邻近等）。不同模型在不同类型的空间关系上展现出不同的强项和弱点。

Conclusion: 研究结果揭示了视觉语言模型在空间推理方面的具体挑战和行为模式，为识别研究空白和未来发展方向提供了洞察，特别是在处理复杂空间语义和实际应用场景中的空间理解能力方面。

Abstract: Spatial Reasoning is an important component of human cognition and is an area
in which the latest Vision-language models (VLMs) show signs of difficulty. The
current analysis works use image captioning tasks and visual question
answering. In this work, we propose using the Referring Expression
Comprehension task instead as a platform for the evaluation of spatial
reasoning by VLMs. This platform provides the opportunity for a deeper analysis
of spatial comprehension and grounding abilities when there is 1) ambiguity in
object detection, 2) complex spatial expressions with a longer sentence
structure and multiple spatial relations, and 3) expressions with negation
('not'). In our analysis, we use task-specific architectures as well as large
VLMs and highlight their strengths and weaknesses in dealing with these
specific situations. While all these models face challenges with the task at
hand, the relative behaviors depend on the underlying models and the specific
categories of spatial semantics (topological, directional, proximal, etc.). Our
results highlight these challenges and behaviors and provide insight into
research gaps and future directions.

</details>


### [193] [BookAsSumQA: An Evaluation Framework for Aspect-Based Book Summarization via Question Answering](https://arxiv.org/abs/2511.06183)
*Ryuhei Miyazato,Ting-Ruen Wei,Xuyang Wu,Hsin-Tai Wu,Kei Harada*

Main category: cs.CL

TL;DR: 提出BookAsSumQA，一个基于QA的评测框架，用于基于方面的书籍摘要，发现RAG方法在长文档上比LLM方法更有效


<details>
  <summary>Details</summary>
Motivation: 基于方面的摘要能生成个性化和有针对性的摘要，但由于长文本难以构建参考摘要，其在书籍领域的应用尚未被探索

Method: 提出BookAsSumQA框架，从叙事知识图谱自动生成方面特定的问答对，通过问答性能来评估摘要质量

Result: 实验显示，LLM方法在较短文本上表现更好，但随着文档长度增加，RAG方法变得更有效，更适合基于方面的书籍摘要

Conclusion: RAG方法对于基于方面的书籍摘要更高效实用，特别是在处理长文档时优势明显

Abstract: Aspect-based summarization aims to generate summaries that highlight specific
aspects of a text, enabling more personalized and targeted summaries. However,
its application to books remains unexplored due to the difficulty of
constructing reference summaries for long text. To address this challenge, we
propose BookAsSumQA, a QA-based evaluation framework for aspect-based book
summarization. BookAsSumQA automatically generates aspect-specific QA pairs
from a narrative knowledge graph to evaluate summary quality based on its
question-answering performance. Our experiments using BookAsSumQA revealed that
while LLM-based approaches showed higher accuracy on shorter texts, RAG-based
methods become more effective as document length increases, making them more
efficient and practical for aspect-based book summarization.

</details>


### [194] [Confidence-Guided Stepwise Model Routing for Cost-Efficient Reasoning](https://arxiv.org/abs/2511.06190)
*Sangmook Lee,Dohyung Kim,Hyukhun Koh,Nakyeong Yang,Kyomin Jung*

Main category: cs.CL

TL;DR: STEER是一种基于置信度引导的逐步模型路由框架，能够在小模型和大模型之间进行细粒度的步骤级路由，显著降低推理成本的同时保持或提升任务准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通过模型扩展和推理时技术提升了推理能力，但代价是更高的推理成本。现有的路由方法缺乏领域适应性的鲁棒性，且需要昂贵的数据合成技术获取路由标签。

Method: STEER框架利用小模型生成推理步骤前的置信度分数，仅在必要时调用大模型。该方法无需训练外部路由模型，通过模型内部置信度信号实现域无关的细粒度路由决策。

Result: 在数学推理、多跳问答和规划等多个领域的挑战性基准测试中，STEER实现了竞争性或增强的准确性，同时大幅降低推理成本（如AIME任务中准确率提升20%，FLOPs减少48%）。

Conclusion: 研究证明了模型内部置信度作为鲁棒、域无关的路由信号的有效性，为高效LLM部署提供了可扩展的解决方案，超越了依赖训练外部模块的基线方法。

Abstract: Recent advances in Large Language Models (LLMs) - particularly model scaling
and test-time techniques - have greatly enhanced the reasoning capabilities of
language models at the expense of higher inference costs. To lower inference
costs, prior works train router models or deferral mechanisms that allocate
easy queries to a small, efficient model, while forwarding harder queries to
larger, more expensive models. However, these trained router models often lack
robustness under domain shifts and require expensive data synthesis techniques
such as Monte Carlo rollouts to obtain sufficient ground-truth routing labels
for training. In this work, we propose Confidence-Guided Stepwise Model Routing
for Cost-Efficient Reasoning (STEER), a domain-agnostic framework that performs
fine-grained, step-level routing between smaller and larger LLMs without
utilizing external models. STEER leverages confidence scores from the smaller
model's logits prior to generating a reasoning step, so that the large model is
invoked only when necessary. Extensive evaluations using different LLMs on a
diverse set of challenging benchmarks across multiple domains such as
Mathematical Reasoning, Multi-Hop QA, and Planning tasks indicate that STEER
achieves competitive or enhanced accuracy while reducing inference costs (up to
+20% accuracy with 48% less FLOPs compared to solely using the larger model on
AIME), outperforming baselines that rely on trained external modules. Our
results establish model-internal confidence as a robust, domain-agnostic signal
for model routing, offering a scalable pathway for efficient LLM deployment.

</details>


### [195] [Explicit Knowledge-Guided In-Context Learning for Early Detection of Alzheimer's Disease](https://arxiv.org/abs/2511.06215)
*Puzhen Su,Yongzhu Miao,Chunxi Guo,Jintao Tang,Shasha Li,Ting Wang*

Main category: cs.CL

TL;DR: EK-ICL是一种新的情境学习框架，通过整合结构化显式知识（小模型置信度得分、解析特征得分、标签词替换）来增强阿尔茨海默病检测中的推理稳定性和任务对齐，在三个AD数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 从叙事转录本中检测阿尔茨海默病(AD)对大型语言模型仍是一个具有挑战性的任务，特别是在分布外(OOD)和数据稀缺条件下。现有情境学习方法在临床领域如AD检测中存在任务识别失败、演示选择次优和标签词与任务目标不匹配等问题。

Method: 提出显式知识情境学习器(EK-ICL)框架，整合三个知识组件：1)来自小语言模型的置信度得分以基于任务相关模式进行预测；2)解析特征得分以捕获结构差异并改进演示选择；3)标签词替换以解决与LLM先验的语义不匹配。同时采用基于解析的检索策略和集成预测来缓解AD转录本的语义同质性影响。

Result: 在三个AD数据集上的广泛实验表明，EK-ICL显著优于最先进的微调和情境学习基线方法。进一步分析揭示AD检测中的情境学习性能高度敏感于标签语义和特定任务上下文的对齐。

Conclusion: 研究表明在低资源临床推理条件下，显式知识的重要性。情境学习在AD检测中的性能与标签语义和任务特定上下文的对齐高度相关，这突显了在临床领域应用LLMs时考虑知识整合和语义对齐的必要性。

Abstract: Detecting Alzheimer's Disease (AD) from narrative transcripts remains a
challenging task for large language models (LLMs), particularly under
out-of-distribution (OOD) and data-scarce conditions. While in-context learning
(ICL) provides a parameter-efficient alternative to fine-tuning, existing ICL
approaches often suffer from task recognition failure, suboptimal demonstration
selection, and misalignment between label words and task objectives, issues
that are amplified in clinical domains like AD detection. We propose Explicit
Knowledge In-Context Learners (EK-ICL), a novel framework that integrates
structured explicit knowledge to enhance reasoning stability and task alignment
in ICL. EK-ICL incorporates three knowledge components: confidence scores
derived from small language models (SLMs) to ground predictions in
task-relevant patterns, parsing feature scores to capture structural
differences and improve demo selection, and label word replacement to resolve
semantic misalignment with LLM priors. In addition, EK-ICL employs a
parsing-based retrieval strategy and ensemble prediction to mitigate the
effects of semantic homogeneity in AD transcripts. Extensive experiments across
three AD datasets demonstrate that EK-ICL significantly outperforms
state-of-the-art fine-tuning and ICL baselines. Further analysis reveals that
ICL performance in AD detection is highly sensitive to the alignment of label
semantics and task-specific context, underscoring the importance of explicit
knowledge in clinical reasoning under low-resource conditions.

</details>


### [196] [SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization](https://arxiv.org/abs/2511.06222)
*Yue Huang,Xiangqi Wang,Xiangliang Zhang*

Main category: cs.CL

TL;DR: 提出了优先对齐(Priority Alignment)范式和SPA框架，通过"可信度优先于有用性"的严格顺序，实现自监督的双重标准对齐，在保证安全性的同时提升模型的有用性。


<details>
  <summary>Details</summary>
Motivation: 在高风险场景(如自残、法律、医疗查询)中，大语言模型需要同时具备可信度和有用性，但这两个目标常常相互冲突，传统的对齐方法难以平衡这一矛盾。

Method: 提出SPA(Self-Priority Alignment)框架，包含三个核心步骤：1)自我生成多样化响应；2)自我评估和精炼；3)双重标准去噪。构建字典顺序偏好对，使用不确定性加权的对齐损失函数进行微调，强调高置信度、大差距的决策。

Result: 在多个基准测试上，SPA在不损害安全性的前提下提升了有用性，超越了强基线模型，同时保持了通用能力。实验证明了该方法的有效性和可扩展性。

Conclusion: SPA提供了一个可扩展且可解释的对齐策略，为关键LLM应用场景下平衡可信度与有用性提供了新的解决方案，具有重要的实践价值。

Abstract: In high-stakes scenarios-such as self-harm, legal, or medical queries-LLMs
must be both trustworthy and helpful. However, these goals often conflict. We
propose priority alignment, a new alignment paradigm that enforces a strict
"trustworthy-before-helpful" ordering: optimization of helpfulness is
conditioned on first meeting trustworthy thresholds (e.g., harmlessness or
honesty). To realize this, we introduce Self-Priority Alignment (SPA)-a fully
unsupervised framework that generates diverse responses, self-evaluates them
and refines them by the model itself, and applies dual-criterion denoising to
remove inconsistency and control variance. From this, SPA constructs
lexicographically ordered preference pairs and fine-tunes the model using an
uncertainty-weighted alignment loss that emphasizes high-confidence, high-gap
decisions. Experiments across multiple benchmarks show that SPA improves
helpfulness without compromising safety, outperforming strong baselines while
preserving general capabilities. Our results demonstrate that SPA provides a
scalable and interpretable alignment strategy for critical LLM applications.

</details>


### [197] [Overview of CHIP 2025 Shared Task 2: Discharge Medication Recommendation for Metabolic Diseases Based on Chinese Electronic Health Records](https://arxiv.org/abs/2511.06230)
*Juntao Li,Haobin Yuan,Ling Luo,Tengxiao Lv,Yan Jiang,Fan Wang,Ping Zhang,Huiyi Lv,Jian Wang,Yuanyuan Sun,Hongfei Lin*

Main category: cs.CL

TL;DR: 本文介绍了CHIP 2025 Shared Task 2竞赛，该竞赛利用中文电子病历数据自动推荐出院药物。研究团队构建了包含5,894条住院记录的CDrugRed数据集，最佳团队使用基于LLM的集成系统达到了0.5102的Jaccard分数和0.6267的F1分数。


<details>
  <summary>Details</summary>
Motivation: 出院药物推荐对确保慢性代谢疾病患者的治疗连续性、预防再入院和改善长期管理至关重要。开发自动化的药物推荐系统能够提高临床效率和治疗质量。

Method: 构建了CDrugRed数据集，包含来自中国3,190名患者的5,894条去标识化住院记录；举办了CHIP 2025 Shared Task 2竞赛，吸引526个团队参与；参赛团队主要采用基于大语言模型(LLM)的集成系统方法。

Result: 167个团队在A阶段提交有效结果，95个团队在B阶段提交；最佳表现团队在最终测试集上达到Jaccard分数0.5102、F1分数0.6267；展示了基于LLM的集成系统在药物推荐任务上的潜力。

Conclusion: 研究表明LLM在中文电子病历的药物推荐方面具有潜力，但仍面临挑战；多标签推荐、异构临床文本和个体化治疗方案是主要难点；竞赛为该领域的发展提供了重要推动作用。

Abstract: Discharge medication recommendation plays a critical role in ensuring
treatment continuity, preventing readmission, and improving long-term
management for patients with chronic metabolic diseases. This paper present an
overview of the CHIP 2025 Shared Task 2 competition, which aimed to develop
state-of-the-art approaches for automatically recommending appro-priate
discharge medications using real-world Chinese EHR data. For this task, we
constructed CDrugRed, a high-quality dataset consisting of 5,894 de-identified
hospitalization records from 3,190 patients in China. This task is challenging
due to multi-label nature of medication recommendation, het-erogeneous clinical
text, and patient-specific variability in treatment plans. A total of 526 teams
registered, with 167 and 95 teams submitting valid results to the Phase A and
Phase B leaderboards, respectively. The top-performing team achieved the
highest overall performance on the final test set, with a Jaccard score of
0.5102, F1 score of 0.6267, demonstrating the potential of advanced large
language model (LLM)-based ensemble systems. These re-sults highlight both the
promise and remaining challenges of applying LLMs to medication recommendation
in Chinese EHRs. The post-evaluation phase remains open at
https://tianchi.aliyun.com/competition/entrance/532411/.

</details>


### [198] [Analyzing and Mitigating Negation Artifacts using Data Augmentation for Improving ELECTRA-Small Model Accuracy](https://arxiv.org/abs/2511.06234)
*Mojtaba Noghabaei*

Main category: cs.CL

TL;DR: 本研究发现ELECTRA-small模型在SNLI数据集上处理否定语句存在困难，通过添加强调否定的对比集和对抗样本进行数据增强，显著提升了模型对否定语句的理解能力。


<details>
  <summary>Details</summary>
Motivation: 预训练的NLI模型在基准测试中往往依赖虚假相关性而非真正理解语言特征，特别是否定这一重要语义现象，这导致模型在处理包含否定的推理任务时表现不佳。

Method: 使用ELECTRA-small模型在SNLI数据集上进行微调，通过添加专门强调否定语义的对比集和对抗样本来增强训练数据，采用有针对性的数据增强策略来解决模型对否定的理解问题。

Result: 实验证明，针对性的数据增强方法有效提高了模型在包含否定样本上的分类准确性，同时保持了整体性能水平，成功缓解了数据集伪影带来的负面影响。

Conclusion: 通过构造包含否定特征的对比集和对抗样本进行数据增强，可以有效解决NLI模型对否定理解的不足问题，这种针对性方法为提高模型的语义理解能力提供了可行途径。

Abstract: Pre-trained models for natural language inference (NLI) often achieve high
performance on benchmark datasets by using spurious correlations, or dataset
artifacts, rather than understanding language touches such as negation. In this
project, we investigate the performance of an ELECTRA-small model fine-tuned on
the Stanford Natural Language Inference (SNLI) dataset, focusing on its
handling of negation. Through analysis, we identify that the model struggles
with correctly classifying examples containing negation. To address this, we
augment the training data with contrast sets and adversarial examples
emphasizing negation. Our results demonstrate that this targeted data
augmentation improves the model's accuracy on negation-containing examples
without adversely affecting overall performance, therefore mitigating the
identified dataset artifact.

</details>


### [199] [TimeSense:Making Large Language Models Proficient in Time-Series Analysis](https://arxiv.org/abs/2511.06344)
*Zhirui Zhang,Changhua Pei,Tianyi Gao,Zhe Xie,Yibo Hao,Zhaoyang Yu,Longlong Xu,Tong Xiao,Jing Han,Dan Pei*

Main category: cs.CL

TL;DR: 该论文提出了TimeSense框架，通过平衡文本推理与时间感知能力，解决了现有方法在时间序列分析中过度依赖文本监督而忽视时间特征的问题，并在EvalTS基准测试上取得了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有结合文本和时间序列数据的方法在训练时过度依赖文本标签进行监督，导致模型偏向文本线索而可能忽略完整的时间特征，这种偏差可能产生与底层时间序列背景相矛盾的输出结果。

Method: 研究者构建了EvalTS基准测试（包含10个任务、3个难度等级），并提出了TimeSense多模态框架。该框架包含时间感知模块来重构输入时间序列，确保文本推理基于时间序列动态，并显式融入基于坐标的位置嵌入以增强时间序列数据的空间理解能力。

Result: 实验结果表明，TimeSense在多个任务上实现了最先进的性能，特别是在复杂多维时间序列推理任务上显著优于现有方法。

Conclusion: TimeSense框架成功解决了文本监督偏差问题，通过平衡文本推理与时间感知，显著提升了LLM在时间序列分析任务中的表现，为复杂时间序列推理任务提供了有效解决方案。

Abstract: In the time-series domain, an increasing number of works combine text with
temporal data to leverage the reasoning capabilities of large language models
(LLMs) for various downstream time-series understanding tasks. This enables a
single model to flexibly perform tasks that previously required specialized
models for each domain. However, these methods typically rely on text labels
for supervision during training, biasing the model toward textual cues while
potentially neglecting the full temporal features. Such a bias can lead to
outputs that contradict the underlying time-series context. To address this
issue, we construct the EvalTS benchmark, comprising 10 tasks across three
difficulty levels, from fundamental temporal pattern recognition to complex
real-world reasoning, to evaluate models under more challenging and realistic
scenarios. We also propose TimeSense, a multimodal framework that makes LLMs
proficient in time-series analysis by balancing textual reasoning with a
preserved temporal sense. TimeSense incorporates a Temporal Sense module that
reconstructs the input time-series within the model's context, ensuring that
textual reasoning is grounded in the time-series dynamics. Moreover, to enhance
spatial understanding of time-series data, we explicitly incorporate
coordinate-based positional embeddings, which provide each time point with
spatial context and enable the model to capture structural dependencies more
effectively. Experimental results demonstrate that TimeSense achieves
state-of-the-art performance across multiple tasks, and it particularly
outperforms existing methods on complex multi-dimensional time-series reasoning
tasks.

</details>


### [200] [HatePrototypes: Interpretable and Transferable Representations for Implicit and Explicit Hate Speech Detection](https://arxiv.org/abs/2511.06391)
*Irina Proskurina,Marc-Antoine Carpentier,Julien Velcin*

Main category: cs.CL

TL;DR: 本文提出HatePrototypes方法，通过类级向量表示实现显式和隐式仇恨言论的高效可迁移检测，无需重复微调。


<details>
  <summary>Details</summary>
Motivation: 现有仇恨内容审核模型需要在新基准上重复微调，但现有基准主要关注显式仇恨，忽略了需要深度语义理解的隐式仇恨（如贬低比较、排斥号召、细微歧视语言等），而这类仇恨同样造成伤害。

Method: 提出HatePrototypes概念，即从优化用于仇恨言论检测的语言模型中提取的类级向量表示。该方法仅需每类50个样本即可构建原型，支持参数自由的提前退出机制。

Result: 研究发现HatePrototypes能够实现显式和隐式仇恨之间的跨任务迁移，原型在不同基准间可互换使用，且参数自由的提前退出对两种仇恨类型都有效。

Conclusion: 该研究为仇恨言论检测提供了高效且可迁移的解决方案，减少了重复微调的需求，同时有效处理显式和隐式仇恨，推动该领域研究发展。

Abstract: Optimization of offensive content moderation models for different types of
hateful messages is typically achieved through continued pre-training or
fine-tuning on new hate speech benchmarks. However, existing benchmarks mainly
address explicit hate toward protected groups and often overlook implicit or
indirect hate, such as demeaning comparisons, calls for exclusion or violence,
and subtle discriminatory language that still causes harm. While explicit hate
can often be captured through surface features, implicit hate requires deeper,
full-model semantic processing. In this work, we question the need for repeated
fine-tuning and analyze the role of HatePrototypes, class-level vector
representations derived from language models optimized for hate speech
detection and safety moderation. We find that these prototypes, built from as
few as 50 examples per class, enable cross-task transfer between explicit and
implicit hate, with interchangeable prototypes across benchmarks. Moreover, we
show that parameter-free early exiting with prototypes is effective for both
hate types. We release the code, prototype resources, and evaluation scripts to
support future research on efficient and transferable hate speech detection.

</details>


### [201] [SugarTextNet: A Transformer-Based Framework for Detecting Sugar Dating-Related Content on Social Media with Context-Aware Focal Loss](https://arxiv.org/abs/2511.06402)
*Lionel Z. Wang,Shihan Ben,Yulu Huang,Simeng Qing*

Main category: cs.CL

TL;DR: SugarTextNet是一种基于transformer的框架，通过结合注意力提示提取器和上下文短语编码器，以及创新的上下文感知焦点损失函数，有效检测社交媒体中的糖约会相关内容。


<details>
  <summary>Details</summary>
Motivation: 糖约会相关内容在主流社交媒体平台快速扩散，引发了亲密关系商业化和交易关系正常化等严重社会和监管关切。由于存在微妙的委婉语、模糊的语言线索和现实世界数据中的极端类别不平衡，检测此类内容极具挑战性。

Method: 提出SugarTextNet框架，集成预训练transformer编码器、基于注意力的提示提取器和上下文短语编码器来捕捉用户生成文本中的显著和细微特征。引入上下文感知焦点损失函数，结合焦点损失缩放和上下文加权来解决类别不平衡问题并提升少数类检测能力。

Result: 在包含3,067条中文微博帖子的人工标注新数据集上评估，SugarTextNet在多个指标上显著优于传统机器学习模型、深度学习基线和大语言模型。全面的消融研究证实了各组件的不可或缺作用。

Conclusion: 研究结果突出了领域特定、上下文感知建模在敏感内容检测中的重要性，为复杂真实场景中的内容审核提供了强大解决方案，强调了专门化方法在处理社交媒体敏感内容方面的价值。

Abstract: Sugar dating-related content has rapidly proliferated on mainstream social
media platforms, giving rise to serious societal and regulatory concerns,
including commercialization of intimate relationships and the normalization of
transactional relationships.~Detecting such content is highly challenging due
to the prevalence of subtle euphemisms, ambiguous linguistic cues, and extreme
class imbalance in real-world data.~In this work, we present SugarTextNet, a
novel transformer-based framework specifically designed to identify sugar
dating-related posts on social media.~SugarTextNet integrates a pretrained
transformer encoder, an attention-based cue extractor, and a contextual phrase
encoder to capture both salient and nuanced features in user-generated text.~To
address class imbalance and enhance minority-class detection, we introduce
Context-Aware Focal Loss, a tailored loss function that combines focal loss
scaling with contextual weighting.~We evaluate SugarTextNet on a newly curated,
manually annotated dataset of 3,067 Chinese social media posts from Sina Weibo,
demonstrating that our approach substantially outperforms traditional machine
learning models, deep learning baselines, and large language models across
multiple metrics.~Comprehensive ablation studies confirm the indispensable role
of each component.~Our findings highlight the importance of domain-specific,
context-aware modeling for sensitive content detection, and provide a robust
solution for content moderation in complex, real-world scenarios.

</details>


### [202] [How Well Do LLMs Understand Drug Mechanisms? A Knowledge + Reasoning Evaluation Dataset](https://arxiv.org/abs/2511.06418)
*Sunil Mohan,Theofanis Karaletsos*

Main category: cs.CL

TL;DR: 该研究评估了大语言模型在药物作用机制知识记忆和推理能力方面的表现，发现o4-mini模型表现最佳，而小型Qwen3-4B-thinking模型在某些情况下甚至超越了它。研究还揭示了开放世界设置和反事实推理链内部链接的挑战性。


<details>
  <summary>Details</summary>
Motivation: 随着预训练大语言模型在药物开发/重新利用和个性化医学领域的兴趣日益增长，需要评估模型在药物作用机制方面的实际知识和深层理解能力，以确保它们能在新情境下准确回忆和推理相关知识。

Method: 研究者构建了一个评估数据集，用于测试LLMs在已知药物作用机制的事实知识以及在反事实情境下的推理能力。该数据集包含开放世界（需回忆相关知识）和封闭世界（提供事实知识）两种设置，并对比分析了多种模型的性能差异。

Result: 实验结果显示o4-mini模型整体表现优于OpenAI的4o、o3和o3-mini模型，而Qwen3-4B-thinking小型模型在部分测试中甚至超越了o4-mini。研究发现开放世界推理比封闭世界更具挑战性，且影响推理链内部链接的反事实问题比影响药物链接的反事实问题更难解决。

Conclusion: 该研究为评估LLMs在生物医药领域的推理能力提供了新的评估框架，证实了开放世界设置和复杂反事实推理的挑战性，为未来开发更强大的生物医药专用语言模型指明了方向。

Abstract: Two scientific fields showing increasing interest in pre-trained large
language models (LLMs) are drug development / repurposing, and personalized
medicine. For both, LLMs have to demonstrate factual knowledge as well as a
deep understanding of drug mechanisms, so they can recall and reason about
relevant knowledge in novel situations. Drug mechanisms of action are described
as a series of interactions between biomedical entities, which interlink into
one or more chains directed from the drug to the targeted disease. Composing
the effects of the interactions in a candidate chain leads to an inference
about whether the drug might be useful or not for that disease. We introduce a
dataset that evaluates LLMs on both factual knowledge of known mechanisms, and
their ability to reason about them under novel situations, presented as
counterfactuals that the models are unlikely to have seen during training.
Using this dataset, we show that o4-mini outperforms the 4o, o3, and o3-mini
models from OpenAI, and the recent small Qwen3-4B-thinking model closely
matches o4-mini's performance, even outperforming it in some cases. We
demonstrate that the open world setting for reasoning tasks, which requires the
model to recall relevant knowledge, is more challenging than the closed world
setting where the needed factual knowledge is provided. We also show that
counterfactuals affecting internal links in the reasoning chain present a much
harder task than those affecting a link from the drug mentioned in the prompt.

</details>


### [203] [Dutch Metaphor Extraction from Cancer Patients' Interviews and Forum Data using LLMs and Human in the Loop](https://arxiv.org/abs/2511.06427)
*Lifeng Han,David Lindevelt,Sander Puts,Erik van Mulligen,Suzan Verberne*

Main category: cs.CL

TL;DR: 本文利用大型语言模型从荷兰癌症患者的访谈和论坛数据中提取隐喻语言，构建了一个名为HealthQuote.NL的语料库，旨在改善医患沟通和患者护理。


<details>
  <summary>Details</summary>
Motivation: 隐喻在医疗沟通中至关重要，理解患者使用的隐喻有助于改善医患沟通、共同决策和患者健康素养，从而提升护理质量。

Method: 研究者利用大型语言模型（LLMs），通过思维链、少样本学习和自我提示等不同策略，从荷兰癌症患者的访谈和在线论坛数据中提取隐喻。他们采用人在回路的方式验证提取结果，并将其构建成一个名为HealthQuote.NL的语料库。

Result: 研究成功构建了一个名为HealthQuote.NL的荷兰语医疗隐喻语料库。此外，他们验证了不同提示策略下LLMs在该任务上的表现，并公开了相关的提示和资源。

Conclusion: 研究表明，结合先进的LLM和人在回路的方法可以有效提取患者语言中的隐喻。构建的HealthQuote.NL语料库为改善患者护理、支持个性化医疗设计提供了宝贵资源。

Abstract: Metaphors and metaphorical language (MLs) play an important role in
healthcare communication between clinicians, patients, and patients' family
members. In this work, we focus on Dutch language data from cancer patients. We
extract metaphors used by patients using two data sources: (1) cancer patient
storytelling interview data and (2) online forum data, including patients'
posts, comments, and questions to professionals. We investigate how current
state-of-the-art large language models (LLMs) perform on this task by exploring
different prompting strategies such as chain of thought reasoning, few-shot
learning, and self-prompting. With a human-in-the-loop setup, we verify the
extracted metaphors and compile the outputs into a corpus named HealthQuote.NL.
We believe the extracted metaphors can support better patient care, for example
shared decision making, improved communication between patients and clinicians,
and enhanced patient health literacy. They can also inform the design of
personalized care pathways. We share prompts and related resources at
https://github.com/aaronlifenghan/HealthQuote.NL

</details>


### [204] [SR-KI: Scalable and Real-Time Knowledge Integration into LLMs via Supervised Attention](https://arxiv.org/abs/2511.06446)
*Bohan Yu,Wei Huang,Kang Liu*

Main category: cs.CL

TL;DR: SR-KI是一种将大规模结构化知识库实时集成到大型语言模型中的新方法，通过在模型潜在空间内进行检索，实现端到端推理和高效知识压缩。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成方法依赖外部检索器和多阶段管道，存在性能瓶颈和效率问题。需要一种能够在LLM内部直接进行知识检索和整合的解决方案，以提高效率并支持动态知识更新。

Method: SR-KI首先将知识库编码为键值对并注入LLM的KV缓存，然后采用两阶段训练：定位专用检索层，再通过注意力损失监督模型关注相关知识条目。整个检索过程完全在模型潜在空间内进行，支持端到端推理。

Result: 在单A100 40GB GPU上成功将40K知识库集成到7B LLM中，最佳任务Recall@10超过98%，平均超过88%，同时实现高达99.75%的知识库压缩，在问答和知识库ID生成任务上保持强大性能。

Conclusion: SR-KI通过在模型内部进行知识检索和整合，突破了传统方法的局限，实现了高效的大规模知识集成、动态更新和高压缩率，为LLM的知识增强提供了新的技术路径。

Abstract: This paper proposes SR-KI, a novel approach for integrating real-time and
large-scale structured knowledge bases (KBs) into large language models (LLMs).
SR-KI begins by encoding KBs into key-value pairs using a pretrained encoder,
and injects them into LLMs' KV cache. Building on this representation, we
employ a two-stage training paradigm: first locating a dedicated retrieval
layer within the LLM, and then applying an attention-based loss at this layer
to explicitly supervise attention toward relevant KB entries. Unlike
traditional retrieval-augmented generation methods that rely heavily on the
performance of external retrievers and multi-stage pipelines, SR-KI supports
end-to-end inference by performing retrieval entirely within the models latent
space. This design enables efficient compression of injected knowledge and
facilitates dynamic knowledge updates. Comprehensive experiments demonstrate
that SR-KI enables the integration of up to 40K KBs into a 7B LLM on a single
A100 40GB GPU, and achieves strong retrieval performance, maintaining over 98%
Recall@10 on the best-performing task and exceeding 88% on average across all
tasks. Task performance on question answering and KB ID generation also
demonstrates that SR-KI maintains strong performance while achieving up to
99.75% compression of the injected KBs.

</details>


### [205] [Rethinking what Matters: Effective and Robust Multilingual Realignment for Low-Resource Languages](https://arxiv.org/abs/2511.06497)
*Quang Phuoc Nguyen,David Anugraha,Felix Gaschi,Jun Bin Cheng,En-Shiun Annie Lee*

Main category: cs.CL

TL;DR: 重校准对提升跨语言迁移有效，特别是对低资源语言；精选的语言子集比使用所有语言效果更好，且能减少数据收集开销。


<details>
  <summary>Details</summary>
Motivation: 现有重校准方法对类型学差异大或低资源语言效果不稳定，且依赖高质量平行数据，本研究旨在探索是否需要所有语言或精选子集即可。

Method: 通过大规模实验，比较使用全部语言与精选语言多样性子集的重校准效果，重点评估对低资源语言的影响。

Result: 重校准对低资源语言特别有效，精选语言多样性子集能匹配甚至超越全语言重校准，尤其在未见过的低资源语言上表现更好。

Conclusion: 有效重校准无需全面语言覆盖，通过合理语言选择可减少数据收集开销，同时保持高效性和鲁棒性。

Abstract: Realignment is a promising strategy to improve cross-lingual transfer in
multilingual language models. However, empirical results are mixed and often
unreliable, particularly for typologically distant or low-resource languages
(LRLs) compared to English. Moreover, word realignment tools often rely on
high-quality parallel data, which can be scarce or noisy for many LRLs. In this
work, we conduct an extensive empirical study to investigate whether
realignment truly benefits from using all available languages, or if
strategically selected subsets can offer comparable or even improved
cross-lingual transfer, and study the impact on LRLs. Our controlled
experiments show that realignment can be particularly effective for LRLs and
that using carefully selected, linguistically diverse subsets can match full
multilingual alignment, and even outperform it for unseen LRLs. This indicates
that effective realignment does not require exhaustive language coverage and
can reduce data collection overhead, while remaining both efficient and robust
when guided by informed language selection.

</details>


### [206] [You Had One Job: Per-Task Quantization Using LLMs' Hidden Representations](https://arxiv.org/abs/2511.06516)
*Amit LeVi,Raz Lapid,Rom Himelstein,Yaniv Nemcovsky,Ravid Shwartz Ziv,Avi Mendelson*

Main category: cs.CL

TL;DR: 该论文提出了两种任务感知的PTQ方法TAQ和TAQO，通过识别任务相关层并保持其精度，同时在其他层进行激进量化，在保持接近原始精度的同时显著提升量化效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在很多应用中只需要有限能力，但现有大模型在内存和延迟方面效率低下。现有PTQ方法大多忽略任务特定信号在各层的分布，导致量化效果不佳。

Method: 利用编码任务显著特征的隐藏表示作为量化指导。提出TAQ(基于任务条件统计分配位宽)和TAQO(基于直接层敏感性测试分配精度)两种方法，从小规模校准集中识别任务相关层，保持这些层的精度而激进量化其他层。

Result: TAQ和TAQO在多个模型上都优于基线方法。TAQ在Phi-4上领先，TAQO在Llama-3.1、Qwen3和Qwen2.5上领先。例如在Phi-4上达到42.33 EM/50.81 F1，远超AWQ的2.25/7.07，同时在较低平均精度下保持在原精度的1.0%以内。

Conclusion: 任务感知的量化方法能有效识别和保护任务相关层，实现更高效的模型量化，为实际应用中的资源受限部署提供了有效解决方案。

Abstract: Large Language Models (LLMs) excel across diverse tasks, yet many
applications require only limited capabilities, making large variants
inefficient in memory and latency. Existing approaches often combine
distillation and quantization, but most post-training quantization (PTQ)
methods are task-agnostic, ignoring how task-specific signals are distributed
across layers. In this work, we propose to use hidden representations that
encode task-salient signals as a guideline for quantization. In order to fully
utilize our innovative idea, this paper compares two new task-aware PTQ
methods: Task-Aware Quantization (TAQ), which allocates bitwidths using
task-conditioned statistics from hidden activations, and TAQO, which allocates
precision based on direct layer sensitivity tests. From a small calibration
set, these approaches identify task-relevant layers, preserving their precision
while aggressively quantizing the rest. This yields stable task sensitivity
profiles and efficient task-specialized models. Across models, TAQ and TAQO
outperform the baselines; TAQ leads on Phi-4, while TAQO leads on Llama-3.1,
Qwen3, and Qwen2.5. For instances, on Phi-4 it achieves 42.33 EM / 50.81 F1,
far surpassing Activation-aware Weight Quantization (AWQ) (2.25 / 7.07), while
remaining within < 1.0% of the original accuracy at lower average precision.

</details>


### [207] [Better Datasets Start From RefineLab: Automatic Optimization for High-Quality Dataset Refinement](https://arxiv.org/abs/2511.06530)
*Xiaonan Luo,Yue Huang,Ping He,Xiangliang Zhang*

Main category: cs.CL

TL;DR: RefineLab是一个基于LLM的框架，能够自动在可控制的token预算约束下，将原始QA数据细化为高质量数据集，通过选择性地应用细化操作来最大化数据集质量。


<details>
  <summary>Details</summary>
Motivation: 现有高质量QA数据集存在领域覆盖不全、难度分布不匹配和事实不一致等问题，而生成模型产生的数据集进一步加剧了质量挑战，需要一种自动化的数据集细化方法。

Method: RefineLab将数据集细化为约束优化问题，在预定义token预算内，通过分配模块选择最优细化策略（如重述、干扰项替换），最大化整体数据集质量。

Result: 实验表明RefineLab在覆盖度、难度对齐、事实保真度和干扰项质量方面都能有效缩小与专家数据集的差异。

Conclusion: RefineLab为LLM评估提供了可扩展、可定制的数据集设计路径，对大规模语言模型评估具有广泛影响。

Abstract: High-quality Question-Answer (QA) datasets are foundational for reliable
Large Language Model (LLM) evaluation, yet even expert-crafted datasets exhibit
persistent gaps in domain coverage, misaligned difficulty distributions, and
factual inconsistencies. The recent surge in generative model-powered datasets
has compounded these quality challenges. In this work, we introduce RefineLab,
the first LLM-driven framework that automatically refines raw QA textual data
into high-quality datasets under a controllable token-budget constraint.
RefineLab takes a set of target quality attributes (such as coverage and
difficulty balance) as refinement objectives, and performs selective edits
within a predefined token budget to ensure practicality and efficiency. In
essence, RefineLab addresses a constrained optimization problem: improving the
quality of QA samples as much as possible while respecting resource
limitations. With a set of available refinement operations (e.g., rephrasing,
distractor replacement), RefineLab takes as input the original dataset, a
specified set of target quality dimensions, and a token budget, and determines
which refinement operations should be applied to each QA sample. This process
is guided by an assignment module that selects optimal refinement strategies to
maximize overall dataset quality while adhering to the budget constraint.
Experiments demonstrate that RefineLab consistently narrows divergence from
expert datasets across coverage, difficulty alignment, factual fidelity, and
distractor quality. RefineLab pioneers a scalable, customizable path to
reproducible dataset design, with broad implications for LLM evaluation.

</details>


### [208] [Ibom NLP: A Step Toward Inclusive Natural Language Processing for Nigeria's Minority Languages](https://arxiv.org/abs/2511.06531)
*Oluwadara Kalejaiye,Luel Hagos Beyene,David Ifeoluwa Adelani,Mmekut-Mfon Gabriel Edet,Aniefon Daniel Akpan,Eno-Abasi Urua,Anietie Andy*

Main category: cs.CL

TL;DR: 该研究介绍了ibom数据集，涵盖四种尼日利亚沿海语言，填补了NLP研究的空白；实验发现现有LLM在机器翻译上表现不佳，但少样本学习能改善主题分类效果。


<details>
  <summary>Details</summary>
Motivation: 尼日利亚语言多样性极高但NLP研究仅集中在不到1%的语言上，主要是由于缺乏文本数据资源。四种来自Akwa Ibom State的沿海语言完全缺失于主流翻译系统和基准测试中。

Method: 构建ibom数据集，包含Anaang、Efik、Ibibio和Oro四种语言的机器翻译和主题分类数据；通过扩展Flores-200基准并基于SIB-200数据集进行主题标签对齐来创建数据。

Result: 评估显示当前大型语言模型在零样本和少样本设置下对这些语言的机器翻译效果都较差；但少样本样本能够稳定地提高主题分类性能。

Conclusion: 该研究为尼日利亚沿海语言提供了宝贵的NLP资源，揭示了现有模型在低资源语言翻译上的局限性，同时证明了少样本学习在主题分类任务中的有效性。

Abstract: Nigeria is the most populous country in Africa with a population of more than
200 million people. More than 500 languages are spoken in Nigeria and it is one
of the most linguistically diverse countries in the world. Despite this,
natural language processing (NLP) research has mostly focused on the following
four languages: Hausa, Igbo, Nigerian-Pidgin, and Yoruba (i.e <1% of the
languages spoken in Nigeria). This is in part due to the unavailability of
textual data in these languages to train and apply NLP algorithms. In this
work, we introduce ibom -- a dataset for machine translation and topic
classification in four Coastal Nigerian languages from the Akwa Ibom State
region: Anaang, Efik, Ibibio, and Oro. These languages are not represented in
Google Translate or in major benchmarks such as Flores-200 or SIB-200. We focus
on extending Flores-200 benchmark to these languages, and further align the
translated texts with topic labels based on SIB-200 classification dataset. Our
evaluation shows that current LLMs perform poorly on machine translation for
these languages in both zero-and-few shot settings. However, we find the
few-shot samples to steadily improve topic classification with more shots.

</details>


### [209] [Rep2Text: Decoding Full Text from a Single LLM Token Representation](https://arxiv.org/abs/2511.06571)
*Haiyan Zhao,Zirui He,Fan Yang,Ali Payani,Mengnan Du*

Main category: cs.CL

TL;DR: 本研究提出了Rep2Text框架，能够从大语言模型的单个last-token表示中解码出完整文本，实验证明可恢复超过一半的序列信息。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的内部机制仍然很不透明，本文试图解决一个基本问题：从LLM的单个last-token表示中能在多大程度上恢复原始输入文本。

Method: 提出Rep2Text框架，使用可训练适配器将目标模型的内部表示投影到解码语言模型的嵌入空间，然后通过自回归方式重构输入文本。

Result: 在多种模型组合实验中，平均可以从16-token序列中恢复超过一半的信息，同时保持强语义完整性和连贯性。发现信息瓶颈效应：更长序列在token级别恢复减少但语义完整性保持良好。

Conclusion: 研究揭示了LLM内部表示的信息保留能力，框架对分布外医疗数据表现出鲁棒性，为理解LLM机制提供了新视角。

Abstract: Large language models (LLMs) have achieved remarkable progress across diverse
tasks, yet their internal mechanisms remain largely opaque. In this work, we
address a fundamental question: to what extent can the original input text be
recovered from a single last-token representation within an LLM? We propose
Rep2Text, a novel framework for decoding full text from last-token
representations. Rep2Text employs a trainable adapter that projects a target
model's internal representations into the embedding space of a decoding
language model, which then autoregressively reconstructs the input text.
Experiments on various model combinations (Llama-3.1-8B, Gemma-7B,
Mistral-7B-v0.1, Llama-3.2-3B) demonstrate that, on average, over half of the
information in 16-token sequences can be recovered from this compressed
representation while maintaining strong semantic integrity and coherence.
Furthermore, our analysis reveals an information bottleneck effect: longer
sequences exhibit decreased token-level recovery while preserving strong
semantic integrity. Besides, our framework also demonstrates robust
generalization to out-of-distribution medical data.

</details>


### [210] [TabRAG: Tabular Document Retrieval via Structured Language Representations](https://arxiv.org/abs/2511.06582)
*Jacob Si,Mike Qu,Michelle Lee,Yingzhen Li*

Main category: cs.CL

TL;DR: TabRAG是一种基于解析的RAG流水线，通过结构化语言表示有效处理包含大量表格的文档，在生成和检索任务中优于现有的流行解析方法。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG数据摄取方法存在两难：直接微调嵌入模型准确但计算硬件要求高；解析文档进行嵌入模型编码在处理表格数据时性能不佳。本研究聚焦解决后者的问题。

Method: 提出TabRAG，一种基于解析的RAG流水线，采用结构化语言表示方法来专门处理表格密集型文档，避免了直接微调模型的高硬件需求，同时提升了表格数据处理的性能。

Result: TabRAG在生成和检索任务上的表现优于现有的流行解析方法，证明了其在处理表格密集文档方面的有效性。

Conclusion: TabRAG为处理表格密集型文档提供了一个高效且实用的解决方案，在保持较低硬件要求的同时显著提升了表格数据的RAG性能，为实际应用提供了有价值的技术选择。

Abstract: Ingesting data for Retrieval-Augmented Generation (RAG) involves either
fine-tuning the embedding model directly on the target corpus or parsing
documents for embedding model encoding. The former, while accurate, incurs high
computational hardware requirements, while the latter suffers from suboptimal
performance when extracting tabular data. In this work, we address the latter
by presenting TabRAG, a parsing-based RAG pipeline designed to tackle
table-heavy documents via structured language representations. TabRAG
outperforms existing popular parsing-based methods for generation and
retrieval. Code is available at https://github.com/jacobyhsi/TabRAG.

</details>


### [211] [MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making](https://arxiv.org/abs/2511.06592)
*Zhi Rui Tam,Yun-Nung Chen*

Main category: cs.CL

TL;DR: 研究发现，在临床环境中，从文本转向音频交互的大语言模型存在严重的模态偏见，音频输入会导致手术建议产生高达35%的差异，模型会根据患者的声音特征而非医疗证据做出临床决策，存在加剧医疗不平等的风险。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型从基于文本的界面转向临床环境中的音频交互，可能会通过音频中的副语言线索引入新的漏洞。这种转变需要评估其对临床决策公平性和准确性的影响，特别是在医疗敏感领域。

Method: 研究人员在170个临床案例上评估了这些模型，每个案例被合成为36个不同声音特征的语音版本，涵盖了年龄、性别和情感的变化。比较了音频输入与相同文本输入在手术建议上的差异，并测试了思维链提示和显式推理对减少偏见的效果。

Result: 发现严重的模态偏见：音频输入的手术建议比相同文本输入差异高达35%，一个模型减少了80%的建议；年轻和老年声音之间存在高达12%的年龄差异；尽管使用思维链提示，大多数模型仍存在这些偏见；显式推理成功消除了性别偏见；由于识别性能差，未能检测到情感的影响。

Conclusion: 音频大语言模型容易根据患者的声音特征而非医疗证据做出临床决策，这种缺陷有延续医疗不平等的风险。在这些模型临床部署之前，急需具有偏见感知能力的架构。

Abstract: As large language models transition from text-based interfaces to audio
interactions in clinical settings, they might introduce new vulnerabilities
through paralinguistic cues in audio. We evaluated these models on 170 clinical
cases, each synthesized into speech from 36 distinct voice profiles spanning
variations in age, gender, and emotion. Our findings reveal a severe modality
bias: surgical recommendations for audio inputs varied by as much as 35%
compared to identical text-based inputs, with one model providing 80% fewer
recommendations. Further analysis uncovered age disparities of up to 12%
between young and elderly voices, which persisted in most models despite
chain-of-thought prompting. While explicit reasoning successfully eliminated
gender bias, the impact of emotion was not detected due to poor recognition
performance. These results demonstrate that audio LLMs are susceptible to
making clinical decisions based on a patient's voice characteristics rather
than medical evidence, a flaw that risks perpetuating healthcare disparities.
We conclude that bias-aware architectures are essential and urgently needed
before the clinical deployment of these models.

</details>


### [212] [Duality-based Mode Operations and Pyramid Multilayer Mapping for Rhetorical Modes](https://arxiv.org/abs/2511.06601)
*Zi-Niu Wu*

Main category: cs.CL

TL;DR: 本文提出了基于对偶性的修辞模式操作和金字塔多层映射框架，将静态的修辞分类学转化为动态可测量的系统，为AI的多层修辞推理奠定基础。


<details>
  <summary>Details</summary>
Motivation: 修辞模式在学术和非学术写作中都很重要，但在语言研究、计算建模等领域之间存在概念鸿沟。建立这些领域之间的概念桥梁可以让各方相互受益。

Method: 提出了基于对偶性的模式操作（分裂-联合、前向-后向、扩展-缩减和正交对偶）来扩展修辞模式集合；构建金字塔多层映射框架（从修辞模型层到认知层再到认识层）来降低认知复杂性；使用二项式组合和香农熵分析量化表达多样性和复杂性降低程度。

Result: 识别出边际修辞位（MRB），定义了可扩展的修辞参数来测量表达增长速度；层次化选择比扁平选择显著降低了选择不确定性；将静态的修辞分类学转化为动态可测量的话语设计系统。

Conclusion: 该工作为未来AI系统不仅在语言标记上操作，而是在分层修辞推理结构上操作提供了可能路径，架起了语言学、教育学、学术和计算研究之间的桥梁。

Abstract: Rhetorical modes are useful in both academic and non-academic writing, and
can be subjects to be studied within linguistic research and computational
modeling. Establishing a conceptual bridge among these domains could enable
each to benefit from the others. This paper proposes duality-based mode
operations (split-unite, forward-backward, expansion-reduction and orthogonal
dualities) to expand the set of rhetorical modes, introducing generated modes
like combination and generalization, thereby enhancing epistemic diversity
across multiple applications. It further presents a pyramid multilayer mapping
framework (e.g., three layers from the rhetorical model layer, to cognitive
layer, and to epistemic layers) that reduces the resulting cognitive
complexity. The degrees of expressive diversity and complexity reduction are
quantified through binomial combinatorics and Shannon entropy analysis. A
Marginal Rhetorical Bit (MRB) is identified, permitting the definition of a
rhetorical-scalable parameter that measures expressive growth speed in bits per
stage. A direct entropy measure shows that hierarchical selection over smaller
subsets markedly reduces choice uncertainty compared with flat selection across
all modes. These considerations appear to transform static and non-measurable
rhetorical taxonomies into more dynamic and more measurable systems for
discourse design. From this work, it would be possible to identify a pathway
for future AI systems to operate not only on language tokens but on layered
rhetorical reasoning structures, bridging linguistic, pedagogical, academic,
and computational research

</details>


### [213] [Steering LLMs toward Korean Local Speech: Iterative Refinement Framework for Faithful Dialect Translation](https://arxiv.org/abs/2511.06680)
*Keunhyeung Park,Seunguk Yu,Youngbin Kim*

Main category: cs.CL

TL;DR: 本文提出DIA-REFINE框架，通过翻译-验证-反馈的迭代循环引导大语言模型进行标准语到方言的翻译，并引入新的评估指标DFS和TDR来解决传统n-gram指标的局限性，在韩国方言翻译实验中验证了框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 标准语到方言的机器翻译面临两大挑战：1) 大语言模型存在持续的方言鸿沟；2) 传统n-gram评估指标倾向于源语复制而非真实的方言翻译，导致评估失真。需要新的框架和评估方法来实现忠实、准确的方言翻译。

Method: 提出DIA-REFINE框架，包含三个核心组件：1) 使用外部方言分类器的迭代翻译-验证-反馈循环；2) 方言保真度分数(DFS)量化语言转换；3) 目标方言比例(TDR)衡量方言翻译成功度。在韩国方言上进行零样本和上下文学习实验。

Result: 实验表明DIA-REFINE显著提升方言保真度；新指标成功区分"虚假成功"(高n-gram分但方言翻译失败)和"真实尝试"(真实方言尝试但n-gram分低)；模型响应程度存在差异；上下文示例集成进一步改善方言表达翻译。

Conclusion: 建立了定向、包容的方言翻译鲁棒框架，提供严格的评估体系，为模型性能提供关键洞察，推动方言机器翻译领域发展，解决了传统方法的局限性。

Abstract: Standard-to-dialect machine translation remains challenging due to a
persistent dialect gap in large language models and evaluation distortions
inherent in n-gram metrics, which favor source copying over authentic dialect
translation. In this paper, we propose the dialect refinement (DIA-REFINE)
framework, which guides LLMs toward faithful target dialect outputs through an
iterative loop of translation, verification, and feedback using external
dialect classifiers. To address the limitations of n-gram-based metrics, we
introduce the dialect fidelity score (DFS) to quantify linguistic shift and the
target dialect ratio (TDR) to measure the success of dialect translation.
Experiments on Korean dialects across zero-shot and in-context learning
baselines demonstrate that DIA-REFINE consistently enhances dialect fidelity.
The proposed metrics distinguish between False Success cases, where high n-gram
scores obscure failures in dialectal translation, and True Attempt cases, where
genuine attempts at dialectal translation yield low n-gram scores. We also
observed that models exhibit varying degrees of responsiveness to the
framework, and that integrating in-context examples further improves the
translation of dialectal expressions. Our work establishes a robust framework
for goal-directed, inclusive dialect translation, providing both rigorous
evaluation and critical insights into model performance.

</details>


### [214] [Sentiment Analysis On YouTube Comments Using Machine Learning Techniques Based On Video Games Content](https://arxiv.org/abs/2511.06708)
*Adi Danish Bin Muhammad Amin,Mohaiminul Islam Bhuiyan,Nur Shazwani Kamarudin,Zulfahmi Toh,Nur Syafiqah Nafis*

Main category: cs.CL

TL;DR: 本研究基于YouTube评论对电子游戏进行情感分析，发现SVM算法在分类准确性上表现最佳，为游戏开发者提供有价值的用户反馈。


<details>
  <summary>Details</summary>
Motivation: 游戏行业快速发展和游戏社区的不断壮大，需要深入了解用户情感，特别是通过YouTube等热门社交媒体平台表达的情感，为游戏改进提供依据。

Method: 使用YouTube API收集与各种电子游戏相关的评论，采用TextBlob情感分析工具预处理数据，并运用机器学习算法（朴素贝叶斯、逻辑回归和支持向量机）进行分类分析。

Result: SVM在不同数据集上展现出最优性能，获得最高的分类准确率。分析涵盖了多个热门游戏视频，揭示了用户偏好和批评的趋势与见解。

Conclusion: 先进的情感分析技术对捕捉用户评论中表达的情感细微差别至关重要，为游戏开发者提供了宝贵反馈，以改进游戏设计和用户体验。未来研究将集成更复杂的自然语言处理技术并探索更多数据源。

Abstract: The rapid evolution of the gaming industry, driven by technological
advancements and a burgeoning community, necessitates a deeper understanding of
user sentiments, especially as expressed on popular social media platforms like
YouTube. This study presents a sentiment analysis on video games based on
YouTube comments, aiming to understand user sentiments within the gaming
community. Utilizing YouTube API, comments related to various video games were
collected and analyzed using the TextBlob sentiment analysis tool. The
pre-processed data underwent classification using machine learning algorithms,
including Na\"ive Bayes, Logistic Regression, and Support Vector Machine (SVM).
Among these, SVM demonstrated superior performance, achieving the highest
classification accuracy across different datasets. The analysis spanned
multiple popular gaming videos, revealing trends and insights into user
preferences and critiques. The findings underscore the importance of advanced
sentiment analysis in capturing the nuanced emotions expressed in user
comments, providing valuable feedback for game developers to enhance game
design and user experience. Future research will focus on integrating more
sophisticated natural language processing techniques and exploring additional
data sources to further refine sentiment analysis in the gaming domain.

</details>


### [215] [Sensitivity of Small Language Models to Fine-tuning Data Contamination](https://arxiv.org/abs/2511.06763)
*Nicy Scaria,Silvester John Joseph Kennedy,Deepak Subramani*

Main category: cs.CL

TL;DR: 该研究系统调查了23个小语言模型在指令调优期间对数据污染的行为鲁棒性，发现模型对句法转换（如字符反转）极其脆弱，而语义转换表现出不同的抵抗模式，同时发现更大能力模型更容易学习语义污染的"能力诅咒"现象。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在资源受限环境中部署日益增多，但它们在指令调优过程中对数据污染的行为鲁棒性仍缺乏深入理解，需要系统性研究来填补这一知识空白。

Method: 研究测试了23个SLMs（270M至40亿参数），通过测量在指令调优期间对两种转换类型的敏感性：句法转换（字符和单词反转）和语义转换（无关和反事实响应），每种转换分别以25%、50%、75%和100%的污染水平应用。

Result: 研究发现基本的不对称性：句法转换导致灾难性性能下降，字符反转在所有模型中产生近乎完全的失败；语义转换表现出明显的阈值行为和核心语言能力的更强韧性；发现"能力诅咒"现象，即更大能力模型更容易学习语义污染；对齐提供不一致的鲁棒性好处，有时甚至降低韧性。

Conclusion: 该研究为SLMs的污染鲁棒性评估建立了三个核心贡献：实证证据显示SLMs对句法模式污染的不成比例脆弱性、句法和语义转换间不对称敏感性模式的识别、以及污染鲁棒性评估的系统化协议。研究结果表明当前鲁棒性假设可能不适用于小模型，强调需要污染感知的训练协议。

Abstract: Small Language Models (SLMs) are increasingly being deployed in
resource-constrained environments, yet their behavioral robustness to data
contamination during instruction tuning remains poorly understood. We
systematically investigate the contamination sensitivity of 23 SLMs (270M to 4B
parameters) across multiple model families by measuring susceptibility to
syntactic and semantic transformation types during instruction tuning:
syntactic transformations (character and word reversal) and semantic
transformations (irrelevant and counterfactual responses), each applied at
contamination levels of 25\%, 50\%, 75\%, and 100\%. Our results reveal
fundamental asymmetries in vulnerability patterns: syntactic transformations
cause catastrophic performance degradation, with character reversal producing
near-complete failure across all models regardless of size or family, while
semantic transformations demonstrate distinct threshold behaviors and greater
resilience in core linguistic capabilities. Critically, we discover a
``\textit{capability curse}" where larger, more capable models become more
susceptible to learning semantic corruptions, effectively following harmful
instructions more readily, while our analysis of base versus instruction-tuned
variants reveals that alignment provides inconsistent robustness benefits,
sometimes even reducing resilience. Our work establishes three core
contributions: (1) empirical evidence of SLMs' disproportionate vulnerability
to syntactic pattern contamination, (2) identification of asymmetric
sensitivity patterns between syntactic and semantic transformations, and (3)
systematic evaluation protocols for contamination robustness assessment. These
findings have immediate deployment implications, suggesting that current
robustness assumptions may not hold for smaller models and highlighting the
need for contamination-aware training protocols.

</details>


### [216] [Learning to Focus: Focal Attention for Selective and Scalable Transformers](https://arxiv.org/abs/2511.06818)
*Dhananjay Ram,Wei Xia,Stefano Soatto*

Main category: cs.CL

TL;DR: Focal Attention是一种改进的注意力机制，通过控制softmax温度来锐化注意力分布，使transformer模型能够更有效地关注相关tokens并抑制无关tokens。


<details>
  <summary>Details</summary>
Motivation: 标准softmax注意力机制会产生噪声概率分布，影响transformer模型各层的有效特征选择，特别是在处理长上下文时问题更加突出。

Method: 提出Focal Attention，通过控制softmax温度来锐化注意力分布。温度参数可以设定为固定超参数，也可以作为可学习参数在训练过程中优化。

Result: Focal Attention在模型大小、训练数据和上下文长度方面比标准transformer具有更好的扩展性。在多个基准测试中，使用多达42%更少参数或33%更少训练数据即可达到相同精度，在长上下文任务上实现17%-82%的相对改进。

Conclusion: Focal Attention是一种简单而有效的改进，能够显著提升transformer模型在长上下文任务中的性能，并在保持精度的同时大幅提高参数和数据效率。

Abstract: Attention is a core component of transformer architecture, whether
encoder-only, decoder-only, or encoder-decoder model. However, the standard
softmax attention often produces noisy probability distribution, which can
impair effective feature selection at every layer of these models, particularly
for long contexts. We propose Focal Attention, a simple yet effective
modification that sharpens the attention distribution by controlling the
softmax temperature, either as a fixed hyperparameter or as a learnable
parameter during training. This sharpening enables the model to concentrate on
the most relevant tokens while suppressing irrelevant ones. Empirically, Focal
Attention scales more favorably than standard transformer with respect to model
size, training data, and context length. Across diverse benchmarks, it achieves
the same accuracy with up to 42% fewer parameters or 33% less training data. On
long-context tasks, it delivers substantial relative improvements ranging from
17% to 82%, demonstrating its effectiveness in real world applications.

</details>


### [217] [Beyond Plain Demos: A Demo-centric Anchoring Paradigm for In-Context Learning in Alzheimer's Disease Detection](https://arxiv.org/abs/2511.06826)
*Puzhen Su,Haoran Yin,Yongzhu Miao,Jintao Tang,Shasha Li,Ting Wang*

Main category: cs.CL

TL;DR: DA4ICL是一个基于演示的锚定框架，通过多样对比检索扩展上下文宽度，通过投影向量锚定深化每个演示的信号，显著提升了LLM在AD检测任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 从叙述转录文本中检测阿尔茨海默病对大语言模型具有挑战性：预训练很少覆盖这种分布外任务，且所有演示样本描述相同场景，导致上下文高度同质化，这些因素损害了模型的内置任务知识和上下文感知能力。

Method: 提出DA4ICL框架，包含两个核心组件：1) 多样对比检索(DCR)扩展上下文宽度；2) 投影向量锚定(PVA)在每个Transformer层深化演示信号。这是一个以演示为中心的锚定框架，通过改进演示集合来增强上下文感知。

Result: 在三个AD基准测试上，DA4ICL相比标准ICL和任务向量基线取得了显著且稳定的性能提升，为细粒度、分布外和低资源的LLM适应开辟了新范式。

Conclusion: 通过精心设计演示集合来增强上下文感知能力，是解决OOD任务中LLM适应瓶颈的有效途径，DA4ICL框架为类似低资源、细粒度分类任务提供了新的解决方案思路。

Abstract: Detecting Alzheimer's disease (AD) from narrative transcripts challenges
large language models (LLMs): pre-training rarely covers this
out-of-distribution task, and all transcript demos describe the same scene,
producing highly homogeneous contexts. These factors cripple both the model's
built-in task knowledge (\textbf{task cognition}) and its ability to surface
subtle, class-discriminative cues (\textbf{contextual perception}). Because
cognition is fixed after pre-training, improving in-context learning (ICL) for
AD detection hinges on enriching perception through better demonstration (demo)
sets. We demonstrate that standard ICL quickly saturates, its demos lack
diversity (context width) and fail to convey fine-grained signals (context
depth), and that recent task vector (TV) approaches improve broad task
adaptation by injecting TV into the LLMs' hidden states (HSs), they are
ill-suited for AD detection due to the mismatch of injection granularity,
strength and position. To address these bottlenecks, we introduce
\textbf{DA4ICL}, a demo-centric anchoring framework that jointly expands
context width via \emph{\textbf{Diverse and Contrastive Retrieval}} (DCR) and
deepens each demo's signal via \emph{\textbf{Projected Vector Anchoring}} (PVA)
at every Transformer layer. Across three AD benchmarks, DA4ICL achieves large,
stable gains over both ICL and TV baselines, charting a new paradigm for
fine-grained, OOD and low-resource LLM adaptation.

</details>


### [218] [CLiFT-ASR: A Cross-Lingual Fine-Tuning Framework for Low-Resource Taiwanese Hokkien Speech Recognition](https://arxiv.org/abs/2511.06860)
*Hung-Yang Sung,Chien-Chun Wang,Kuan-Tang Huang,Tien-Hong Lo,Yu-Sheng Tsao,Yung-Chang Hsu,Berlin Chen*

Main category: cs.CL

TL;DR: CLiFT-ASR是一个两阶段跨语言微调框架，结合语音注音和汉字转录，显著提升台语闽南语ASR性能，相对降低24.88%字错误率。


<details>
  <summary>Details</summary>
Motivation: 低资源语言如台语闽南语的自动语音识别面临标注数据稀缺问题。现有方法存在局限：直接在汉字转录上微调无法捕获详细语音和声调线索，仅使用罗马化注音训练则缺乏词汇和句法覆盖。

Method: 提出CLiFT-ASR跨语言微调框架，基于普通话HuBERT模型，采用两阶段渐进式适应：第一阶段从语音台罗注音学习声学和声调表示，第二阶段从汉字转录捕获词汇和句法，实现语音与正字结构的有效对齐。

Result: 在TAT-MOE语料库上的实验表明，CLiFT-ASR相比强基线方法实现了24.88%的相对字错误率降低，证明了方法的有效性和参数效率。

Conclusion: CLiFT-ASR为台语闽南语ASR提供了高效且参数经济的解决方案，具有推广到其他低资源语言场景的潜力，填补了整合多种注音类型的分阶段策略研究空白。

Abstract: Automatic speech recognition (ASR) for low-resource languages such as
Taiwanese Hokkien is difficult due to the scarcity of annotated data. However,
direct fine-tuning on Han-character transcriptions often fails to capture
detailed phonetic and tonal cues, while training only on romanization lacks
lexical and syntactic coverage. In addition, prior studies have rarely explored
staged strategies that integrate both annotation types. To address this gap, we
present CLiFT-ASR, a cross-lingual fine-tuning framework that builds on
Mandarin HuBERT models and progressively adapts them to Taiwanese Hokkien. The
framework employs a two-stage process in which it first learns acoustic and
tonal representations from phonetic Tai-lo annotations and then captures
vocabulary and syntax from Han-character transcriptions. This progressive
adaptation enables effective alignment between speech sounds and orthographic
structures. Experiments on the TAT-MOE corpus demonstrate that CLiFT-ASR
achieves a 24.88\% relative reduction in character error rate (CER) compared
with strong baselines. The results indicate that CLiFT-ASR provides an
effective and parameter-efficient solution for Taiwanese Hokkien ASR and that
it has potential to benefit other low-resource language scenarios.

</details>


### [219] [Inclusion of Role into Named Entity Recognition and Ranking](https://arxiv.org/abs/2511.06886)
*Neelesh Kumar Shukla,Sanasam Ranbir Singh*

Main category: cs.CL

TL;DR: 该论文研究通过将实体角色检测问题建模为命名实体识别(NER)和实体检索/排序任务来解决这个问题，提出了自动化学习代表性词汇和构建角色实体表示的方法。


<details>
  <summary>Details</summary>
Motivation: 在自然语言处理系统中，实体需要在特定上下文中根据其行为或属性扮演不同角色。当需要基于角色检索实体子集时，如何定义角色和识别具有这些角色的实体成为关键挑战。传统实体类型（如人物、地点、组织）的细粒度角色检测对信息抽取、问答系统、文本摘要等任务具有重要意义。

Method: 提出两种建模方法：1)作为NER任务，将角色视为互斥类别，使用序列标注方法；2)作为实体检索/排序任务，将角色 formulate 为查询，实体作为文档集合。开发了自动化学习代表性词汇和短语的算法，构建角色和实体的表示形式。探索了句子级和文档级等不同上下文。针对领域特定数据集稀缺的问题，采用领域无关方式利用小数据集信息。

Result: 论文展示了将实体角色检测建模为NER和实体检索任务的可行性，成功构建了自动化学习代表性词汇的系统，并在不同上下文环境中验证了方法的有效性。特别是在数据有限的情况下，领域无关的方法表现出良好潜力。

Conclusion: 实体角色检测可以通过NER和实体检索两种范式有效解决。关键挑战在于角色和实体的间接描述特性，以及上下文依赖性。自动化学习表示方法和领域无关的泛化能力为实际应用提供了可行路径，特别是在缺乏大规模领域标注数据的场景下。

Abstract: Most of the Natural Language Processing sys- tems are involved in
entity-based processing for several tasks like Information Extraction,
Question-Answering, Text-Summarization and so on. A new challenge comes when
entities play roles according to their act or attributes in certain context.
Entity Role Detection is the task of assigning such roles to the entities. Usu-
ally real-world entities are of types: person, lo- cation and organization etc.
Roles could be con- sidered as domain-dependent subtypes of these types. In the
cases, where retrieving a subset of entities based on their roles is needed,
poses the problem of defining the role and entities having those roles. This
paper presents the study of study of solving Entity Role Detection prob- lem by
modeling it as Named Entity Recogni- tion (NER) and Entity Retrieval/Ranking
task. In NER, these roles could be considered as mutually exclusive classes and
standard NER methods like sequence tagging could be used. For Entity Retrieval,
Roles could be formulated as Query and entities as Collection on which the
query needs to be executed. The aspect of Entity Retrieval task, which is
different than document retrieval task is that the entities and roles against
which they need to be retrieved are indirectly described. We have formulated
au- tomated ways of learning representative words and phrases and building
representations of roles and entities using them. We have also explored
different contexts like sentence and document. Since the roles depend upon con-
text, so it is not always possible to have large domain-specific dataset or
knowledge bases for learning purposes, so we have tried to exploit the
information from small dataset in domain- agnostic way.

</details>


### [220] [AI Brown and AI Koditex: LLM-Generated Corpora Comparable to Traditional Corpora of English and Czech Texts](https://arxiv.org/abs/2509.22996)
*Jiří Milička,Anna Marklová,Václav Cvrček*

Main category: cs.CL

TL;DR: 这篇论文呈现了两个基于大型语言模型生成的英语和捷克语语料库，用于语言学上比较人类写作文本和AI生成文本。


<details>
  <summary>Details</summary>
Motivation: 创建一个多体裁、多主题、多文本类型的资源，用于从语言学角度比较人类写作文本与LLM生成文本，同时保持与现有人类语料库的可比性。

Method: 使用OpenAI、Anthropic、Alphabet、Meta和DeepSeek的模型（从GPT-3到GPT-4.5）生成文本，复制BE21英语语料库和Koditex捷克语料库的结构，按通用依存标准进行词法、句法标注。

Result: 构建了英语部分平均每个模型86.4万tokens（总计2700万tokens）和捷克语部分平均每个模型76.8万tokens（总计2150万tokens）的语料库。

Conclusion: 语料库以CC BY 4.0许可（标注数据为CC BY-NC-SA 4.0）免费开放下载，可通过捷克国家语料库搜索界面访问使用。

Abstract: This article presents two corpora of English and Czech texts generated with
large language models (LLMs). The motivation is to create a resource for
comparing human-written texts with LLM-generated text linguistically. Emphasis
was placed on ensuring these resources are multi-genre and rich in terms of
topics, authors, and text types, while maintaining comparability with existing
human-created corpora. These generated corpora replicate reference human
corpora: BE21 by Paul Baker, which is a modern version of the original Brown
Corpus, and Koditex corpus that also follows the Brown Corpus tradition but in
Czech. The new corpora were generated using models from OpenAI, Anthropic,
Alphabet, Meta, and DeepSeek, ranging from GPT-3 (davinci-002) to GPT-4.5, and
are tagged according to the Universal Dependencies standard (i.e., they are
tokenized, lemmatized, and morphologically and syntactically annotated). The
subcorpus size varies according to the model used (the English part contains on
average 864k tokens per model, 27M tokens altogether, the Czech partcontains on
average 768k tokens per model, 21.5M tokens altogether). The corpora are freely
available for download under the CC BY 4.0 license (the annotated data are
under CC BY-NC-SA 4.0 licence) and are also accessible through the search
interface of the Czech National Corpus.

</details>


### [221] [EduGuardBench: A Holistic Benchmark for Evaluating the Pedagogical Fidelity and Adversarial Safety of LLMs as Simulated Teachers](https://arxiv.org/abs/2511.06890)
*Yilin Jiang,Mingzi Zhang,Xuanyu Yin,Sheng Jin,Suyu Lu,Zuocan Ying,Zengyi Yu,Xiangjie Kong*

Main category: cs.CL

TL;DR: 本研究提出了EduGuardBench基准，用于评估模拟教师角色的语言模型的专业忠实度和教学安全，发现了中等规模模型最脆弱的反直觉现象和教育转换效应。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在教育个性化中的应用，确保模拟教师角色的LLMs具备专业能力和伦理安全成为关键挑战，但现有基准无法评估角色扮演忠实度或处理教育场景特有的教学伤害。

Method: 提出双组件基准EduGuardBench：使用角色扮演忠实度评分(RFS)评估专业忠实度，诊断教学职业特有的伤害；通过基于人设的对抗性提示探测安全漏洞，使用攻击成功率(ASR)和三层拒绝质量评估指标。

Result: 对14个主流模型的实验显示性能极化：推理导向模型表现更优，但无能是主要失败模式；发现反直觉的扩展悖论，中等规模模型最脆弱；识别出强大的教育转换效应，最安全的模型能将有害请求转化为可教学时刻。

Conclusion: EduGuardBench提供了可重现的框架，超越了孤立的知识测试，实现了专业、伦理和教学对齐的整体评估，揭示了在教育中部署可信AI所需的复杂动态。

Abstract: Large Language Models for Simulating Professions (SP-LLMs), particularly as
teachers, are pivotal for personalized education. However, ensuring their
professional competence and ethical safety is a critical challenge, as existing
benchmarks fail to measure role-playing fidelity or address the unique teaching
harms inherent in educational scenarios. To address this, we propose
EduGuardBench, a dual-component benchmark. It assesses professional fidelity
using a Role-playing Fidelity Score (RFS) while diagnosing harms specific to
the teaching profession. It also probes safety vulnerabilities using
persona-based adversarial prompts targeting both general harms and,
particularly, academic misconduct, evaluated with metrics including Attack
Success Rate (ASR) and a three-tier Refusal Quality assessment. Our extensive
experiments on 14 leading models reveal a stark polarization in performance.
While reasoning-oriented models generally show superior fidelity, incompetence
remains the dominant failure mode across most models. The adversarial tests
uncovered a counterintuitive scaling paradox, where mid-sized models can be the
most vulnerable, challenging monotonic safety assumptions. Critically, we
identified a powerful Educational Transformation Effect: the safest models
excel at converting harmful requests into teachable moments by providing ideal
Educational Refusals. This capacity is strongly negatively correlated with ASR,
revealing a new dimension of advanced AI safety. EduGuardBench thus provides a
reproducible framework that moves beyond siloed knowledge tests toward a
holistic assessment of professional, ethical, and pedagogical alignment,
uncovering complex dynamics essential for deploying trustworthy AI in
education. See https://github.com/YL1N/EduGuardBench for Materials.

</details>


### [222] [RPTS: Tree-Structured Reasoning Process Scoring for Faithful Multimodal Evaluation](https://arxiv.org/abs/2511.06899)
*Haofeng Wang,Yu Zhang*

Main category: cs.CL

TL;DR: 提出基于树结构的推理过程评分方法RPTS，构建多模态推理评估基准RPTS-Eval，揭示LVLMs在推理过程中的局限性


<details>
  <summary>Details</summary>
Motivation: 现有LVLMs评估基准主要采用选择题或简答题形式，无法评估推理过程；部分评估推理过程的方法过于简单，只在答案错误时检查推理，忽略错误推理导致正确答案的情况，且未考虑模态间关系对推理的影响

Method: 提出推理过程树评分(RPTS)，将推理步骤组织成推理树结构，利用层次信息为每个推理步骤分配加权忠实度分数；构建包含374张图片和390个推理实例的RPTS-Eval基准，定义三种模态间关系类型

Result: 通过RPTS-Eval评估代表性LVLMs(如GPT4o、Llava-Next)，发现它们在多模态推理中的局限性，揭示开源与闭源商业LVLMs之间的差异

Conclusion: RPTS不仅能评估推理整体正确性，还能精确定位模型推理失败位置；该基准将促进多模态推理领域研究发展，为改进LVLMs的推理能力提供重要指导

Abstract: Large Vision-Language Models (LVLMs) excel in multimodal reasoning and have
shown impressive performance on various multimodal benchmarks. However, most of
these benchmarks evaluate models primarily through multiple-choice or
short-answer formats, which do not take the reasoning process into account.
Although some benchmarks assess the reasoning process, their methods are often
overly simplistic and only examine reasoning when answers are incorrect. This
approach overlooks scenarios where flawed reasoning leads to correct answers.
In addition, these benchmarks do not consider the impact of intermodal
relationships on reasoning. To address this issue, we propose the Reasoning
Process Tree Score (RPTS), a tree structure-based metric to assess reasoning
processes. Specifically, we organize the reasoning steps into a reasoning tree
and leverage its hierarchical information to assign weighted faithfulness
scores to each reasoning step. By dynamically adjusting these weights, RPTS not
only evaluates the overall correctness of the reasoning, but also pinpoints
where the model fails in the reasoning. To validate RPTS in real-world
multimodal scenarios, we construct a new benchmark, RPTS-Eval, comprising 374
images and 390 reasoning instances. Each instance includes reliable
visual-textual clues that serve as leaf nodes of the reasoning tree.
Furthermore, we define three types of intermodal relationships to investigate
how intermodal interactions influence the reasoning process. We evaluated
representative LVLMs (e.g., GPT4o, Llava-Next), uncovering their limitations in
multimodal reasoning and highlighting the differences between open-source and
closed-source commercial LVLMs. We believe that this benchmark will contribute
to the advancement of research in the field of multimodal reasoning.

</details>


### [223] [HLPD: Aligning LLMs to Human Language Preference for Machine-Revised Text Detection](https://arxiv.org/abs/2511.06942)
*Fangqi Dai,Xingjian Jiang,Zizhuang Deng*

Main category: cs.CL

TL;DR: 本文提出了人类语言偏好检测(HLPD)方法，通过奖励对齐使模型更敏感于人类写作模式，在黑盒设置下有效识别LLM生成和修改的文本，性能显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 防止LLM生成的可信内容导致错误信息和社会问题，需要开发高效可靠的文本溯源方法。现有方法在检测完全由LLM生成的文本表现良好，但在面对高级LLM输出或经过对抗性多任务机器修改的文本时，特别是在黑盒设置下表现不佳。

Method: 基于人类写作具有独特风格模式的假设，提出人类语言偏好检测(HLPD)方法。HLPD采用基于奖励的对齐过程——人类语言偏好优化(HLPO)，将评分模型的token分布向类似人类的写作方向调整，使模型对人类写作更敏感，从而增强对机器修改文本的识别能力。在五维提示生成器和多个高级LLM构成的对抗性多任务评估框架中进行测试。

Result: 在检测GPT系列模型修改的文本时，HLPD相对于ImBD实现了15.11%的AUROC相对提升，超越Fast-DetectGPT 45.56%；在评估高级LLM生成的文本时，HLPD获得最高平均AUROC，超过ImBD 5.53%和Fast-DetectGPT 34.14%。

Conclusion: HLPD通过让模型学习人类语言偏好，显著提升了对机器生成和修改文本的检测能力，特别是在黑盒设置和对抗性场景下表现出色，为防止LLM产生的错误信息提供了有效的技术解决方案。

Abstract: To prevent misinformation and social issues arising from trustworthy-looking
content generated by LLMs, it is crucial to develop efficient and reliable
methods for identifying the source of texts. Previous approaches have
demonstrated exceptional performance in detecting texts fully generated by
LLMs. However, these methods struggle when confronting more advanced LLM output
or text with adversarial multi-task machine revision, especially in the
black-box setting, where the generating model is unknown. To address this
challenge, grounded in the hypothesis that human writing possesses distinctive
stylistic patterns, we propose Human Language Preference Detection (HLPD). HLPD
employs a reward-based alignment process, Human Language Preference
Optimization (HLPO), to shift the scoring model's token distribution toward
human-like writing, making the model more sensitive to human writing, therefore
enhancing the identification of machine-revised text. We test HLPD in an
adversarial multi-task evaluation framework that leverages a five-dimensional
prompt generator and multiple advanced LLMs to create diverse revision
scenarios. When detecting texts revised by GPT-series models, HLPD achieves a
15.11% relative improvement in AUROC over ImBD, surpassing Fast-DetectGPT by
45.56%. When evaluated on texts generated by advanced LLMs, HLPD achieves the
highest average AUROC, exceeding ImBD by 5.53% and Fast-DetectGPT by 34.14%.
Code will be made available at https://github.com/dfq2021/HLPD.

</details>


### [224] [SCOPE: Intrinsic Semantic Space Control for Mitigating Copyright Infringement in LLMs](https://arxiv.org/abs/2511.07001)
*Zhenliang Zhang,Xinyu Hu,Xiaojun Wan*

Main category: cs.CL

TL;DR: SCOPE提出一种无需参数更新的推理时方法，通过语义空间控制来缓解大型语言模型的版权侵权问题，在不降低模型实用性的前提下有效防止版权内容泄露。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在生成内容时会无意中复制受版权保护的段落，给下游应用带来法律风险。现有的推理时防御方法主要关注表层token匹配，依赖外部阻止列表或过滤器，不仅增加了部署复杂性，还可能忽略语义上转述的泄露问题。

Method: SCOPE将版权侵权缓解重新定义为内在语义空间控制，提出一种无需参数更新或辅助过滤器的推理时方法。具体使用稀疏自编码器将隐藏状态投影到高维、近单语义空间，利用这种表示识别出版权敏感子空间，并在解码过程中钳制其激活。

Result: 在广泛认可的基准测试上，实验表明SCOPE能够在不降低通用实用性的前提下缓解版权侵权。进一步的可解释性分析证实，隔离出的子空间捕获了高级语义信息。

Conclusion: SCOPE通过在语义空间层面进行控制而非表层匹配，实现了更有效的版权内容泄露防护，为大型语言模型的安全部署提供了新的技术路径，同时保持了模型的原有性能。

Abstract: Large language models sometimes inadvertently reproduce passages that are
copyrighted, exposing downstream applications to legal risk. Most existing
studies for inference-time defences focus on surface-level token matching and
rely on external blocklists or filters, which add deployment complexity and may
overlook semantically paraphrased leakage. In this work, we reframe copyright
infringement mitigation as intrinsic semantic-space control and introduce
SCOPE, an inference-time method that requires no parameter updates or auxiliary
filters. Specifically, the sparse autoencoder (SAE) projects hidden states into
a high-dimensional, near-monosemantic space; benefiting from this
representation, we identify a copyright-sensitive subspace and clamp its
activations during decoding. Experiments on widely recognized benchmarks show
that SCOPE mitigates copyright infringement without degrading general utility.
Further interpretability analyses confirm that the isolated subspace captures
high-level semantics.

</details>


### [225] [Beyond English: Toward Inclusive and Scalable Multilingual Machine Translation with LLMs](https://arxiv.org/abs/2511.07003)
*Yingfeng Luo,Ziqiang Xu,Yuxuan Ouyang,Murun Yang,Dingyang Lin,Kaiyan Chang,Tong Zheng,Bei Li,Peinan Feng,Quan Du,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: LMT是一套以中英为中心的大型多语言翻译模型，覆盖60种语言和234个翻译方向，通过识别方向性退化现象并提出战略性降采样和平行多语言提示方法，在可比语言覆盖模型中达到SOTA性能，4B参数模型超越更大规模的Aya-101-13B和NLLB-54B模型。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在多语言机器翻译领域面临语言覆盖范围广、翻译质量一致性差、英语中心偏见等挑战，需要开发更全面、高质量的多语言翻译系统。

Method: 1. 构建以中英为中心的LMT模型套件，覆盖60种语言；2. 发现并解决方向性退化现象，提出战略性降采样方法；3. 设计平行多语言提示技术，利用类型学相关辅助语言增强跨语言迁移；4. 采用严格数据筛选和精细适应策略。

Result: LMT在可比语言覆盖模型中达到SOTA性能，4B参数模型显著超越更大规模的Aya-101-13B和NLLB-54B模型，发布0.6B/1.7B/4B/8B四种规格供研究使用。

Conclusion: LMT为包容性、可扩展和高质量的多语言机器翻译提供了强有力的基线，通过创新的数据处理和模型优化策略，有效解决了现有大型语言模型在多语言翻译中的关键挑战。

Abstract: Large language models have significantly advanced Multilingual Machine
Translation (MMT), yet the broad language coverage, consistent translation
quality, and English-centric bias remain open challenges. To address these
challenges, we introduce \textbf{LMT}, a suite of \textbf{L}arge-scale
\textbf{M}ultilingual \textbf{T}ranslation models centered on both Chinese and
English, covering 60 languages and 234 translation directions. During
development, we identify a previously overlooked phenomenon of
\textbf{directional degeneration}, where symmetric multi-way fine-tuning data
overemphasize reverse directions (X $\to$ En/Zh), leading to excessive
many-to-one mappings and degraded translation quality. We propose
\textbf{Strategic Downsampling}, a simple yet effective method to mitigate this
degeneration. In addition, we design \textbf{Parallel Multilingual Prompting
(PMP)}, which leverages typologically related auxiliary languages to enhance
cross-lingual transfer. Through rigorous data curation and refined adaptation
strategies, LMT achieves SOTA performance among models of comparable language
coverage, with our 4B model (LMT-60-4B) surpassing the much larger Aya-101-13B
and NLLB-54B models by a substantial margin. We release LMT in four sizes
(0.6B/1.7B/4B/8B) to catalyze future research and provide strong baselines for
inclusive, scalable, and high-quality MMT
\footnote{\href{https://github.com/NiuTrans/LMT}{https://github.com/NiuTrans/LMT}}.

</details>


### [226] [A Picture is Worth a Thousand (Correct) Captions: A Vision-Guided Judge-Corrector System for Multimodal Machine Translation](https://arxiv.org/abs/2511.07010)
*Siddharth Betala,Kushan Raj,Vipul Betala,Rohan Saswade*

Main category: cs.CL

TL;DR: 本文提出了一个两阶段英印多模态翻译系统BLEU Monday，通过视觉增强的评判-校正管道自动检测和修正训练数据中的翻译错误，然后使用LoRA对IndicTrans2模型进行高效微调，在四种英印语言对的翻译任务中实现了显著的BLEU分数提升。


<details>
  <summary>Details</summary>
Motivation: 为了解决英印多模态翻译任务中训练数据质量问题，特别是缺乏视觉上下文导致的翻译歧义和纯翻译质量问题，需要开发自动化的数据清洗方法来提升模型性能。

Method: 提出两阶段方法：1）视觉增强的评判-校正管道，使用多模态语言模型将翻译分为正确、视觉模糊、错误三类，分别用GPT-4o-mini处理视觉模糊问题，IndicTrans2处理纯质量问题；2）使用LoRA对IndicTrans2 200M蒸馏模型进行参数高效微调。

Result: 处理了28,928个训练样本，平均每语言修正17.1%的标注。修正数据训练的模型在测试集上取得显著提升：英-孟加拉语+1.30 BLEU(42.00→43.30)，挑战集+0.70(44.90→45.60)；英-奥里亚语+0.60(41.00→41.60)；英-印地语挑战集+0.10(53.90→54.00)。

Conclusion: 自动化数据质量提升结合参数高效微调是改进英印翻译的有效方法，视觉增强的评判-校正管道能系统性识别和修正训练数据中的翻译错误，在保持模型效率的同时提升翻译质量。

Abstract: In this paper, we describe our system under the team name BLEU Monday for the
English-to-Indic Multimodal Translation Task at WAT 2025. We participate in the
text-only translation tasks for English-Hindi, English-Bengali,
English-Malayalam, and English-Odia language pairs. We present a two-stage
approach that addresses quality issues in the training data through automated
error detection and correction, followed by parameter-efficient model
fine-tuning.
  Our methodology introduces a vision-augmented judge-corrector pipeline that
leverages multimodal language models to systematically identify and correct
translation errors in the training data. The judge component classifies
translations into three categories: correct, visually ambiguous (requiring
image context), or mistranslated (poor translation quality). Identified errors
are routed to specialized correctors: GPT-4o-mini regenerates captions
requiring visual disambiguation, while IndicTrans2 retranslates cases with pure
translation quality issues. This automated pipeline processes 28,928 training
examples across four languages, correcting an average of 17.1% of captions per
language.
  We then apply Low-Rank Adaptation (LoRA) to fine-tune the IndicTrans2
en-indic 200M distilled model on both original and corrected datasets. Training
on corrected data yields consistent improvements, with BLEU score gains of
+1.30 for English-Bengali on the evaluation set (42.00 -> 43.30) and +0.70 on
the challenge set (44.90 -> 45.60), +0.60 for English-Odia on the evaluation
set (41.00 -> 41.60), and +0.10 for English-Hindi on the challenge set (53.90
-> 54.00).

</details>


### [227] [Multilingual Lexical Feature Analysis of Spoken Language for Predicting Major Depression Symptom Severity](https://arxiv.org/abs/2511.07011)
*Anastasiia Tokareva,Judith Dineley,Zoe Firth,Pauline Conde,Faith Matcham,Sara Siddi,Femke Lamers,Ewan Carr,Carolin Oetzmann,Daniel Leightley,Yuezhou Zhang,Amos A. Folarin,Josep Maria Haro,Brenda W. J. H. Penninx,Raquel Bailon,Srinivasan Vairavan,Til Wykes,Richard J. B. Dobson,Vaibhav A. Narayan,Matthew Hotopf,Nicholas Cummins,The RADAR-CNS Consortium*

Main category: cs.CL

TL;DR: 本研究探索了使用移动设备收集的语音数据中的词汇特征来评估重度抑郁症症状严重程度的可行性，发现某些词汇特征与症状相关，但预测效果有限。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用移动设备收集的语音语言数据，实现对抑郁症症状严重程度的客观、定期评估和复发早期检测，克服现有研究基于书面语言、样本非临床性和方法可解释性差的限制。

Method: 分析了RADAR-MDD研究中586名参与者的5,836个语音录音和PHQ-8评估数据，使用线性混合效应模型识别可解释词汇特征，并用四种机器学习回归模型测试预测性能。

Result: 英语数据显示7个词汇特征与MDD症状相关（如词汇多样性、绝对主义语言）；荷兰语显示句子长度和积极词频的关联；西班牙语无显著关联。所有语言的预测性能均接近偶然水平。

Conclusion: 需要在大规模多语言样本中进一步研究，改进数据收集协议，开发考虑个体语言变异的机器学习模型，以确定词汇标记在临床实践中的价值。

Abstract: Background: Captured between clinical appointments using mobile devices,
spoken language has potential for objective, more regular assessment of symptom
severity and earlier detection of relapse in major depressive disorder.
However, research to date has largely been in non-clinical cross-sectional
samples of written language using complex machine learning (ML) approaches with
limited interpretability.
  Methods: We describe an initial exploratory analysis of longitudinal speech
data and PHQ-8 assessments from 5,836 recordings of 586 participants in the UK,
Netherlands, and Spain, collected in the RADAR-MDD study. We sought to identify
interpretable lexical features associated with MDD symptom severity with linear
mixed-effects modelling. Interpretable features and high-dimensional vector
embeddings were also used to test the prediction performance of four regressor
ML models.
  Results: In English data, MDD symptom severity was associated with 7 features
including lexical diversity measures and absolutist language. In Dutch,
associations were observed with words per sentence and positive word frequency;
no associations were observed in recordings collected in Spain. The predictive
power of lexical features and vector embeddings was near chance level across
all languages.
  Limitations: Smaller samples in non-English speech and methodological
choices, such as the elicitation prompt, may have also limited the effect sizes
observable. A lack of NLP tools in languages other than English restricted our
feature choice.
  Conclusion: To understand the value of lexical markers in clinical research
and practice, further research is needed in larger samples across several
languages using improved protocols, and ML models that account for within- and
between-individual variations in language.

</details>


### [228] [Llama-Embed-Nemotron-8B: A Universal Text Embedding Model for Multilingual and Cross-Lingual Tasks](https://arxiv.org/abs/2511.07025)
*Yauhen Babakhin,Radek Osmulski,Ronay Ak,Gabriel Moreira,Mengyao Xu,Benedikt Schifferer,Bo Liu,Even Oldridge*

Main category: cs.CL

TL;DR: 本文介绍了llama-embed-nemotron-8b，一个开源权重的文本嵌入模型，在MMTEB基准测试中达到最先进性能，通过1610万查询-文档对训练，支持多语言场景和用户自定义指令。


<details>
  <summary>Details</summary>
Motivation: 针对现有文本嵌入模型训练数据和方法不够透明的问题，作者旨在开发一个完全开源的模型，公开权重、详细消融研究和训练数据集，推动文本嵌入领域的开放研究。

Method: 使用1610万查询-文档对训练模型（770万来自公开数据集，840万通过开源LLM合成生成），进行详尽的消融研究分析对比损失实现、合成数据生成策略和模型融合的影响，构建支持用户自定义指令的指令感知模型。

Result: 模型在MMTEB排行榜达到最先进性能，在检索、分类和语义文本相似度等所有主要嵌入任务上表现优异，特别在低资源语言和跨语言场景下表现出色，成为通用文本嵌入解决方案。

Conclusion: llama-embed-nemotron-8b通过顶级性能、广泛适用性和用户驱动灵活性的结合，成功实现了作为通用文本嵌入解决方案的目标，为文本嵌入领域提供了重要的开源贡献。

Abstract: We introduce llama-embed-nemotron-8b, an open-weights text embedding model
that achieves state-of-the-art performance on the Multilingual Massive Text
Embedding Benchmark (MMTEB) leaderboard as of October 21, 2025. While recent
models show strong performance, their training data or methodologies are often
not fully disclosed. We aim to address this by developing a fully open-source
model, publicly releasing its weights and detailed ablation studies, and
planning to share the curated training datasets. Our model demonstrates
superior performance across all major embedding tasks -- including retrieval,
classification and semantic textual similarity (STS) -- and excels in
challenging multilingual scenarios, such as low-resource languages and
cross-lingual setups. This state-of-the-art performance is driven by a novel
data mix of 16.1 million query-document pairs, split between 7.7 million
samples from public datasets and 8.4 million synthetically generated examples
from various open-weight LLMs. One of our key contributions is a detailed
ablation study analyzing core design choices, including a comparison of
contrastive loss implementations, an evaluation of synthetic data generation
(SDG) strategies, and the impact of model merging. The llama-embed-nemotron-8b
is an instruction-aware model, supporting user-defined instructions to enhance
performance for specific use-cases. This combination of top-tier performance,
broad applicability, and user-driven flexibility enables it to serve as a
universal text embedding solution.

</details>


### [229] [Evaluating LLMs for Anxiety, Depression, and Stress Detection Evaluating Large Language Models for Anxiety, Depression, and Stress Detection: Insights into Prompting Strategies and Synthetic Data](https://arxiv.org/abs/2511.07044)
*Mihael Arcan,David-Paul Niland*

Main category: cs.CL

TL;DR: 研究比较了多种方法用于文本心理健康检测，发现基于Transformer的模型（Distil-RoBERTa和XLNet）表现最佳，合成数据技术有效提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 心理健康障碍影响全球五分之一成年人，但症状表达的微妙性和多样性使得通过文本检测这些状况极具挑战性，需要更有效的自动化评估方法。

Method: 在DAIC-WOZ临床访谈数据集上，比较评估了LLMs（Llama、GPT）、传统机器学习和Transformer模型（BERT、XLNet、Distil-RoBERTa），对焦虑、抑郁、压力分类进行微调，并应用合成数据生成技术缓解类别不平衡问题。

Result: Distil-RoBERTa在GAD-2焦虑检测上F1分数最高（0.883），XLNet在PHQ抑郁任务上表现最佳（F1达0.891），零样本合成方法在压力检测上F1为0.884、ROC AUC为0.886。

Conclusion: 基于Transformer的模型和合成数据技术在提高召回率和泛化能力方面效果显著，但需要谨慎校准以防止精度损失，高级语言模型与数据增强的结合有潜力增强文本自动心理健康评估。

Abstract: Mental health disorders affect over one-fifth of adults globally, yet
detecting such conditions from text remains challenging due to the subtle and
varied nature of symptom expression. This study evaluates multiple approaches
for mental health detection, comparing Large Language Models (LLMs) such as
Llama and GPT with classical machine learning and transformer-based
architectures including BERT, XLNet, and Distil-RoBERTa. Using the DAIC-WOZ
dataset of clinical interviews, we fine-tuned models for anxiety, depression,
and stress classification and applied synthetic data generation to mitigate
class imbalance. Results show that Distil-RoBERTa achieved the highest F1 score
(0.883) for GAD-2, while XLNet outperformed others on PHQ tasks (F1 up to
0.891). For stress detection, a zero-shot synthetic approach
(SD+Zero-Shot-Basic) reached an F1 of 0.884 and ROC AUC of 0.886. Findings
demonstrate the effectiveness of transformer-based models and highlight the
value of synthetic data in improving recall and generalization. However,
careful calibration is required to prevent precision loss. Overall, this work
emphasizes the potential of combining advanced language models and data
augmentation to enhance automated mental health assessment from text.

</details>


### [230] [When Sufficient is not Enough: Utilizing the Rashomon Effect for Complete Evidence Extraction](https://arxiv.org/abs/2511.07055)
*Katharina Beckh,Stefan Rüping*

Main category: cs.CL

TL;DR: 该研究表明，通过聚合多个模型的特征归因证据，可以显著提高完整证据识别的召回率，从单一最佳模型的约60%提升到集成模型的约86%。


<details>
  <summary>Details</summary>
Motivation: 传统的特征归因方法通常只提供最小充分证据来证明模型决策，但在合规性和目录管理等应用中，需要识别所有贡献特征的完整证据集。现有方法无法满足这种完整性要求。

Method: 在包含人工标注完整证据的医疗数据集上进行案例研究，通过聚合多个模型的证据来提高证据召回率，并分析召回-精确率权衡、证据训练的作用以及基于确定性阈值的动态集成方法。

Result: 单一模型通常只能恢复完整证据的子集，而通过聚合多个模型的证据，将证据召回率从约0.60（单一最佳模型）提升到约0.86（集成模型）。研究还揭示了召回-精确率之间的权衡关系。

Conclusion: 集成方法能够有效改善特征归因的完整性，为需要完整证据的应用场景提供了可行的解决方案。研究还探讨了证据训练和动态集成策略的影响，为实际应用提供了指导。

Abstract: Feature attribution methods typically provide minimal sufficient evidence
justifying a model decision. However, in many applications this is inadequate.
For compliance and cataloging, the full set of contributing features must be
identified - complete evidence. We perform a case study on a medical dataset
which contains human-annotated complete evidence. We show that individual
models typically recover only subsets of complete evidence and that aggregating
evidence from several models improves evidence recall from $\sim$0.60 (single
best model) to $\sim$0.86 (ensemble). We analyze the recall-precision
trade-off, the role of training with evidence, dynamic ensembles with certainty
thresholds, and discuss implications.

</details>


### [231] [Aligning Attention with Human Rationales for Self-Explaining Hate Speech Detection](https://arxiv.org/abs/2511.07065)
*Brage Eilertsen,Røskva Bjørgfinsdóttir,Francielle Vargas,Ali Ramezani-Kebrya*

Main category: cs.CL

TL;DR: SRA是一种监督理性注意力框架，通过将模型注意力与人类推理依据对齐，在仇恨言论分类中实现2.4倍更好的可解释性，同时保持竞争性的公平性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型的不透明特性给仇恨言论检测系统的伦理部署带来了重大挑战，需要提高模型的可解释性和公平性。

Method: 将监督注意力机制集成到基于transformer的分类器中，优化联合目标函数，该函数结合标准分类损失与对齐损失项，最小化注意力权重与人工标注推理依据之间的差异。

Result: 在英文和葡萄牙语仇恨言论基准数据集上，SRA相比当前基线实现2.4倍更好的可解释性，生成更忠实且与人类对齐的词级解释，在所有公平性指标上达到竞争性表现。

Conclusion: 将人类推理依据纳入注意力机制能够增强仇恨言论检测的可解释性和忠实度，而不会牺牲公平性，为伦理部署提供了有效解决方案。

Abstract: The opaque nature of deep learning models presents significant challenges for
the ethical deployment of hate speech detection systems. To address this
limitation, we introduce Supervised Rational Attention (SRA), a framework that
explicitly aligns model attention with human rationales, improving both
interpretability and fairness in hate speech classification. SRA integrates a
supervised attention mechanism into transformer-based classifiers, optimizing a
joint objective that combines standard classification loss with an alignment
loss term that minimizes the discrepancy between attention weights and
human-annotated rationales. We evaluated SRA on hate speech benchmarks in
English (HateXplain) and Portuguese (HateBRXplain) with rationale annotations.
Empirically, SRA achieves 2.4x better explainability compared to current
baselines, and produces token-level explanations that are more faithful and
human-aligned. In terms of fairness, SRA achieves competitive fairness across
all measures, with second-best performance in detecting toxic posts targeting
identity groups, while maintaining comparable results on other metrics. These
findings demonstrate that incorporating human rationales into attention
mechanisms can enhance interpretability and faithfulness without compromising
fairness.

</details>


### [232] [Importance-Aware Data Selection for Efficient LLM Instruction Tuning](https://arxiv.org/abs/2511.07074)
*Tingyu Jiang,Shen Li,Yiyao Song,Lan Zhang,Hualei Zhu,Yuan Zhao,Xiaohang Xu,Kenjiro Taura,Hao Henry Wang*

Main category: cs.CL

TL;DR: 提出MIWV指标用于筛选对指令调优最有价值的数据，仅用top 1%数据就能超越全量数据训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究关注数据质量评分而非针对特定LLM筛选最优数据，需要找到能最大化提升给定模型指令调优性能的高质量数据选择方法。

Method: 提出模型指令弱点值(MIWV)指标，通过分析模型在上下文学习(ICL)时的响应差异来量化指令数据重要性，筛选对模型能力提升最有益的数据。

Result: 实验表明，基于MIWV选择仅top 1%的数据训练效果优于使用完整数据集，证明了该方法的有效性。

Conclusion: MIWV为针对特定LLM选择最优指令数据提供了新思路，超越了单纯的数据质量评分方法，为高效指令调优提供了实证支持。

Abstract: Instruction tuning plays a critical role in enhancing the performance and
efficiency of Large Language Models (LLMs). Its success depends not only on the
quality of the instruction data but also on the inherent capabilities of the
LLM itself. Some studies suggest that even a small amount of high-quality data
can achieve instruction fine-tuning results that are on par with, or even
exceed, those from using a full-scale dataset. However, rather than focusing
solely on calculating data quality scores to evaluate instruction data, there
is a growing need to select high-quality data that maximally enhances the
performance of instruction tuning for a given LLM. In this paper, we propose
the Model Instruction Weakness Value (MIWV) as a novel metric to quantify the
importance of instruction data in enhancing model's capabilities. The MIWV
metric is derived from the discrepancies in the model's responses when using
In-Context Learning (ICL), helping identify the most beneficial data for
enhancing instruction tuning performance. Our experimental results demonstrate
that selecting only the top 1\% of data based on MIWV can outperform training
on the full dataset. Furthermore, this approach extends beyond existing
research that focuses on data quality scoring for data selection, offering
strong empirical evidence supporting the effectiveness of our proposed method.

</details>


### [233] [Wasm: A Pipeline for Constructing Structured Arabic Interleaved Multimodal Corpora](https://arxiv.org/abs/2511.07080)
*Khalil Hennara,Ahmad Bastati,Muhammad Hreden,Mohamed Motasim Hamed,Zeina Aldallal,Sara Chrouf,Safwan AlModhayan*

Main category: cs.CL

TL;DR: 本文提出了Wasm流水线，用于处理Common Crawl数据集，创建了首个提供markdown输出的阿拉伯语多模态数据集，解决了该语言缺乏高质量结构化多模态数据的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和多模态模型的性能严重依赖于预训练数据集的质量和规模。最新研究表明，在自然文档上训练的多模态模型优于仅在图像-文本对上训练的模型，但阿拉伯语缺乏保留文档结构的高质量多模态数据集，这限制了相关研究的进展。

Method: 开发了Wasm处理流水线，从Common Crawl数据集中提取和处理阿拉伯语网页内容， uniquely提供markdown格式输出，保留网页内容的结构完整性，同时支持纯文本和多模态预训练场景。

Result: 创建了一个新的阿拉伯语多模态数据集，提供了代表性数据集样本和处理流水线的公开版本，与现有数据集的处理方法进行了详细比较分析，验证了过滤策略的收敛性和设计选择的合理性。

Conclusion: 本研究填补了阿拉伯语多模态数据集的空白，通过Wasm流水线成功创建了保留文档结构的数据集，为未来的阿拉伯语多模态模型研究提供了重要资源支持。

Abstract: The performance of large language models (LLMs) and large multimodal models
(LMMs) depends heavily on the quality and scale of their pre-training datasets.
Recent research shows that large multimodal models trained on natural documents
where images and text are interleaved outperform those trained only on
image-text pairs across a wide range of benchmarks, leveraging advanced pre-
trained models to enforce semantic alignment, image-sequence consistency, and
textual coherence. For Arabic, however, the lack of high-quality multimodal
datasets that preserve document structure has limited progress. In this paper,
we present our pipeline Wasm for processing the Common Crawl dataset to create
a new Arabic multimodal dataset that uniquely provides markdown output. Unlike
existing Arabic corpora that focus solely on text extraction, our approach
preserves the structural integrity of web content while maintaining flexibility
for both text-only and multimodal pre-training scenarios. We provide a
comprehensive comparative analysis of our data processing pipeline against
those used for major existing datasets, highlighting the convergences in
filtering strategies and justifying our specific design choices. To support
future research, we publicly release a representative dataset dump along with
the multimodal processing pipeline for Arabic.

</details>


### [234] [Think Consistently, Reason Efficiently: Energy-Based Calibration for Implicit Chain-of-Thought](https://arxiv.org/abs/2511.07124)
*Zhikang Chen,Sen Cui,Deheng Ye,Yu Zhang,Yatao Bian,Tingting Zhu*

Main category: cs.CL

TL;DR: EBM-CoT是一个基于能量的思维链校准框架，通过EBM模型优化隐空间中的推理轨迹，提升大语言模型多步推理的一致性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统思维链方法依赖离散token级推理，存在错误传播和词汇表达限制，推理轨迹僵化不一致；现有隐式推理方法虽缓解部分问题，但缺乏显式机制保证推理步骤间一致性，导致推理路径发散和结果不稳定。

Method: 提出EBM-CoT框架，使用基于能量的模型校准潜在思维表示，动态调整潜在推理轨迹向嵌入空间中低能量、高一致性区域移动，无需修改基础语言模型。

Result: 在数学、常识和符号推理基准上的广泛实验表明，该框架显著增强了LLMs中多步推理的一致性和效率。

Conclusion: EBM-CoT通过能量模型优化隐空间推理轨迹，在不改变基础模型的前提下有效提升了多步推理的准确性和一致性，为解决推理过程稳定性问题提供了新思路。

Abstract: Large Language Models (LLMs) have demonstrated strong reasoning capabilities
through \emph{Chain-of-Thought} (CoT) prompting, which enables step-by-step
intermediate reasoning. However, explicit CoT methods rely on discrete
token-level reasoning processes that are prone to error propagation and limited
by vocabulary expressiveness, often resulting in rigid and inconsistent
reasoning trajectories. Recent research has explored implicit or continuous
reasoning in latent spaces, allowing models to perform internal reasoning
before generating explicit output. Although such approaches alleviate some
limitations of discrete CoT, they generally lack explicit mechanisms to enforce
consistency among reasoning steps, leading to divergent reasoning paths and
unstable outcomes. To address this issue, we propose EBM-CoT, an Energy-Based
Chain-of-Thought Calibration framework that refines latent thought
representations through an energy-based model (EBM). Our method dynamically
adjusts latent reasoning trajectories toward lower-energy, high-consistency
regions in the embedding space, improving both reasoning accuracy and
consistency without modifying the base language model. Extensive experiments
across mathematical, commonsense, and symbolic reasoning benchmarks demonstrate
that the proposed framework significantly enhances the consistency and
efficiency of multi-step reasoning in LLMs.

</details>


### [235] [LoRA on the Go: Instance-level Dynamic LoRA Selection and Merging](https://arxiv.org/abs/2511.07129)
*Seungeon Lee,Soumi Das,Manish Gupta,Krishna P. Gummadi*

Main category: cs.CL

TL;DR: LoGo是一个训练免费的框架，通过动态选择和合并LoRA适配器来处理多任务场景，无需额外训练或标注数据。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA适配器通常只能处理单一任务，在多样化的实际应用场景中受限；现有的多LoRA组合方法需要标注数据或额外训练，成本高昂。

Method: LoGo框架通过单次前向传播提取信号，在实例级别动态识别最相关适配器并实时确定其贡献权重，实现训练免费的适配器选择与合并。

Result: 在5个NLP基准测试、27个数据集和3个模型家族上，LoGo在某些任务上比基于训练的方法最高提升3.6%，在其他任务上保持竞争力，同时维持推理吞吐量。

Conclusion: LoGo证明了无需额外训练的动态适配器组合方法的有效性和实用性，为大规模多任务语言模型应用提供了高效解决方案。

Abstract: Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient approach for
fine-tuning large language models.However, conventional LoRA adapters are
typically trained for a single task, limiting their applicability in real-world
settings where inputs may span diverse and unpredictable domains. At inference
time, existing approaches combine multiple LoRAs for improving performance on
diverse tasks, while usually requiring labeled data or additional task-specific
training, which is expensive at scale. In this work, we introduce LoRA on the
Go (LoGo), a training-free framework that dynamically selects and merges
adapters at the instance level without any additional requirements. LoGo
leverages signals extracted from a single forward pass through LoRA adapters,
to identify the most relevant adapters and determine their contributions
on-the-fly. Across 5 NLP benchmarks, 27 datasets, and 3 model families, LoGo
outperforms training-based baselines on some tasks upto a margin of 3.6% while
remaining competitive on other tasks and maintaining inference throughput,
highlighting its effectiveness and practicality.

</details>


### [236] [TCM-Eval: An Expert-Level Dynamic and Extensible Benchmark for Traditional Chinese Medicine](https://arxiv.org/abs/2511.07148)
*Zihao Cheng,Yuheng Lu,Huaiqian Ye,Zeming Liu,Minqi Wang,Jingjing Liu,Zihan Li,Wei Fan,Yuanfang Guo,Ruiji Fu,Shifeng She,Gang Wang,Yunhong Wang*

Main category: cs.CL

TL;DR: 该研究提出了TCM-Eval基准测试和ZMT中医大模型，解决了LLM在中医领域缺乏标准化评估和高质量训练数据的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在现代医学表现优异，但在中医领域应用严重受限，主要原因是缺乏标准化基准评估体系和高质量训练数据稀缺。

Method: 构建TCM-Eval动态基准（源自国家医学考试）；创建大规模训练语料；提出SI-CoTE方法通过拒绝采样自动丰富推理链；训练专门的ZMT中医模型。

Result: 开发的ZMT模型在中医评估中显著超越人类执业者的通过门槛，建立了数据和模型共同进化的良性循环。

Conclusion: 发布公共排行榜促进社区参与，为中医AI研究建立了完整的评估-训练-优化生态体系，推动该领域持续发展。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
modern medicine, yet their application in Traditional Chinese Medicine (TCM)
remains severely limited by the absence of standardized benchmarks and the
scarcity of high-quality training data. To address these challenges, we
introduce TCM-Eval, the first dynamic and extensible benchmark for TCM,
meticulously curated from national medical licensing examinations and validated
by TCM experts. Furthermore, we construct a large-scale training corpus and
propose Self-Iterative Chain-of-Thought Enhancement (SI-CoTE) to autonomously
enrich question-answer pairs with validated reasoning chains through rejection
sampling, establishing a virtuous cycle of data and model co-evolution. Using
this enriched training data, we develop ZhiMingTang (ZMT), a state-of-the-art
LLM specifically designed for TCM, which significantly exceeds the passing
threshold for human practitioners. To encourage future research and
development, we release a public leaderboard, fostering community engagement
and continuous improvement.

</details>


### [237] [Categorical Emotions or Appraisals - Which Emotion Model Explains Argument Convincingness Better?](https://arxiv.org/abs/2511.07162)
*Lynn Greschner,Meike Bauer,Sabine Weber,Roman Klinger*

Main category: cs.CL

TL;DR: 本研究评估认知评估理论在论证情感分析中的应用效果，发现基于认知评估的说服力预测比传统情感分类更有效，为计算论证领域提供了新的理论和方法支持。


<details>
  <summary>Details</summary>
Motivation: 论证的说服力不仅取决于逻辑结构和论证者可信度，还取决于引起的主观情感。传统情感分析方法忽略情感的主观性，而认知评估理论能更好地连接主观认知评估与情感，但其对说服力预测的适用性尚未探索。

Method: 基于ContArgA语料库进行零样本提示实验，系统比较情感分类和认知评估两种方法，评估黄金标注和预测的情感/评估对主观说服力标签预测的影响。

Result: 情感分类信息能改善说服力预测，但认知评估的改善效果更加显著。这是首次对情感模型在说服力预测方面的系统比较研究。

Conclusion: 认知评估理论适合论证情感分析，在说服力预测方面优于传统情感分类，为计算论证的理论研究和实际应用提供了重要见解。

Abstract: The convincingness of an argument does not only depend on its structure
(logos), the person who makes the argument (ethos), but also on the emotion
that it causes in the recipient (pathos). While the overall intensity and
categorical values of emotions in arguments have received considerable
attention in the research community, we argue that the emotion an argument
evokes in a recipient is subjective. It depends on the recipient's goals,
standards, prior knowledge, and stance. Appraisal theories lend themselves as a
link between the subjective cognitive assessment of events and emotions. They
have been used in event-centric emotion analysis, but their suitability for
assessing argument convincingness remains unexplored. In this paper, we
evaluate whether appraisal theories are suitable for emotion analysis in
arguments by considering subjective cognitive evaluations of the importance and
impact of an argument on its receiver. Based on the annotations in the recently
published ContArgA corpus, we perform zero-shot prompting experiments to
evaluate the importance of gold-annotated and predicted emotions and appraisals
for the assessment of the subjective convincingness labels. We find that, while
categorical emotion information does improve convincingness prediction, the
improvement is more pronounced with appraisals. This work presents the first
systematic comparison between emotion models for convincingness prediction,
demonstrating the advantage of appraisals, providing insights for theoretical
and practical applications in computational argumentation.

</details>


### [238] [AdaRec: Adaptive Recommendation with LLMs via Narrative Profiling and Dual-Channel Reasoning](https://arxiv.org/abs/2511.07166)
*Meiyun Wang,Charin Polpanumas*

Main category: cs.CL

TL;DR: AdaRec是一个基于大语言模型的小样本上下文学习框架，通过叙事化用户画像和双变量推理范式，实现自适应个性化推荐，在少样本和零样本场景下显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有推荐系统需要大量手动特征工程、难以快速跨任务适应、以及在交互数据稀疏时表现不佳的问题，本研究提出了一个能够利用语义表征消除人工工程、支持最小监督下快速适应的自适应个性化推荐框架。

Method: AdaRec采用叙事化画像技术将用户-物品交互转换为自然语言表示，基于双变量推理范式的双通道架构：横向行为对齐发现同侪驱动的模式，纵向因果归因突出用户偏好背后的决定性因素。通过语义表征消除手动特征工程，支持最小监督下的快速跨任务适应。

Result: 在真实电商数据集上，AdaRec在少样本设置下比机器学习模型和LLM基线方法提升高达8%；在零样本场景下相比专家 crafted 画像提升19%；轻量级微调匹配完全微调模型的性能，证明了其在多样化任务上的效率和泛化能力。

Conclusion: AdaRec通过创新的叙事化画像和双变量推理架构，成功实现了在最小交互数据下的高效个性化推荐，为长尾用户和冷启动场景提供了有效解决方案，同时展现了优秀的跨任务泛化能力和计算效率。

Abstract: We propose AdaRec, a few-shot in-context learning framework that leverages
large language models for an adaptive personalized recommendation. AdaRec
introduces narrative profiling, transforming user-item interactions into
natural language representations to enable unified task handling and enhance
human readability. Centered on a bivariate reasoning paradigm, AdaRec employs a
dual-channel architecture that integrates horizontal behavioral alignment,
discovering peer-driven patterns, with vertical causal attribution,
highlighting decisive factors behind user preferences. Unlike existing
LLM-based approaches, AdaRec eliminates manual feature engineering through
semantic representations and supports rapid cross-task adaptation with minimal
supervision. Experiments on real ecommerce datasets demonstrate that AdaRec
outperforms both machine learning models and LLM-based baselines by up to eight
percent in few-shot settings. In zero-shot scenarios, it achieves up to a
nineteen percent improvement over expert-crafted profiling, showing
effectiveness for long-tail personalization with minimal interaction data.
Furthermore, lightweight fine-tuning on synthetic data generated by AdaRec
matches the performance of fully fine-tuned models, highlighting its efficiency
and generalization across diverse tasks.

</details>


### [239] [EMODIS: A Benchmark for Context-Dependent Emoji Disambiguation in Large Language Models](https://arxiv.org/abs/2511.07193)
*Jiacheng Huang,Ning Yu,Xiaoyin Yi*

Main category: cs.CL

TL;DR: 本研究提出了EMODIS基准来评估LLMs在最小但对比性的文本语境下解释模糊表情符号的能力，发现即使是最强的模型在仅有细微语境线索时也频繁失败。


<details>
  <summary>Details</summary>
Motivation: 大语言模型越来越多地部署在现实交流环境中，但它们解决上下文相关模糊性的能力尚未得到充分探索，特别是在处理表情符号等模糊表达时。

Method: 研究者创建了EMODIS基准，包含含表情符号的模糊句子、两个导致不同解释的去歧义语境，以及需要上下文推理的具体问题。评估了开源和基于API的LLMs性能。

Result: 评估发现，即使是最强的模型也经常在仅有细微语境线索时无法区分含义，揭示了对主导解释的系统性偏向和对语用对比的有限敏感性。

Conclusion: EMODIS为评估上下文去歧义提供了严格的测试平台，突出了人类和LLMs在语义推理方面的差距，表明当前模型在处理现实交流中的模糊性方面仍存在显著不足。

Abstract: Large language models (LLMs) are increasingly deployed in real-world
communication settings, yet their ability to resolve context-dependent
ambiguity remains underexplored. In this work, we present EMODIS, a new
benchmark for evaluating LLMs' capacity to interpret ambiguous emoji
expressions under minimal but contrastive textual contexts. Each instance in
EMODIS comprises an ambiguous sentence containing an emoji, two distinct
disambiguating contexts that lead to divergent interpretations, and a specific
question that requires contextual reasoning. We evaluate both open-source and
API-based LLMs, and find that even the strongest models frequently fail to
distinguish meanings when only subtle contextual cues are present. Further
analysis reveals systematic biases toward dominant interpretations and limited
sensitivity to pragmatic contrast. EMODIS provides a rigorous testbed for
assessing contextual disambiguation, and highlights the gap in semantic
reasoning between humans and LLMs.

</details>


### [240] [Who Is the Story About? Protagonist Entity Recognition in News](https://arxiv.org/abs/2511.07296)
*Jorge Gabín,M. Eduardo Ares,Javier Parapar*

Main category: cs.CL

TL;DR: 论文提出主角实体识别(PER)任务，通过LLMs识别新闻中推动叙事发展的组织实体，实验证明该方法可行且能有效近似人类判断。


<details>
  <summary>Details</summary>
Motivation: 传统命名实体识别(NER)平等对待所有实体，无法区分哪些实体真正推动叙事发展，限制了下游任务对事件显著性、影响力和叙事焦点的理解。

Method: 提出主角实体识别(PER)任务识别新闻故事中的核心组织；将LLMs预测与四位专家标注对比验证；使用最先进LLMs通过NER引导提示自动标注大规模新闻；评估LLMs在受限条件下推断主角的能力。

Result: 实验证明PER是叙事中心信息提取的可行且有意义的扩展；引导的LLMs能够大规模近似人类对叙事重要性的判断；在减少上下文和候选指导情况下，LLMs仍能正确推断主角。

Conclusion: 主角实体识别任务有效解决了传统NER的局限性，结合引导式LLM方法能够在新闻文本中准确识别推动叙事的核心实体，为理解事件重要性和叙事结构提供了新的技术路径。

Abstract: News articles often reference numerous organizations, but traditional Named
Entity Recognition (NER) treats all mentions equally, obscuring which entities
genuinely drive the narrative. This limits downstream tasks that rely on
understanding event salience, influence, or narrative focus. We introduce
Protagonist Entity Recognition (PER), a task that identifies the organizations
that anchor a news story and shape its main developments. To validate PER, we
compare he predictions of Large Language Models (LLMs) against annotations from
four expert annotators over a gold corpus, establishing both inter-annotator
consistency and human-LLM agreement. Leveraging these findings, we use
state-of-the-art LLMs to automatically label large-scale news collections
through NER-guided prompting, generating scalable, high-quality supervision. We
then evaluate whether other LLMs, given reduced context and without explicit
candidate guidance, can still infer the correct protagonists. Our results
demonstrate that PER is a feasible and meaningful extension to
narrative-centered information extraction, and that guided LLMs can approximate
human judgments of narrative importance at scale.

</details>


### [241] [Retriv at BLP-2025 Task 1: A Transformer Ensemble and Multi-Task Learning Approach for Bangla Hate Speech Identification](https://arxiv.org/abs/2511.07304)
*Sourav Saha,K M Nafi Asib,Mohammed Moshiul Hoque*

Main category: cs.CL

TL;DR: 该论文提出了基于transformer模型集成的方法来解决孟加拉语仇恨言论识别问题，在BLP Workshop的三个子任务中分别获得72.75%、72.69%和72.62%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语仇恨言论识别是一个社会影响大但语言上极具挑战性的任务，特别是在资源匮乏的环境下，需要有效的技术解决方案。

Method: 针对子任务1A和1B采用BanglaBERT、MuRIL、IndicBERTv2的软投票集成方法；对子任务1C训练三个多任务变体模型并通过加权投票集成聚合预测结果。

Result: 系统在三个子任务中分别达到72.75%（第9名）、72.69%（第10名）和72.62%（第7名）的F1分数，证明了transformer集成方法的有效性。

Conclusion: transformer模型集成和加权多任务框架在推进孟加拉语仇恨言论检测方面展现出良好前景，为低资源环境下的仇恨言论识别提供了有效解决方案。

Abstract: This paper addresses the problem of Bangla hate speech identification, a
socially impactful yet linguistically challenging task. As part of the "Bangla
Multi-task Hate Speech Identification" shared task at the BLP Workshop,
IJCNLP-AACL 2025, our team "Retriv" participated in all three subtasks: (1A)
hate type classification, (1B) target group identification, and (1C) joint
detection of type, severity, and target. For subtasks 1A and 1B, we employed a
soft-voting ensemble of transformer models (BanglaBERT, MuRIL, IndicBERTv2).
For subtask 1C, we trained three multitask variants and aggregated their
predictions through a weighted voting ensemble. Our systems achieved micro-f1
scores of 72.75% (1A) and 72.69% (1B), and a weighted micro-f1 score of 72.62%
(1C). On the shared task leaderboard, these corresponded to 9th, 10th, and 7th
positions, respectively. These results highlight the promise of transformer
ensembles and weighted multitask frameworks for advancing Bangla hate speech
detection in low-resource contexts. We made experimental scripts publicly
available for the community.

</details>


### [242] [ACE-ICD: Acronym Expansion As Data Augmentation For Automated ICD Coding](https://arxiv.org/abs/2511.07311)
*Tuan-Dung Le,Shohreh Haddadan,Thanh Q. Thieu*

Main category: cs.CL

TL;DR: 提出ACE-ICD方法，利用大语言模型进行医学缩写扩展和一致性训练，显著提升自动ICD编码性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动ICD编码方法主要关注代码层次结构和同义词，但忽视了临床笔记中普遍使用的医学缩写，这是ICD代码推断的关键因素。

Method: 提出新颖的数据增强技术：1）使用大语言模型扩展医学缩写为完整形式；2）引入一致性训练，强制原始文档与增强文档的预测结果保持一致。

Result: 在MIMIC-III数据集上的广泛实验表明，ACE-ICD在常见代码、罕见代码和完整代码分配等多个设置下均建立了新的最先进性能。

Conclusion: 通过有效利用医学缩写信息和一致性正则化，ACE-ICD为自动ICD编码任务提供了新的解决方案，代码已公开可用。

Abstract: Automatic ICD coding, the task of assigning disease and procedure codes to
electronic medical records, is crucial for clinical documentation and billing.
While existing methods primarily enhance model understanding of code
hierarchies and synonyms, they often overlook the pervasive use of medical
acronyms in clinical notes, a key factor in ICD code inference. To address this
gap, we propose a novel effective data augmentation technique that leverages
large language models to expand medical acronyms, allowing models to be trained
on their full form representations. Moreover, we incorporate consistency
training to regularize predictions by enforcing agreement between the original
and augmented documents. Extensive experiments on the MIMIC-III dataset
demonstrate that our approach, ACE-ICD establishes new state-of-the-art
performance across multiple settings, including common codes, rare codes, and
full-code assignments. Our code is publicly available.

</details>


### [243] [RLVE: Scaling Up Reinforcement Learning for Language Models with Adaptive Verifiable Environments](https://arxiv.org/abs/2511.07317)
*Zhiyuan Zeng,Hamish Ivison,Yiping Wang,Lifan Yuan,Shuyue Stella Li,Zhuorui Ye,Siting Li,Jacqueline He,Runlong Zhou,Tong Chen,Chenyang Zhao,Yulia Tsvetkov,Simon Shaolei Du,Natasha Jaques,Hao Peng,Pang Wei Koh,Hannaneh Hajishirzi*

Main category: cs.CL

TL;DR: RLVE是一种使用自适应可验证环境进行强化学习的方法，通过动态调整问题难度来提升语言模型的推理能力，在400个环境中联合训练实现了3.37%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统的静态数据分布会导致学习信号消失——当问题对策略模型来说过于简单或过于困难时，模型无法有效学习，这限制了大语言模型推理能力的规模化提升。

Method: 提出了RLVE框架，创建包含400个可验证环境的RLVE-Gym套件，每个环境能够根据模型能力动态调整问题难度分布，提供算法可验证的奖励信号。

Result: 在RLVE-Gym的400个环境中联合训练，相比最强基线模型在6个推理基准上平均提升3.37%；而继续原始RL训练仅提升0.49%，尽管使用了3倍以上的计算资源。

Conclusion: 环境扩展（增加训练环境数量）能够持续提升可泛化的推理能力，RLVE通过自适应难度调节实现了比传统RL更高效的训练效果，验证了可验证环境规模化在语言模型训练中的价值。

Abstract: We introduce Reinforcement Learning (RL) with Adaptive Verifiable
Environments (RLVE), an approach using verifiable environments that
procedurally generate problems and provide algorithmically verifiable rewards,
to scale up RL for language models (LMs). RLVE enables each verifiable
environment to dynamically adapt its problem difficulty distribution to the
policy model's capabilities as training progresses. In contrast, static data
distributions often lead to vanishing learning signals when problems are either
too easy or too hard for the policy. To implement RLVE, we create RLVE-Gym, a
large-scale suite of 400 verifiable environments carefully developed through
manual environment engineering. Using RLVE-Gym, we show that environment
scaling, i.e., expanding the collection of training environments, consistently
improves generalizable reasoning capabilities. RLVE with joint training across
all 400 environments in RLVE-Gym yields a 3.37% absolute average improvement
across six reasoning benchmarks, starting from one of the strongest 1.5B
reasoning LMs. By comparison, continuing this LM's original RL training yields
only a 0.49% average absolute gain despite using over 3x more compute. We
release our code publicly.

</details>


### [244] [When Bias Pretends to Be Truth: How Spurious Correlations Undermine Hallucination Detection in LLMs](https://arxiv.org/abs/2511.07318)
*Shaowen Wang,Yiqi Dong,Ruinian Chang,Tansheng Zhu,Yuebo Sun,Kaifeng Lyu,Jian Li*

Main category: cs.CL

TL;DR: 本文揭示了由训练数据中的伪相关性引起的LLM幻觉问题，证明现有检测方法对此类幻觉失效，并强调需要开发专门针对伪相关性的新解决方案。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型取得显著进展，但幻觉问题仍然存在。作者发现了一种由伪相关性驱动的特殊类型幻觉，这些幻觉具有高置信度、难以检测、不受模型规模影响的特点，且现有方法无法有效处理。

Method: 通过系统性控制的合成实验和对最先进开源及专有LLM（包括GPT-5）的实证评估，验证了伪相关性导致幻觉的现象，并进行了理论分析阐明现有置信度检测方法失效的根本原因。

Result: 研究表明，基于置信度的过滤和内部状态探测等现有幻觉检测方法在存在伪相关性时根本性失效。此类幻觉具有高置信度生成、免疫模型扩展、逃避当前检测方法、拒绝微调后仍然持续的特征。

Conclusion: 研究结果强调了迫切需要开发专门设计用于解决伪相关性引起的幻觉的新方法，现有技术框架在应对此类问题方面存在根本性局限。

Abstract: Despite substantial advances, large language models (LLMs) continue to
exhibit hallucinations, generating plausible yet incorrect responses. In this
paper, we highlight a critical yet previously underexplored class of
hallucinations driven by spurious correlations -- superficial but statistically
prominent associations between features (e.g., surnames) and attributes (e.g.,
nationality) present in the training data. We demonstrate that these spurious
correlations induce hallucinations that are confidently generated, immune to
model scaling, evade current detection methods, and persist even after refusal
fine-tuning. Through systematically controlled synthetic experiments and
empirical evaluations on state-of-the-art open-source and proprietary LLMs
(including GPT-5), we show that existing hallucination detection methods, such
as confidence-based filtering and inner-state probing, fundamentally fail in
the presence of spurious correlations. Our theoretical analysis further
elucidates why these statistical biases intrinsically undermine
confidence-based detection techniques. Our findings thus emphasize the urgent
need for new approaches explicitly designed to address hallucinations caused by
spurious correlations.

</details>


### [245] [FinRpt: Dataset, Evaluation System and LLM-based Multi-agent Framework for Equity Research Report Generation](https://arxiv.org/abs/2511.07322)
*Song Jin,Shuqi Li,Shukun Zhang,Rui Yan*

Main category: cs.CL

TL;DR: 本文首次提出Equity Research Report Generation任务，创建开源评估基准FinRpt，开发了多智能体框架FinRpt-Gen，实验证明了其数据质量、指标有效性和模型性能。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在股票预测和问答等金融任务中表现优异，但在完全自动化Equity Research Report生成领域仍是空白，存在数据稀缺和评估指标缺失两大挑战。

Method: 提出ERR Generation任务定义；构建集成7种金融数据类型的Dataset Construction Pipeline；设计包含11个指标的comprehensive evaluation system；开发多智能体框架FinRpt-Gen，使用Supervised Fine-Tuning和Reinforcement Learning训练LLM智能体。

Result: 实验结果表明FinRpt基准具有良好的数据质量和有效的评估指标，FinRpt-Gen模型展现出强大的性能，证明了该方法在ERR生成领域的创新潜力。

Conclusion: 本研究为Equity Research Report自动化生成奠定了基础，FinRpt基准和FinRpt-Gen框架为该领域的发展提供了重要工具和参考，有望推动金融文本生成技术的创新。

Abstract: While LLMs have shown great success in financial tasks like stock prediction
and question answering, their application in fully automating Equity Research
Report generation remains uncharted territory. In this paper, we formulate the
Equity Research Report (ERR) Generation task for the first time. To address the
data scarcity and the evaluation metrics absence, we present an open-source
evaluation benchmark for ERR generation - FinRpt. We frame a Dataset
Construction Pipeline that integrates 7 financial data types and produces a
high-quality ERR dataset automatically, which could be used for model training
and evaluation. We also introduce a comprehensive evaluation system including
11 metrics to assess the generated ERRs. Moreover, we propose a multi-agent
framework specifically tailored to address this task, named FinRpt-Gen, and
train several LLM-based agents on the proposed datasets using Supervised
Fine-Tuning and Reinforcement Learning. Experimental results indicate the data
quality and metrics effectiveness of the benchmark FinRpt and the strong
performance of FinRpt-Gen, showcasing their potential to drive innovation in
the ERR generation field. All code and datasets are publicly available.

</details>


### [246] [Selecting Auxiliary Data via Neural Tangent Kernels for Low-Resource Domains](https://arxiv.org/abs/2511.07380)
*Pingjie Wang,Hongcheng Liu,Yusheng Liao,Ziqing Fan,Yaxin Du,Shuo Tang,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: NTK-Selector是一种基于神经正切核的框架，能有效选择通用领域辅助数据来增强低资源领域的大语言模型性能，在医疗、金融、法律、心理等四个领域实现了显著提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在低资源领域面临数据稀缺和过拟合风险。虽然领域内数据有限，但存在大量相似的通用领域数据可作为辅助监督，关键问题是如何在没有大型领域内数据池或验证集的情况下有效选择最有价值的辅助数据。

Method: 提出NTK-Selector框架，利用神经正切核选择通用领域辅助数据。通过实证展示LLM在LoRA微调期间具有稳定的类NTK行为，并提出雅可比无关的近似方法来解决理论假设和高计算成本的挑战。

Result: 在四个低资源领域实验中，仅用1000个领域内样本微调仅获得+0.8和+0.9分提升；而使用NTK-Selector选择的9000个辅助样本可实现+8.7和+5.1分的显著提升，相当于10.9倍和5.7倍的改进。

Conclusion: NTK-Selector成功解决了低资源场景下辅助数据选择的核心问题，通过理论创新和高效实现显著提升了大语言模型在专业领域的表现，为资源受限的领域适应提供了有效解决方案。

Abstract: Large language models (LLMs) have achieved remarkable success across
widespread tasks, yet their application in low-resource domains remains a
significant challenge due to data scarcity and the high risk of overfitting.
While in-domain data is limited, there exist vast amounts of similar
general-domain data, and our initial findings reveal that they could
potentially serve as auxiliary supervision for domain enhancement. This
observation leads us to our central research question: \textbf{\textit{how to
effectively select the most valuable auxiliary data to maximize domain-specific
performance}}, particularly when traditional methods are inapplicable due to a
lack of large in-domain data pools or validation sets. To address this, we
propose \textbf{NTK-Selector}, a principled and efficient framework for
selecting general-domain auxiliary data to enhance domain-specific performance
via neural tangent kernels (NTK). Our method tackles two challenges of directly
applying NTK to LLMs, theoretical assumptions and prohibitive computational
cost, by empirically demonstrating a stable NTK-like behavior in LLMs during
LoRA fine-tuning and proposing a Jacobian-free approximation method. Extensive
experiments across four low-resource domains (medical, financial, legal, and
psychological) demonstrate that NTK-Selector consistently improves downstream
performance. Specifically, fine-tuning on 1,000 in-domain samples alone only
yielded +0.8 points for Llama3-8B-Instruct and +0.9 points for Qwen3-8B. In
contrast, enriching with 9,000 auxiliary samples selected by NTK-Selector led
to substantial \textbf{gains of +8.7 and +5.1 points}, which corresponds to a
\textbf{10.9x and 5.7x improvement} over the domain-only setting.

</details>


### [247] [Retriv at BLP-2025 Task 2: Test-Driven Feedback-Guided Framework for Bangla-to-Python Code Generation](https://arxiv.org/abs/2511.07382)
*K M Nafi Asib,Sourav Saha,Mohammed Moshiul Hoque*

Main category: cs.CL

TL;DR: 本文提出了一种结合指令提示和测试驱动反馈引导迭代优化的方法，使用微调的Qwen2.5-14B模型，在孟加拉语代码生成任务中获得第二名，Pass@1分数为0.934。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言（如孟加拉语）在代码生成任务中因缺乏指令到代码数据集和评估基准而表现不佳的问题。

Method: 采用微调的Qwen2.5-14B模型，结合指令提示和测试驱动的反馈引导迭代优化过程，通过三次评估测试来改进失败的代码输出。

Result: 团队"Retriv"在BLP Workshop的共享任务中获得第二名，Pass@1分数达到0.934。

Conclusion: 该方法有效提升了孟加拉语代码生成效果，突出了在低资源语言中开发针对性方法的重要性，并公开了实验脚本供社区使用。

Abstract: Large Language Models (LLMs) have advanced the automated generation of code
from natural language prompts. However, low-resource languages (LRLs) like
Bangla remain underrepresented due to the limited availability of
instruction-to-code datasets and evaluation benchmarks. To address this, the
BLP Workshop at IJCNLP-AACL 2025 introduced a shared task on "Code Generation
in Bangla". In this work, we propose a method that combines instruction
prompting with a test-driven, feedback-guided iterative refinement process
using a fine-tuned Qwen2.5-14B model. The model generates code from Bangla
instructions, tests it against unit tests, and iteratively refines any failing
outputs through three evaluation passes, using test feedback to guide each
step. This approach helped our team "Retriv" to secure 2nd place in the shared
task with a Pass@1 score of 0.934. The analysis highlights challenges in Bangla
instruction understanding and Python code generation, emphasizing the need for
targeted methods in LRLs. We made experimental scripts publicly available for
the community.

</details>


### [248] [Teaching Pretrained Language Models to Think Deeper with Retrofitted Recurrence](https://arxiv.org/abs/2511.07384)
*Sean McLeish,Ang Li,John Kirchenbauer,Dayal Singh Kalra,Brian R. Bartoldson,Bhavya Kailkhura,Avi Schwarzschild,Jonas Geiping,Tom Goldstein,Micah Goldblum*

Main category: cs.CL

TL;DR: 研究如何将预训练非循环语言模型转换为深度循环模型，通过递增课程保持性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 深度循环语言模型能解耦训练时和测试时的计算，但如何有效转换现有预训练模型是关键问题。

Method: 使用递增课程的循环策略逐步增加模型有效深度，将预训练非循环模型转换为循环模型。

Result: 数学任务实验显示，在相同计算预算下，转换的循环模型性能优于原模型的后训练版本。

Conclusion: 递增课程的循环转换方法能够在保持性能的同时显著降低计算成本，是高效利用预训练模型的有效途径。

Abstract: Recent advances in depth-recurrent language models show that recurrence can
decouple train-time compute and parameter count from test-time compute. In this
work, we study how to convert existing pretrained non-recurrent language models
into depth-recurrent models. We find that using a curriculum of recurrences to
increase the effective depth of the model over the course of training preserves
performance while reducing total computational cost. In our experiments, on
mathematics, we observe that converting pretrained models to recurrent ones
results in better performance at a given compute budget than simply
post-training the original non-recurrent language model.

</details>


### [249] [SPOT: An Annotated French Corpus and Benchmark for Detecting Critical Interventions in Online Conversations](https://arxiv.org/abs/2511.07405)
*Manon Berriche,Célia Nouri,Chloé Clavel,Jean-Philippe Cointet*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce SPOT (Stopping Points in Online Threads), the first annotated
corpus translating the sociological concept of stopping point into a
reproducible NLP task. Stopping points are ordinary critical interventions that
pause or redirect online discussions through a range of forms (irony, subtle
doubt or fragmentary arguments) that frameworks like counterspeech or social
correction often overlook. We operationalize this concept as a binary
classification task and provide reliable annotation guidelines. The corpus
contains 43,305 manually annotated French Facebook comments linked to URLs
flagged as false information by social media users, enriched with contextual
metadata (article, post, parent comment, page or group, and source). We
benchmark fine-tuned encoder models (CamemBERT) and instruction-tuned LLMs
under various prompting strategies. Results show that fine-tuned encoders
outperform prompted LLMs in F1 score by more than 10 percentage points,
confirming the importance of supervised learning for emerging non-English
social media tasks. Incorporating contextual metadata further improves encoder
models F1 scores from 0.75 to 0.78. We release the anonymized dataset, along
with the annotation guidelines and code in our code repository, to foster
transparency and reproducible research.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [250] [From Prompts to Power: Measuring the Energy Footprint of LLM Inference](https://arxiv.org/abs/2511.05597)
*Francisco Caravaca,Ángel Cuevas,Rubén Cuevas*

Main category: cs.AI

TL;DR: 这是一项大规模测量研究，分析了超过32,500次GPU上LLM推理的能量消耗，并开发了预测模型和浏览器扩展工具来提高对生成AI环境影响的认知。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速扩张，推理工作负载带来了前所未有的能源需求，甚至需要用核能为数据中心供电，但系统性的推理能耗分析仍然有限。

Method: 通过21种GPU配置和155种模型架构进行超过32,500次测量，使用vLLM推理引擎在提示级别量化能量使用，分析架构和运营因素对能源需求的影响。

Result: 识别了影响推理能耗的关键因素，开发出能准确预测未知架构和硬件能耗的预测模型，并实现了浏览器扩展工具。

Conclusion: 该研究为理解LLM推理的能源消耗提供了系统性分析框架，通过测量驱动的洞察和预测工具，帮助提高对生成AI环境影响的认识。

Abstract: The rapid expansion of Large Language Models (LLMs) has introduced
unprecedented energy demands, extending beyond training to large-scale
inference workloads that often dominate total lifecycle consumption. Deploying
these models requires energy-intensive GPU infrastructure, and in some cases
has even prompted plans to power data centers with nuclear energy. Despite this
growing relevance, systematic analyses of inference energy consumption remain
limited. In this work, we present a large-scale measurement-based study
comprising over 32,500 measurements across 21 GPU configurations and 155 model
architectures, from small open-source models to frontier systems. Using the
vLLM inference engine, we quantify energy usage at the prompt level and
identify how architectural and operational factors shape energy demand.
Building on these insights, we develop a predictive model that accurately
estimates inference energy consumption across unseen architectures and
hardware, and implement it as a browser extension to raise awareness of the
environmental impact of generative AI.

</details>


### [251] [CoT-X: An Adaptive Framework for Cross-Model Chain-of-Thought Transfer and Optimization](https://arxiv.org/abs/2511.05747)
*Ziqian Bi,Kaijie Chen,Tianyang Wang,Junfeng Hao,Xinyuan Song*

Main category: cs.AI

TL;DR: 本文提出了一种自适应推理总结框架，通过语义分割、动态压缩和连贯重建来压缩思维链推理轨迹，在保证推理质量的同时显著减少token使用，实现跨模型的高效推理迁移。


<details>
  <summary>Details</summary>
Motivation: 思维链推理虽然提升了大语言模型的问题解决能力，但带来了巨大的推理开销，限制了在资源受限环境中的应用部署。需要找到一种方法既能保持CoT的推理优势，又能降低计算成本。

Method: 提出自适应推理总结框架，包括：1）基于语义分割和重要性评分的推理轨迹压缩；2）预算感知的动态压缩机制；3）连贯性重建技术；4）高斯过程贝叶斯优化模块用于减少评估成本。

Result: 在7,501个医学考试问题上比截断方法提升40%准确率；在8个LLMs的64个模型对上验证了强大跨模型迁移性；贝叶斯优化减少84%评估成本；发现模型规模与跨领域鲁棒性间的幂律关系。

Conclusion: 推理总结为高效的CoT迁移提供了实用路径，能够在严格的计算约束下实现先进推理能力，为资源受限环境部署大模型推理提供了可行解决方案。

Abstract: Chain-of-Thought (CoT) reasoning enhances the problem-solving ability of
large language models (LLMs) but leads to substantial inference overhead,
limiting deployment in resource-constrained settings. This paper investigates
efficient CoT transfer across models of different scales and architectures
through an adaptive reasoning summarization framework. The proposed method
compresses reasoning traces via semantic segmentation with importance scoring,
budget-aware dynamic compression, and coherence reconstruction, preserving
critical reasoning steps while significantly reducing token usage. Experiments
on 7{,}501 medical examination questions across 10 specialties show up to 40%
higher accuracy than truncation under the same token budgets. Evaluations on 64
model pairs from eight LLMs (1.5B-32B parameters, including DeepSeek-R1 and
Qwen3) confirm strong cross-model transferability. Furthermore, a Gaussian
Process-based Bayesian optimization module reduces evaluation cost by 84% and
reveals a power-law relationship between model size and cross-domain
robustness. These results demonstrate that reasoning summarization provides a
practical path toward efficient CoT transfer, enabling advanced reasoning under
tight computational constraints. Code will be released upon publication.

</details>


### [252] [Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs](https://arxiv.org/abs/2511.05766)
*Felipe Valencia-Clavijo*

Main category: cs.AI

TL;DR: 研究发现LLMs确实存在锚定偏差，通过对数概率分析和归因方法证明这种偏差是稳健的、可测量的，但会根据模型规模和提示设计而变化。


<details>
  <summary>Details</summary>
Motivation: 当前不确定LLMs表现出的认知偏差是表面模仿还是深层概率转移的结果。锚定偏差作为经典的人类判断偏差，为研究LLMs的内部机制提供了关键测试案例。

Method: 论文通过三个贡献推进研究：(1)基于对数概率的行为分析，证明锚点会改变整个输出分布并控制训练数据污染；(2)对结构化提示字段进行精确Shapley值归因，量化锚点对模型对数概率的影响；(3)整合行为和归因证据的统一锚定偏差敏感性评分系统。

Result: Gemma-2B、Phi-2和Llama-2-7B等较大模型显示稳健的锚定效应，归因分析表明锚点影响权重重新分配；GPT-2等较小模型表现不稳定，暗示模型规模可能调节敏感性。但归因效应在不同提示设计中变化显著。

Conclusion: LLMs中的锚定偏差是稳健、可测量且可解释的，但提示设计的脆弱性凸显了将LLMs视为人类替代品的风险。该框架连接了行为科学、LLM安全性和可解释性，为评估其他认知偏差提供了可复制的路径。

Abstract: Large language models (LLMs) are increasingly examined as both behavioral
subjects and decision systems, yet it remains unclear whether observed
cognitive biases reflect surface imitation or deeper probability shifts.
Anchoring bias, a classic human judgment bias, offers a critical test case.
While prior work shows LLMs exhibit anchoring, most evidence relies on
surface-level outputs, leaving internal mechanisms and attributional
contributions unexplored. This paper advances the study of anchoring in LLMs
through three contributions: (1) a log-probability-based behavioral analysis
showing that anchors shift entire output distributions, with controls for
training-data contamination; (2) exact Shapley-value attribution over
structured prompt fields to quantify anchor influence on model
log-probabilities; and (3) a unified Anchoring Bias Sensitivity Score
integrating behavioral and attributional evidence across six open-source
models. Results reveal robust anchoring effects in Gemma-2B, Phi-2, and
Llama-2-7B, with attribution signaling that the anchors influence reweighting.
Smaller models such as GPT-2, Falcon-RW-1B, and GPT-Neo-125M show variability,
suggesting scale may modulate sensitivity. Attributional effects, however, vary
across prompt designs, underscoring fragility in treating LLMs as human
substitutes. The findings demonstrate that anchoring bias in LLMs is robust,
measurable, and interpretable, while highlighting risks in applied domains.
More broadly, the framework bridges behavioral science, LLM safety, and
interpretability, offering a reproducible path for evaluating other cognitive
biases in LLMs.

</details>


### [253] [DiagnoLLM: A Hybrid Bayesian Neural Language Framework for Interpretable Disease Diagnosis](https://arxiv.org/abs/2511.05810)
*Bowen Xu,Xinyue Zeng,Jiazhen Hu,Tuo Wang,Adithya Kulkarni*

Main category: cs.AI

TL;DR: DiagnoLLM是一个混合框架，结合贝叶斯解卷积、eQTL引导的深度学习和LLM叙事生成，用于可解释的疾病诊断，在阿尔茨海默病检测中达到88.0%准确率，并生成针对医生和患者的诊断报告。


<details>
  <summary>Details</summary>
Motivation: 构建可信赖的临床AI系统不仅需要准确的预测，还需要透明且具有生物学基础的解释。现有方法缺乏可解释性和生物学依据。

Method: 1) GP-unmix：基于高斯过程的层次模型，从批量单细胞RNA-seq数据推断细胞类型特异性基因表达谱；2) 结合eQTL分析得到的调控先验，训练神经分类器；3) LLM推理模块将模型输出转换为针对不同受众的诊断报告。

Result: 在阿尔茨海默病检测中达到88.0%准确率，生成的诊断报告被评估为准确、可操作且适合医生和患者。

Conclusion: LLM作为事后推理器而非端到端预测器时，可以在混合诊断管道中作为有效的沟通工具，为构建可信赖的临床AI系统提供了新思路。

Abstract: Building trustworthy clinical AI systems requires not only accurate
predictions but also transparent, biologically grounded explanations. We
present \texttt{DiagnoLLM}, a hybrid framework that integrates Bayesian
deconvolution, eQTL-guided deep learning, and LLM-based narrative generation
for interpretable disease diagnosis. DiagnoLLM begins with GP-unmix, a Gaussian
Process-based hierarchical model that infers cell-type-specific gene expression
profiles from bulk and single-cell RNA-seq data while modeling biological
uncertainty. These features, combined with regulatory priors from eQTL
analysis, power a neural classifier that achieves high predictive performance
in Alzheimer's Disease (AD) detection (88.0\% accuracy). To support human
understanding and trust, we introduce an LLM-based reasoning module that
translates model outputs into audience-specific diagnostic reports, grounded in
clinical features, attribution signals, and domain knowledge. Human evaluations
confirm that these reports are accurate, actionable, and appropriately tailored
for both physicians and patients. Our findings show that LLMs, when deployed as
post-hoc reasoners rather than end-to-end predictors, can serve as effective
communicators within hybrid diagnostic pipelines.

</details>


### [254] [Can a Small Model Learn to Look Before It Leaps? Dynamic Learning and Proactive Correction for Hallucination Detection](https://arxiv.org/abs/2511.05854)
*Zepeng Bao,Shen Zhou,Qiankun Pi,Jianhao Chen,Mayi Xu,Ming Zhong,Yuanyuan Zhu,Tieyun Qian*

Main category: cs.AI

TL;DR: LEAP框架通过动态策略学习和师生蒸馏机制，使小模型能够自适应地学习和修正幻觉检测策略，解决了固定验证策略的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有工具增强的幻觉检测方法依赖预定义的固定验证策略，在动态变化的执行环境中缺乏适应性，导致检测失败；而使用闭源大模型成本过高，师生架构的轻量化方法又受限于固定策略。

Method: 提出LEAP框架，将幻觉检测建模为动态策略学习问题：1) 教师模型在动态学习循环中生成轨迹并根据执行失败调整策略；2) 通过agent tuning将动态规划能力蒸馏到高效的学生模型；3) 学生模型在策略执行时采用主动修正机制，在执行前提出、审查和优化验证策略。

Result: 在三个具有挑战性的基准测试上，LEAP调优模型的性能超过了现有最先进方法，证明了动态策略学习和主动修正机制的有效性。

Conclusion: LEAP框架成功地将教师模型的动态学习和主动修正能力转移到高效的学生模型中，为解决幻觉检测中固定策略适应性问题提供了一个创新且成本效益高的解决方案。

Abstract: Hallucination in large language models (LLMs) remains a critical barrier to
their safe deployment. Existing tool-augmented hallucination detection methods
require pre-defined fixed verification strategies, which are crucial to the
quality and effectiveness of tool calls. Some methods directly employ powerful
closed-source LLMs such as GPT-4 as detectors, which are effective but too
costly. To mitigate the cost issue, some methods adopt the teacher-student
architecture and finetune open-source small models as detectors via agent
tuning. However, these methods are limited by fixed strategies. When faced with
a dynamically changing execution environment, they may lack adaptability and
inappropriately call tools, ultimately leading to detection failure. To address
the problem of insufficient strategy adaptability, we propose the innovative
``Learning to Evaluate and Adaptively Plan''(LEAP) framework, which endows an
efficient student model with the dynamic learning and proactive correction
capabilities of the teacher model. Specifically, our method formulates the
hallucination detection problem as a dynamic strategy learning problem. We
first employ a teacher model to generate trajectories within the dynamic
learning loop and dynamically adjust the strategy based on execution failures.
We then distill this dynamic planning capability into an efficient student
model via agent tuning. Finally, during strategy execution, the student model
adopts a proactive correction mechanism, enabling it to propose, review, and
optimize its own verification strategies before execution. We demonstrate
through experiments on three challenging benchmarks that our LEAP-tuned model
outperforms existing state-of-the-art methods.

</details>


### [255] [An Empirical Study of Reasoning Steps in Thinking Code LLMs](https://arxiv.org/abs/2511.05874)
*Haoran Xue,Gias Uddin,Song Wang*

Main category: cs.AI

TL;DR: 本研究对6个最先进的思维LLMs在代码生成中的推理过程进行了全面实证分析，发现推理链完整性是主要失效模式，任务复杂性显著影响推理质量，且适当调整步骤数量可优化解决率。


<details>
  <summary>Details</summary>
Motivation: 思维LLMs在代码生成时会产生显式的中间推理轨迹，可能提高透明度、可解释性和解决方案准确性，但这些推理链的质量尚未得到充分探索，需要系统性研究来了解其优势和局限性。

Method: 评估6个SOTA推理LLMs在BigCodeBench 100个代码生成任务上的表现，通过步骤计数和冗长度量化推理链结构，进行步骤预算调整实验，并组织21名参与者从效率、逻辑正确性和完整性三个维度进行人工评估。

Result: 研究发现推理链完整性是主要失效模式，困难任务比标准任务更容易不完整；目标化增加步骤可提高解决率；思维LLMs在不同计算水平下保持一致逻辑结构并能自我纠错；建立了推理问题分类法。

Conclusion: 该研究为当前思维LLMs在软件工程中的优势和局限性提供了新见解，为理解和改进这些模型的推理能力奠定了基础，对代码生成领域的发展具有重要指导意义。

Abstract: Thinking Large Language Models (LLMs) generate explicit intermediate
reasoning traces before final answers, potentially improving transparency,
interpretability, and solution accuracy for code generation. However, the
quality of these reasoning chains remains underexplored. We present a
comprehensive empirical study examining the reasoning process and quality of
thinking LLMs for code generation. We evaluate six state-of-the-art reasoning
LLMs (DeepSeek-R1, OpenAI-o3-mini, Claude-3.7-Sonnet-Thinking,
Gemini-2.0-Flash-Thinking, Gemini-2.5-Flash, and Qwen-QwQ) across 100 code
generation tasks of varying difficulty from BigCodeBench. We quantify
reasoning-chain structure through step counts and verbosity, conduct controlled
step-budget adjustments, and perform a 21-participant human evaluation across
three dimensions: efficiency, logical correctness, and completeness. Our
step-count interventions reveal that targeted step increases can improve
resolution rates for certain models/tasks, while modest reductions often
preserve success on standard tasks, rarely on hard ones. Through systematic
analysis, we develop a reasoning-problematic taxonomy, identifying completeness
as the dominant failure mode. Task complexity significantly impacts reasoning
quality; hard problems are substantially more prone to incompleteness than
standard tasks. Our stability analysis demonstrates that thinking LLMs maintain
consistent logical structures across computational effort levels and can
self-correct previous errors. This study provides new insights into the
strengths and limitations of current thinking LLMs in software engineering.

</details>


### [256] [Unveiling Modality Bias: Automated Sample-Specific Analysis for Multimodal Misinformation Benchmarks](https://arxiv.org/abs/2511.05883)
*Hehai Lin,Hui Liu,Shilei Cao,Jing Li,Haoliang Li,Wenya Wang*

Main category: cs.AI

TL;DR: 本文提出了三种不同粒度的自动化方法来识别多模态虚假信息中的模态偏见：粗粒度的模态效益评估、中等粒度的信息流量化和细粒度的因果关系分析。通过人工验证发现，集成多重视角对可靠分析至关重要，自动化分析容易受到检测器波动影响，不同方法在平衡样本上更一致，但在有偏样本上存在分歧。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态虚假信息基准测试存在对特定模态的偏见，使得检测器可以仅基于单一模态进行预测。以往的研究要么在数据集层面量化偏见，要么手动识别模态与标签之间的虚假相关性，这些方法缺乏样本层面的有意义的洞见，且难以扩展到海量在线信息。

Method: 提出了三种基于不同粒度理论/观点的偏见量化方法：1) 粗粒度的模态效益评估；2) 中等粒度的信息流量化；3) 细粒度的因果关系分析。通过在两个流行基准测试上进行人工评估来验证方法的有效性。

Result: 实验结果揭示了三个有趣的发现：1) 集成多个视角对可靠的自动化分析至关重要；2) 自动化分析容易受到检测器引起的波动影响；3) 不同视角在模态平衡样本上产生更高的协议，但在有偏样本上存在分歧。

Conclusion: 这项工作为未来研究提供了潜在方向，特别是在自动化识别模态偏见方面。研究表明需要综合考虑多个粒度的视角来获得可靠的分析结果，并强调了检测器稳定性对自动化分析的影响。

Abstract: Numerous multimodal misinformation benchmarks exhibit bias toward specific
modalities, allowing detectors to make predictions based solely on one
modality. While previous research has quantified bias at the dataset level or
manually identified spurious correlations between modalities and labels, these
approaches lack meaningful insights at the sample level and struggle to scale
to the vast amount of online information. In this paper, we investigate the
design for automated recognition of modality bias at the sample level.
Specifically, we propose three bias quantification methods based on
theories/views of different levels of granularity: 1) a coarse-grained
evaluation of modality benefit; 2) a medium-grained quantification of
information flow; and 3) a fine-grained causality analysis. To verify the
effectiveness, we conduct a human evaluation on two popular benchmarks.
Experimental results reveal three interesting findings that provide potential
direction toward future research: 1)~Ensembling multiple views is crucial for
reliable automated analysis; 2)~Automated analysis is prone to detector-induced
fluctuations; and 3)~Different views produce a higher agreement on
modality-balanced samples but diverge on biased ones.

</details>


### [257] [Klear-AgentForge: Forging Agentic Intelligence through Posttraining Scaling](https://arxiv.org/abs/2511.05951)
*Qi Wang,Hongzhi Zhang,Jia Fu,Kai Fu,Yahui Liu,Tinghai Zhang,Chenxi Sun,Gangwei Jiang,Jingyi Tang,Xingguang Ji,Yang Yue,Jingyuan Zhang,Fuzheng Zhang,Kun Gai,Guorui Zhou*

Main category: cs.AI

TL;DR: 开发了名为Klear-Qwen3-AgentForge的完全开源训练管道，从Qwen3-8B基础模型出发，通过合成数据监督微调和多轮强化学习训练高性能智能体模型，在同等规模模型中达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有强大的智能体模型缺乏关键的后训练细节，阻碍了开源社区开发对应的高性能模型。

Method: 提出完全开源的Klear-Qwen3-AgentForge训练管道：从Qwen3-8B基础模型开始，设计有效的合成数据监督微调(SFT)，随后进行多轮强化学习(RL)以解锁多种多样化智能体任务的潜力。

Result: Klear-Qwen3-AgentForge-8B在各种智能体基准测试(工具使用和编码领域)中，在同等规模模型中达到最先进性能，并与明显更大的模型保持竞争力。

Conclusion: 该研究为开源社区提供了完整的智能体模型训练方案，证明了通过精心设计的训练管道，较小的基础模型也能达到与大型模型相当的智能体能力。

Abstract: Despite the proliferation of powerful agentic models, the lack of critical
post-training details hinders the development of strong counterparts in the
open-source community. In this study, we present a comprehensive and fully
open-source pipeline for training a high-performance agentic model for
interacting with external tools and environments, named Klear-Qwen3-AgentForge,
starting from the Qwen3-8B base model. We design effective supervised
fine-tuning (SFT) with synthetic data followed by multi-turn reinforcement
learning (RL) to unlock the potential for multiple diverse agentic tasks. We
perform exclusive experiments on various agentic benchmarks in both tool use
and coding domains. Klear-Qwen3-AgentForge-8B achieves state-of-the-art
performance among LLMs of similar size and remains competitive with
significantly larger models.

</details>


### [258] [An Epistemic Perspective on Agent Awareness](https://arxiv.org/abs/2511.05977)
*Pavel Naumov,Alexandra Pavlova*

Main category: cs.AI

TL;DR: 本文提出将智能体意识视为一种知识形式，通过区分de re和de dicto两种知识形式，引入相应的模态词，并使用2D语义学进行形式化规范，最终构建了一个可靠且完备的逻辑系统。


<details>
  <summary>Details</summary>
Motivation: 打破现有意识研究传统，将智能体意识从认知概念转化为知识形式，通过区分de re（关于对象的知识）和de dicto（关于命题的知识）两种形式来更好地理解智能体的认知状态。

Method: 引入两种新的模态词来分别捕获de re和de dicto形式的意识知识，采用2D语义学对这些模态词的含义进行形式化规范，构建逻辑系统描述这些模态词与标准"事实知识"模态词之间的相互作用。

Result: 成功建立了一个可靠且完备的逻辑系统，能够准确描述两种意识模态与标准知识模态之间的相互关系，为智能体意识的逻辑分析提供了严格的数学基础。

Conclusion: 该研究为智能体意识理论提供了新的视角和方法论基础，通过逻辑形式化的方式建立了更为精确的意识分析框架，对人工智能和认知逻辑领域具有重要意义。

Abstract: The paper proposes to treat agent awareness as a form of knowledge, breaking
the tradition in the existing literature on awareness. It distinguishes the de
re and de dicto forms of such knowledge. The work introduces two modalities
capturing these forms and formally specifies their meaning using a version of
2D-semantics. The main technical result is a sound and complete logical system
describing the interplay between the two proposed modalities and the standard
"knowledge of the fact" modality.

</details>


### [259] [When Object-Centric World Models Meet Policy Learning: From Pixels to Policies, and Where It Breaks](https://arxiv.org/abs/2511.06136)
*Stefano Ferraro,Akihiro Nakano,Masahiro Suzuki,Yutaka Matsuo*

Main category: cs.AI

TL;DR: DLPWM是一个无监督的目标中心世界模型，虽然在视觉重建和预测上表现优秀且对OOD变化鲁棒，但在下游控制任务中性能不如DreamerV3，主要原因是多目标交互过程中的潜在漂移问题。


<details>
  <summary>Details</summary>
Motivation: 目标中心世界模型(OCWM)旨在将视觉场景分解为目标级表示，通过提供结构化抽象来改善强化学习中的组合泛化和数据效率。研究者假设，通过定位任务相关信息，明确解纠缠的目标级表示可以提升策略在新型特征组合上的性能。

Method: 提出了DLPWM，一个完全无监督、解纠缠的目标中心世界模型，直接从像素中学习目标级潜在表示。通过潜在轨迹分析，研究多目标交互过程中的表示变化。

Result: DLPWM在重建和预测性能上表现强劲，对多种分布外(OOD)视觉变化具有鲁棒性。然而，用于下游基于模型的控制时，在DLPWM潜在空间上训练的策略性能不如DreamerV3。发现多目标交互过程中的表示漂移是不稳定策略学习的关键驱动因素。

Conclusion: 虽然目标中心感知支持鲁棒的视觉建模，但要实现稳定控制需要减轻潜在漂移问题，这表明视觉表示能力和控制稳定性之间存在关键挑战。

Abstract: Object-centric world models (OCWM) aim to decompose visual scenes into
object-level representations, providing structured abstractions that could
improve compositional generalization and data efficiency in reinforcement
learning. We hypothesize that explicitly disentangled object-level
representations, by localizing task-relevant information, can enhance policy
performance across novel feature combinations. To test this hypothesis, we
introduce DLPWM, a fully unsupervised, disentangled object-centric world model
that learns object-level latents directly from pixels. DLPWM achieves strong
reconstruction and prediction performance, including robustness to several
out-of-distribution (OOD) visual variations. However, when used for downstream
model-based control, policies trained on DLPWM latents underperform compared to
DreamerV3. Through latent-trajectory analyses, we identify representation shift
during multi-object interactions as a key driver of unstable policy learning.
Our results suggest that, although object-centric perception supports robust
visual modeling, achieving stable control requires mitigating latent drift.

</details>


### [260] [Evaluating Implicit Biases in LLM Reasoning through Logic Grid Puzzles](https://arxiv.org/abs/2511.06160)
*Fatima Jahara,Mark Dredze,Sharon Levy*

Main category: cs.AI

TL;DR: PRIME是一个新的评估框架，使用逻辑网格谜题系统性地检测大语言模型在复杂逻辑推理任务中的隐式社会偏见。研究发现模型在解决方案符合刻板印象时推理更准确。


<details>
  <summary>Details</summary>
Motivation: 现有的安全防护措施能有效抑制明显的偏见输出，但在复杂的逻辑推理任务中，更微妙的社会偏见形式会避开当前的评估基准，因此需要新的评估方法来填补这一空白。

Method: 提出了PRIME评估框架，利用逻辑网格谜题来探测社会刻板印象对LLM逻辑推理和决策的影响。该框架能自动生成和验证谜题，并包含刻板印象、反刻板印象和中性三种变体，实现可控的细粒度比较。

Result: 针对性别刻板印象的实验表明，模型在解决方案与刻板印象关联一致时推理更准确，这证实了LLM在演绎推理中确实存在社会偏见。

Conclusion: PRIME框架对于诊断和量化LLM在演绎推理中持续存在的社会偏见具有重要意义，特别是在需要公平性的关键场景中。该框架为评估和减轻AI系统中的隐式偏见提供了新的有效工具。

Abstract: While recent safety guardrails effectively suppress overtly biased outputs,
subtler forms of social bias emerge during complex logical reasoning tasks that
evade current evaluation benchmarks. To fill this gap, we introduce a new
evaluation framework, PRIME (Puzzle Reasoning for Implicit Biases in Model
Evaluation), that uses logic grid puzzles to systematically probe the influence
of social stereotypes on logical reasoning and decision making in LLMs. Our use
of logic puzzles enables automatic generation and verification, as well as
variability in complexity and biased settings. PRIME includes stereotypical,
anti-stereotypical, and neutral puzzle variants generated from a shared puzzle
structure, allowing for controlled and fine-grained comparisons. We evaluate
multiple model families across puzzle sizes and test the effectiveness of
prompt-based mitigation strategies. Focusing our experiments on gender
stereotypes, our findings highlight that models consistently reason more
accurately when solutions align with stereotypical associations. This
demonstrates the significance of PRIME for diagnosing and quantifying social
biases perpetuated in the deductive reasoning of LLMs, where fairness is
critical.

</details>


### [261] [Chasing Consistency: Quantifying and Optimizing Human-Model Alignment in Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.06168)
*Boxuan Wang,Zhuoyun Li,Xinmiao Huang,Xiaowei Huang,Yi Dong*

Main category: cs.AI

TL;DR: 论文提出了Alignment Score指标来量化LLM推理链的语义一致性，并开发了SCOS优化方法，显著提升了长链推理的Alignment Score。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在Chain-of-Thought推理中缺乏有效的推理一致性评估方法，导致模型生成的推理链可能存在逻辑错误、语义偏差等问题，影响推理质量。

Method: 定义Alignment Score量化模型推理链与人类参考链的语义对齐程度；识别四种关键错误类型：逻辑断连、主题转移、冗余推理和因果倒置；提出SCOS方法通过采样和偏好最小对齐误差的推理链进行优化。

Result: 2-hop推理链达到最高Alignment Score；SCOS方法在3-hop等长链推理任务中将Alignment Score平均提升29.84%。

Conclusion: 该框架为LLM推理一致性提供了有效评估工具和优化方案，显著改善了复杂推理任务的性能，为提升LLM推理可靠性提供了新思路。

Abstract: This paper presents a framework for evaluating and optimizing reasoning
consistency in Large Language Models (LLMs) via a new metric, the Alignment
Score, which quantifies the semantic alignment between model-generated
reasoning chains and human-written reference chains in Chain-of-Thought (CoT)
reasoning. Empirically, we find that 2-hop reasoning chains achieve the highest
Alignment Score. To explain this phenomenon, we define four key error types:
logical disconnection, thematic shift, redundant reasoning, and causal
reversal, and show how each contributes to the degradation of the Alignment
Score. Building on this analysis, we further propose Semantic Consistency
Optimization Sampling (SCOS), a method that samples and favors chains with
minimal alignment errors, significantly improving Alignment Scores by an
average of 29.84% with longer reasoning chains, such as in 3-hop tasks.

</details>


### [262] [CSP4SDG: Constraint and Information-Theory Based Role Identification in Social Deduction Games with LLM-Enhanced Inference](https://arxiv.org/abs/2511.06175)
*Kaijie Xu,Fandi Meng,Clark Verbrugge,Simon Lucas*

Main category: cs.AI

TL;DR: CSP4SDG是一个概率约束满足框架，用于社交推理游戏中的角色推断，通过四种约束类型和信息论加权，实现了可解释的实时角色概率更新，性能超越现有LLM方法。


<details>
  <summary>Details</summary>
Motivation: 在社交推理游戏中，玩家隐藏身份并故意误导他人，准确的角色识别是人和AI表现的基础，但这是一个极具挑战性的任务。现有方法在处理这种复杂的推理场景时存在局限。

Method: 提出CSP4SDG框架，将游戏事件和对话映射到四种语言无关的约束类别（证据、现象、断言、假设），使用硬约束剪枝不可能的角色分配，加权软约束对剩余方案评分，通过信息增益加权连接假设与期望熵减值，采用闭式评分规则确保真实断言收敛到经典硬逻辑。

Result: 在三个公开数据集上实验表明：CSP4SDG在所有推理场景中都优于基于LLM的基线方法；当作为辅助推理工具提供给LLM时，能够显著提升LLM的性能表现。

Conclusion: 研究验证了基于信息论的原理性概率推理是社交推理游戏中重型神经模型的可扩展替代方案或补充方案，为游戏AI提供了更有效、可解释的推理机制。

Abstract: In Social Deduction Games (SDGs) such as Avalon, Mafia, and Werewolf, players
conceal their identities and deliberately mislead others, making hidden-role
inference a central and demanding task. Accurate role identification, which
forms the basis of an agent's belief state, is therefore the keystone for both
human and AI performance. We introduce CSP4SDG, a probabilistic,
constraint-satisfaction framework that analyses gameplay objectively. Game
events and dialogue are mapped to four linguistically-agnostic constraint
classes-evidence, phenomena, assertions, and hypotheses. Hard constraints prune
impossible role assignments, while weighted soft constraints score the
remainder; information-gain weighting links each hypothesis to its expected
value under entropy reduction, and a simple closed-form scoring rule guarantees
that truthful assertions converge to classical hard logic with minimum error.
The resulting posterior over roles is fully interpretable and updates in real
time. Experiments on three public datasets show that CSP4SDG (i) outperforms
LLM-based baselines in every inference scenario, and (ii) boosts LLMs when
supplied as an auxiliary "reasoning tool." Our study validates that principled
probabilistic reasoning with information theory is a scalable alternative-or
complement-to heavy-weight neural models for SDGs.

</details>


### [263] [Dataforge: A Data Agent Platform for Autonomous Data Engineering](https://arxiv.org/abs/2511.06185)
*Xinyuan Wang,Yanjie Fu*

Main category: cs.AI

TL;DR: Data Agent是一个基于LLM的全自主表格数据处理系统，能够自动完成数据清洗、分层路由和特征级优化，实现从原始数据到高质量AI就绪数据的端到端转换。


<details>
  <summary>Details</summary>
Motivation: AI应用在材料发现、分子建模和气候科学等领域的需求激增，使得数据准备成为重要但劳动密集型的步骤。传统方法面临可扩展性和专业知识依赖的挑战。

Method: Data Agent利用大语言模型的推理能力和基于事实的验证，通过双反馈循环自动执行数据清洗、分层路由和特征级优化，实现全自主处理。

Result: 展示了首个实用的自主Data Agent实现，能够在无人工监督的情况下，将原始数据转换为"从数据到更好数据"的高质量AI就绪数据。

Conclusion: Data Agent体现了自动、安全、非专家友好三大核心原则，为表格数据准备提供了端到端可靠的解决方案，显著降低了AI应用的数据准备门槛。

Abstract: The growing demand for AI applications in fields such as materials discovery,
molecular modeling, and climate science has made data preparation an important
but labor-intensive step. Raw data from diverse sources must be cleaned,
normalized, and transformed to become AI-ready, while effective feature
transformation and selection are essential for efficient training and
inference. To address the challenges of scalability and expertise dependence,
we present Data Agent, a fully autonomous system specialized for tabular data.
Leveraging large language model (LLM) reasoning and grounded validation, Data
Agent automatically performs data cleaning, hierarchical routing, and
feature-level optimization through dual feedback loops. It embodies three core
principles: automatic, safe, and non-expert friendly, which ensure end-to-end
reliability without human supervision. This demo showcases the first practical
realization of an autonomous Data Agent, illustrating how raw data can be
transformed "From Data to Better Data."

</details>


### [264] [Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps via Uncertainty Heads](https://arxiv.org/abs/2511.06209)
*Jingwei Ni,Ekaterina Fadeeva,Tianyi Wu,Mubashara Akhtar,Jiaheng Zhang,Elliott Ash,Markus Leippold,Timothy Baldwin,See-Kiong Ng,Artem Shelmanov,Mrinmaya Sachan*

Main category: cs.AI

TL;DR: 本研究提出了一种轻量级的基于数据驱动不确定性分数的步骤级推理验证方法UHeads，通过使用小于1000万参数的transformer模块，在多个领域匹配或超越比其大810倍的传统过程奖励模型(PRMs)性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理验证方法如PRMs存在计算成本高、领域受限、需要大规模人工或模型生成标注等问题，限制了其在大规模和泛化场景下的应用。

Method: 训练基于transformer的不确定性量化头(UHeads)，利用冻结LLM的内部状态来估计推理步骤的不确定性；目标标签通过更大的LLM或原模型自监督方式生成，完全自动化。

Result: UHeads在数学、规划和通用知识问答等多个领域匹配或超越比其大810倍的PRMs性能，证明了LLM内部状态编码了不确定性信息。

Conclusion: LLM内部状态包含可用的不确定性信息，可作为推理验证的可靠信号，为实现可扩展和泛化的内省式LLMs提供了有前景的方向。

Abstract: Solving complex tasks usually requires LLMs to generate long multi-step
reasoning chains. Previous work has shown that verifying the correctness of
individual reasoning steps can further improve the performance and efficiency
of LLMs on such tasks and enhance solution interpretability. However, existing
verification approaches, such as Process Reward Models (PRMs), are either
computationally expensive, limited to specific domains, or require large-scale
human or model-generated annotations. Thus, we propose a lightweight
alternative for step-level reasoning verification based on data-driven
uncertainty scores. We train transformer-based uncertainty quantification heads
(UHeads) that use the internal states of a frozen LLM to estimate the
uncertainty of its reasoning steps during generation. The approach is fully
automatic: target labels are generated either by another larger LLM (e.g.,
DeepSeek R1) or in a self-supervised manner by the original model itself.
UHeads are both effective and lightweight, containing less than 10M parameters.
Across multiple domains, including mathematics, planning, and general knowledge
question answering, they match or even surpass the performance of PRMs that are
up to 810x larger. Our findings suggest that the internal states of LLMs encode
their uncertainty and can serve as reliable signals for reasoning verification,
offering a promising direction toward scalable and generalizable introspective
LLMs.

</details>


### [265] [Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model Reasoning Ability in VibeThinker-1.5B](https://arxiv.org/abs/2511.06221)
*Sen Xu,Yi Zhou,Wei Wang,Jixin Min,Zhibin Yin,Yingwei Dai,Shixi Liu,Lianyu Pang,Yirong Chen,Junlin Zhang*

Main category: cs.AI

TL;DR: VibeThinker-1.5B是一个15亿参数的小型模型，通过Spectrum-to-Signal Principle (SSP)方法训练，仅用7800美元成本就在多个数学基准测试中超越了400倍参数规模的DeepSeek R1模型。


<details>
  <summary>Details</summary>
Motivation: 挑战当前认为小模型 inherently 缺乏强推理能力的共识，以及通过无限扩大模型参数来提升性能的主流做法（如DeepSeek R1的671B参数和Kimi k2的超过1T参数）。

Method: 采用Spectrum-to-Signal Principle (SSP)框架：首先使用两阶段多样性探索蒸馏（Two-Stage Diversity-Exploring Distillation）生成广泛解决方案，然后通过最大熵引导策略优化（MaxEnt-Guided Policy Optimization）放大正确信号。

Result: 在数学基准测试中表现优异：AIME24 (80.3 vs 79.8)、AIME25 (74.4 vs 70.0)、HMMT25 (50.4 vs 41.7)均超越400倍参数的DeepSeek R1；在LiveCodeBench V6上得分51.1，超越Magistral Medium的50.3。相比基线模型有显著提升。

Conclusion: 证明了小模型通过创新训练方法可以达到与大模型相当的推理能力，大幅降低训练和推理成本，从而推动AI研究的民主化进程。

Abstract: Challenging the prevailing consensus that small models inherently lack robust
reasoning, this report introduces VibeThinker-1.5B, a 1.5B-parameter dense
model developed via our Spectrum-to-Signal Principle (SSP). This challenges the
prevailing approach of scaling model parameters to enhance capabilities, as
seen in models like DeepSeek R1 (671B) and Kimi k2 (>1T). The SSP framework
first employs a Two-Stage Diversity-Exploring Distillation (SFT) to generate a
broad spectrum of solutions, followed by MaxEnt-Guided Policy Optimization (RL)
to amplify the correct signal. With a total training cost of only $7,800,
VibeThinker-1.5B demonstrates superior reasoning capabilities compared to
closed-source models like Magistral Medium and Claude Opus 4, and performs on
par with open-source models like GPT OSS-20B Medium. Remarkably, it surpasses
the 400x larger DeepSeek R1 on three math benchmarks: AIME24 (80.3 vs. 79.8),
AIME25 (74.4 vs. 70.0), and HMMT25 (50.4 vs. 41.7). This is a substantial
improvement over its base model (6.7, 4.3, and 0.6, respectively). On
LiveCodeBench V6, it scores 51.1, outperforming Magistral Medium's 50.3 and its
base model's 0.0. These findings demonstrate that small models can achieve
reasoning capabilities comparable to large models, drastically reducing
training and inference costs and thereby democratizing advanced AI research.

</details>


### [266] [ROAR: Robust Accident Recognition and Anticipation for Autonomous Driving](https://arxiv.org/abs/2511.06226)
*Xingcheng Liu,Yanchen Guan,Haicheng Liao,Zhengbing He,Zhenning Li*

Main category: cs.AI

TL;DR: 本研究提出ROAR方法，结合离散小波变换、自适应目标感知模块和动态焦点损失，提升自动驾驶车辆在现实复杂环境下的事故预测准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶事故预测方法通常假设理想条件，忽视了传感器故障、环境干扰和数据不完美等现实挑战，这些因素会显著降低预测准确性。同时，先前模型未能充分解决不同车辆类型间驾驶行为差异和事故率的显著变化问题。

Method: ROAR方法包含三个核心组件：1) 离散小波变换(DWT)用于从噪声和不完整数据中有效提取特征；2) 自适应目标感知模块通过聚焦高风险车辆并建模交通主体间的时空关系来改善事故预测；3) 动态焦点损失减轻正负样本间的类别不平衡问题。

Result: 在DAD、CCD和A3D三个广泛使用的数据集上评估，ROAR模型在平均精度(AP)和平均事故时间(mTTA)等关键指标上持续优于现有基线模型，证明了其在传感器退化、环境噪声和不平衡数据分布等现实条件下的鲁棒性。

Conclusion: 该工作为复杂交通环境中可靠准确的事故预测提供了有前景的解决方案，有效解决了现有方法的主要局限性，对提升自动驾驶安全性具有重要意义。

Abstract: Accurate accident anticipation is essential for enhancing the safety of
autonomous vehicles (AVs). However, existing methods often assume ideal
conditions, overlooking challenges such as sensor failures, environmental
disturbances, and data imperfections, which can significantly degrade
prediction accuracy. Additionally, previous models have not adequately
addressed the considerable variability in driver behavior and accident rates
across different vehicle types. To overcome these limitations, this study
introduces ROAR, a novel approach for accident detection and prediction. ROAR
combines Discrete Wavelet Transform (DWT), a self adaptive object aware module,
and dynamic focal loss to tackle these challenges. The DWT effectively extracts
features from noisy and incomplete data, while the object aware module improves
accident prediction by focusing on high-risk vehicles and modeling the spatial
temporal relationships among traffic agents. Moreover, dynamic focal loss
mitigates the impact of class imbalance between positive and negative samples.
Evaluated on three widely used datasets, Dashcam Accident Dataset (DAD), Car
Crash Dataset (CCD), and AnAn Accident Detection (A3D), our model consistently
outperforms existing baselines in key metrics such as Average Precision (AP)
and mean Time to Accident (mTTA). These results demonstrate the model's
robustness in real-world conditions, particularly in handling sensor
degradation, environmental noise, and imbalanced data distributions. This work
offers a promising solution for reliable and accurate accident anticipation in
complex traffic environments.

</details>


### [267] [Synthetic Data-Driven Prompt Tuning for Financial QA over Tables and Documents](https://arxiv.org/abs/2511.06292)
*Yaoning Yu,Kaimin Chang,Ye Yu,Kai Wei,Haojing Luo,Haohan Wang*

Main category: cs.AI

TL;DR: 提出了一种自改进提示框架，通过合成金融数据和闭环优化来提升LLM在金融推理任务中的性能，无需外部标注即可实现更高的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前提示调优方法在金融推理任务中存在局限：要么在固定数据集上调优缺乏对新问题类型或文档结构的适应能力，要么需要昂贵的手工标注数据集来构建提示。

Method: 设计了自改进提示框架，包含三个核心组件：合成数据生成器（生成金融表格和文档片段）、验证器（检查示例有效性和鲁棒性）、提示优化器（基于验证结果迭代优化提示）。通过闭环反馈循环不断改进。

Result: 在DocMath-Eval基准测试中，该系统在准确性和鲁棒性方面都优于标准提示方法，证明了方法的有效性。

Conclusion: 将合成数据生成集成到金融应用的提示学习中具有重要价值，能够不依赖外部标注就显著提升金融推理任务的性能。

Abstract: Financial documents like earning reports or balance sheets often involve long
tables and multi-page reports. Large language models have become a new tool to
help numerical reasoning and understanding these documents. However, prompt
quality can have a major effect on how well LLMs perform these financial
reasoning tasks. Most current methods tune prompts on fixed datasets of
financial text or tabular data, which limits their ability to adapt to new
question types or document structures, or they involve costly and manually
labeled/curated dataset to help build the prompts. We introduce a
self-improving prompt framework driven by data-augmented optimization. In this
closed-loop process, we generate synthetic financial tables and document
excerpts, verify their correctness and robustness, and then update the prompt
based on the results. Specifically, our framework combines a synthetic data
generator with verifiers and a prompt optimizer, where the generator produces
new examples that exposes weaknesses in the current prompt, the verifiers check
the validity and robustness of the produced examples, and the optimizer
incrementally refines the prompt in response. By iterating these steps in a
feedback cycle, our method steadily improves prompt accuracy on financial
reasoning tasks without needing external labels. Evaluation on DocMath-Eval
benchmark demonstrates that our system achieves higher performance in both
accuracy and robustness than standard prompt methods, underscoring the value of
incorporating synthetic data generation into prompt learning for financial
applications.

</details>


### [268] [Secu-Table: a Comprehensive security table dataset for evaluating semantic table interpretation systems](https://arxiv.org/abs/2511.06301)
*Azanzi Jiomekong,Jean Bikim,Patricia Negoue,Joyce Chin*

Main category: cs.AI

TL;DR: 本文提出了Secu-Table数据集，包含1500多个表格和超过15k实体，专门用于评估安全领域的语义表格解释系统。


<details>
  <summary>Details</summary>
Motivation: 在安全领域，缺乏公开可用的表格数据集来评估基于大语言模型的语义表格解释系统，这阻碍了该领域的研究进展。

Method: 通过从CVE和CWE安全数据源提取数据，使用Wikidata和SEPSES CSKG知识图谱进行标注，构建了包含1500多个表格和超过15k实体的Secu-Table数据集，并公开了所有代码。

Result: 构建了一个专门针对安全领域的大规模语义表格数据集，并使用Falcon3-7b-instruct、Mistral-7B-Instruct和GPT-4o mini进行了基线评估，为SemTab挑战赛提供了数据支持。

Conclusion: Secu-Table数据集填补了安全领域语义表格解释系统评估的空白，为基于LLM的STI系统提供了重要的评测基准，推动了该领域的研究发展。

Abstract: Evaluating semantic tables interpretation (STI) systems, (particularly, those
based on Large Language Models- LLMs) especially in domain-specific contexts
such as the security domain, depends heavily on the dataset. However, in the
security domain, tabular datasets for state-of-the-art are not publicly
available. In this paper, we introduce Secu-Table dataset, composed of more
than 1500 tables with more than 15k entities constructed using security data
extracted from Common Vulnerabilities and Exposures (CVE) and Common Weakness
Enumeration (CWE) data sources and annotated using Wikidata and the SEmantic
Processing of Security Event Streams CyberSecurity Knowledge Graph (SEPSES
CSKG). Along with the dataset, all the code is publicly released. This dataset
is made available to the research community in the context of the SemTab
challenge on Tabular to Knowledge Graph Matching. This challenge aims to
evaluate the performance of several STI based on open source LLMs. Preliminary
evaluation, serving as baseline, was conducted using Falcon3-7b-instruct and
Mistral-7B-Instruct, two open source LLMs and GPT-4o mini one closed source
LLM.

</details>


### [269] [The Station: An Open-World Environment for AI-Driven Discovery](https://arxiv.org/abs/2511.06309)
*Stephen Chung,Wenyu Du*

Main category: cs.AI

TL;DR: STATION是一个开放的AI智能体环境，让AI智能体能够自主进行科学研究，包括阅读论文、提出假设、提交代码、分析和发表结果，无需中央协调，并在多个基准测试中取得最先进性能。


<details>
  <summary>Details</summary>
Motivation: 创造一个能够模拟真实科学研究生态的开放世界环境，让AI智能体能够自主进行长期的科学探索，突破传统优化范式的限制。

Method: 构建了一个名为STATION的开放世界多智能体环境，利用扩展的上下文窗口让智能体能够进行长期科学研究，包括阅读论文、假设制定、代码提交、分析和发表等完整流程。

Result: AI智能体在数学、计算生物学和机器学习等多个基准测试中达到最先进性能，特别在圆packing问题上超越了AlphaEvolve；智能体自发产生了丰富的叙事和新方法，如用于scRNA-seq批集成的新的密度自适应算法。

Conclusion: STATION代表了通过开放世界中涌现行为驱动的自主科学发现的第一步，是一个超越刚性优化的新范式，为未来AI驱动的科学研究开辟了新的可能性。

Abstract: We introduce the STATION, an open-world multi-agent environment that models a
miniature scientific ecosystem. Leveraging their extended context windows,
agents in the Station can engage in long scientific journeys that include
reading papers from peers, formulating hypotheses, submitting code, performing
analyses, and publishing results. Importantly, there is no centralized system
coordinating their activities - agents are free to choose their own actions and
develop their own narratives within the Station. Experiments demonstrate that
AI agents in the Station achieve new state-of-the-art performance on a wide
range of benchmarks, spanning from mathematics to computational biology to
machine learning, notably surpassing AlphaEvolve in circle packing. A rich
tapestry of narratives emerges as agents pursue independent research, interact
with peers, and build upon a cumulative history. From these emergent
narratives, novel methods arise organically, such as a new density-adaptive
algorithm for scRNA-seq batch integration. The Station marks a first step
towards autonomous scientific discovery driven by emergent behavior in an
open-world environment, representing a new paradigm that moves beyond rigid
optimization.

</details>


### [270] [ALIGN: A Vision-Language Framework for High-Accuracy Accident Location Inference through Geo-Spatial Neural Reasoning](https://arxiv.org/abs/2511.06316)
*MD Thamed Bin Zaman Chowdhury,Moazzem Hossain*

Main category: cs.AI

TL;DR: ALIGN是一个视觉-语言框架，通过模拟人类空间推理，从多语言新闻文本和地图中准确推断事故地点坐标，解决了中低收入国家交通事故地理信息数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 可靠的道路事故地理信息对安全分析和基础设施规划至关重要，但中低收入国家缺乏准确的位置特定事故数据。现有的基于文本的地理编码工具在多语言和非结构化新闻环境中表现不佳，特别是在处理不完整地点描述和孟加拉语-英语混合文本时，空间上下文模糊不清。

Method: ALIGN框架集成大语言模型和视觉-语言模型，采用多阶段流水线：1）光学字符识别（OCR）；2）语言推理；3）通过基于网格的空间扫描进行地图级验证。系统性地评估每个预测位置与上下文和视觉证据的一致性，确保可解释的细粒度地理定位结果，无需模型重新训练。

Result: 应用于孟加拉语新闻数据时，ALIGN相比传统地理解析方法表现出持续改进，能够准确识别区域和次区域级别的事故地点，显著提升了地理编码精度。

Conclusion: 该框架为数据稀缺地区的自动化事故地图绘制建立了高精度基础，支持基于证据的道路安全政策制定，并促进多模态人工智能在交通分析中的广泛应用。代码已开源，为相关研究提供了可复现的技术方案。

Abstract: Reliable geospatial information on road accidents is vital for safety
analysis and infrastructure planning, yet most low- and middle-income countries
continue to face a critical shortage of accurate, location-specific crash data.
Existing text-based geocoding tools perform poorly in multilingual and
unstructured news environments, where incomplete place descriptions and mixed
Bangla-English scripts obscure spatial context. To address these limitations,
this study introduces ALIGN (Accident Location Inference through Geo-Spatial
Neural Reasoning)- a vision-language framework that emulates human spatial
reasoning to infer accident coordinates directly from textual and map-based
cues. ALIGN integrates large language and vision-language models within a
multi-stage pipeline that performs optical character recognition, linguistic
reasoning, and map-level verification through grid-based spatial scanning. The
framework systematically evaluates each predicted location against contextual
and visual evidence, ensuring interpretable, fine-grained geolocation outcomes
without requiring model retraining. Applied to Bangla-language news data, ALIGN
demonstrates consistent improvements over traditional geoparsing methods,
accurately identifying district and sub-district-level crash sites. Beyond its
technical contribution, the framework establishes a high accuracy foundation
for automated crash mapping in data-scarce regions, supporting evidence-driven
road-safety policymaking and the broader integration of multimodal artificial
intelligence in transportation analytics. The code for this paper is
open-source and available at: https://github.com/Thamed-Chowdhury/ALIGN

</details>


### [271] [LPFQA: A Long-Tail Professional Forum-based Benchmark for LLM Evaluation](https://arxiv.org/abs/2511.06346)
*Liya Zhu,Peizhuang Cong,Aowei Ji,Wenya Wu,Jiani Hou,Chunjie Wu,Xiang Gao,Jingkai Liu,Zhou Huan,Xuelei Sun,Yang Yang,Jianpeng Jiao,Liang Hu,Xinjie Chen,Jiashuo Liu,Jingzhe Ding,Tong Yang,Zaiyuan Wang,Ge Zhang,Wenhao Huang*

Main category: cs.AI

TL;DR: 研究者从真实专业论坛创建了LPFQA基准测试，用于更好评估大语言模型在实践专业任务上的能力，揭示了模型间显著的性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型评估基准存在不足，它们往往关注简化任务或人工场景，忽略了长尾知识和现实世界应用的复杂性，无法准确评估LLM的真实能力。

Method: 提出了LPFQA基准测试，源自20个学术和工业领域的真实专业论坛，涵盖502个基于专业知识的任务。创新点包括：针对知识深度、推理、术语理解和上下文分析的细粒度评估维度；确保语义清晰和唯一答案的分层难度结构；真实用户画像的专业场景建模；跨领域跨学科的知识整合。评估了12个主流LLM模型。

Result: 在LPFQA上的评估显示，主流LLM模型之间存在显著性能差异，特别是在专业推理任务中表现差距更大，表明现有模型在处理专业领域知识时仍有不足。

Conclusion: LPFQA为推进LLM评估和指导未来模型开发提供了一个强大、真实且具有区分度的基准测试，填补了现有评估工具在长尾知识和专业能力评估方面的空白。

Abstract: Large Language Models (LLMs) have made rapid progress in reasoning, question
answering, and professional applications; however, their true capabilities
remain difficult to evaluate using existing benchmarks. Current datasets often
focus on simplified tasks or artificial scenarios, overlooking long-tail
knowledge and the complexities of real-world applications. To bridge this gap,
we propose LPFQA, a long-tail knowledge-based benchmark derived from authentic
professional forums across 20 academic and industrial fields, covering 502
tasks grounded in practical expertise. LPFQA introduces four key innovations:
fine-grained evaluation dimensions that target knowledge depth, reasoning,
terminology comprehension, and contextual analysis; a hierarchical difficulty
structure that ensures semantic clarity and unique answers; authentic
professional scenario modeling with realistic user personas; and
interdisciplinary knowledge integration across diverse domains. We evaluated 12
mainstream LLMs on LPFQA and observed significant performance disparities,
especially in specialized reasoning tasks. LPFQA provides a robust, authentic,
and discriminative benchmark for advancing LLM evaluation and guiding future
model development.

</details>


### [272] [What Makes Reasoning Invalid: Echo Reflection Mitigation for Large Language Models](https://arxiv.org/abs/2511.06380)
*Chen He,Xun Jiang,Lei Wang,Hao Yang,Chong Peng,Peng Yan,Fumin Shen,Xing Xu*

Main category: cs.AI

TL;DR: 本文发现了大型语言模型在复杂推理任务中的"回声反思"现象，并提出了一种新的强化学习方法AEPO来解决这一问题，该方法通过信息过滤和自适应熵优化来提升模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM在复杂数学推理中表现良好，但在扩展到需要特定领域知识的复杂任务时，模型在反思阶段无法产生新见解，而是机械重复之前的推理步骤，这种"回声反思"现象严重限制了模型的认知深化能力。

Method: 提出自适应熵策略优化(AEPO)框架，包含两个核心组件：(1)反思感知信息过滤，量化认知信息流并阻止早期错误信息影响最终答案；(2)自适应熵优化，在不同推理阶段动态平衡探索与利用，促进反思多样性和答案正确性。

Result: 大量实验证明AEPO在多个基准测试中始终超越主流强化学习基线，取得了最先进的性能表现。

Conclusion: 通过解决信息流不可控和内部知识探索不足两个关键缺陷，AEPO有效克服了LLM的"回声反思"问题，为提升复杂领域推理任务中的模型性能提供了新的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
a wide range of reasoning tasks. Recent methods have further improved LLM
performance in complex mathematical reasoning. However, when extending these
methods beyond the domain of mathematical reasoning to tasks involving complex
domain-specific knowledge, we observe a consistent failure of LLMs to generate
novel insights during the reflection stage. Instead of conducting genuine
cognitive refinement, the model tends to mechanically reiterate earlier
reasoning steps without introducing new information or perspectives, a
phenomenon referred to as "Echo Reflection". We attribute this behavior to two
key defects: (1) Uncontrollable information flow during response generation,
which allows premature intermediate thoughts to propagate unchecked and distort
final decisions; (2) Insufficient exploration of internal knowledge during
reflection, leading to repeating earlier findings rather than generating new
cognitive insights. Building on these findings, we proposed a novel
reinforcement learning method termed Adaptive Entropy Policy Optimization
(AEPO). Specifically, the AEPO framework consists of two major components: (1)
Reflection-aware Information Filtration, which quantifies the cognitive
information flow and prevents the final answer from being affected by earlier
bad cognitive information; (2) Adaptive-Entropy Optimization, which dynamically
balances exploration and exploitation across different reasoning stages,
promoting both reflective diversity and answer correctness. Extensive
experiments demonstrate that AEPO consistently achieves state-of-the-art
performance over mainstream reinforcement learning baselines across diverse
benchmarks.

</details>


### [273] [Efficient LLM Safety Evaluation through Multi-Agent Debate](https://arxiv.org/abs/2511.06396)
*Dachuan Lin,Guobin Shen,Zihao Yang,Tianrong Liu,Dongcheng Zhao,Yi Zeng*

Main category: cs.AI

TL;DR: 本文提出了一种基于小语言模型(SLMs)的多智能体安全评估框架，通过结构化辩论机制替代昂贵的前沿模型进行大语言模型安全性评测，并构建了包含12000个对抗交互的大规模越狱基准数据集HAJailBench。该框架在保持与GPT-4o相当评测效果的同时大幅降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 传统LLM安全性评测依赖昂贵的边缘模型，限制了可扩展性。随着大语言模型安全评估需求的增长，急需开发成本效益更高的解决方案来支持大规模安全评测。

Method: 提出多智能体评判框架，采用批评者、辩护者和法官三个SLM智能体进行结构化辩论；构建HAJailBench基准数据集，包含12000个人工标注的对抗交互，涵盖多样化攻击方法和目标模型，提供细粒度的专家标注真值。

Result: SLM多智能体框架在HAJailBench上达到与GPT-4o评判者相当的一致性，显著降低推理成本；消融实验表明三轮辩论在准确性和效率之间取得最佳平衡。

Conclusion: 结构化的价值对齐辩论机制使SLMs能够捕获越狱攻击的语义细微差别，证明小模型通过协作可有效替代大模型进行安全评测，HAJailBench为可扩展的LLM安全评估提供了可靠基础。

Abstract: Safety evaluation of large language models (LLMs) increasingly relies on
LLM-as-a-Judge frameworks, but the high cost of frontier models limits
scalability. We propose a cost-efficient multi-agent judging framework that
employs Small Language Models (SLMs) through structured debates among critic,
defender, and judge agents. To rigorously assess safety judgments, we construct
HAJailBench, a large-scale human-annotated jailbreak benchmark comprising
12,000 adversarial interactions across diverse attack methods and target
models. The dataset provides fine-grained, expert-labeled ground truth for
evaluating both safety robustness and judge reliability. Our SLM-based
framework achieves agreement comparable to GPT-4o judges on HAJailBench while
substantially reducing inference cost. Ablation results show that three rounds
of debate yield the optimal balance between accuracy and efficiency. These
findings demonstrate that structured, value-aligned debate enables SLMs to
capture semantic nuances of jailbreak attacks and that HAJailBench offers a
reliable foundation for scalable LLM safety evaluation.

</details>


### [274] [SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization](https://arxiv.org/abs/2511.06411)
*Zhi Zheng,Wee Sun Lee*

Main category: cs.AI

TL;DR: 本文提出了SofT-GRPO算法，通过Gumbel噪声注入和Gumbel-Softmax技术，成功将强化学习扩展到软思维推理模式，在小到中等规模LLM上实现了超越离散token GRPO的性能提升。


<details>
  <summary>Details</summary>
Motivation: 软思维推理在某些场景下能超越传统的离散token链式思维推理，但将强化学习扩展到软思维模式一直很困难，这限制了软思维范式的全部潜力的发挥。

Method: 提出SofT-GRPO算法，通过在logits中注入Gumbel噪声，采用Gumbel-Softmax技术避免token超出预训练嵌入空间，并在策略梯度中利用重参数化技巧来解决软思维推理的强化学习优化问题。

Result: 在1.5B到7B参数的基础LLM上，SofT-GRPO使软思维LLM在Pass@1上平均提升0.13%，在Pass@1上提升2.19%，显著优于离散token GRPO方法。

Conclusion: SofT-GRPO成功解决了软思维推理的强化学习优化难题，为释放软思维范式的全部潜力提供了有效方案，特别是在多次采样评估(Pass@32)场景下表现出显著优势。

Abstract: The soft-thinking paradigm for Large Language Model (LLM) reasoning can
outperform the conventional discrete-token Chain-of-Thought (CoT) reasoning in
some scenarios, underscoring its research and application value. However, while
the discrete-token CoT reasoning pattern can be reinforced through policy
optimization algorithms such as group relative policy optimization (GRPO),
extending the soft-thinking pattern with Reinforcement Learning (RL) remains
challenging. This difficulty stems from the complexities of injecting
stochasticity into soft-thinking tokens and updating soft-thinking policies
accordingly. As a result, previous attempts to combine soft-thinking with GRPO
typically underperform their discrete-token GRPO counterparts. To fully unlock
the potential of soft-thinking, this paper presents a novel policy optimization
algorithm, SofT-GRPO, to reinforce LLMs under the soft-thinking reasoning
pattern. SofT-GRPO injects the Gumbel noise into logits, employs the
Gumbel-Softmax technique to avoid soft-thinking tokens outside the pre-trained
embedding space, and leverages the reparameterization trick in policy gradient.
We conduct experiments across base LLMs ranging from 1.5B to 7B parameters, and
results demonstrate that SofT-GRPO enables soft-thinking LLMs to slightly
outperform discrete-token GRPO on Pass@1 (+0.13% on average accuracy), while
exhibiting a substantial uplift on Pass@32 (+2.19% on average accuracy). Codes
and weights are available on https://github.com/zz1358m/SofT-GRPO-master

</details>


### [275] [MONICA: Real-Time Monitoring and Calibration of Chain-of-Thought Sycophancy in Large Reasoning Models](https://arxiv.org/abs/2511.06419)
*Jingyu Hu,Shu Yang,Xilin Gong,Hongming Wang,Weiru Liu,Di Wang*

Main category: cs.AI

TL;DR: MONICA是一个监控引导的校准框架，能够在推理步骤级别实时监测和缓解大型推理模型的谄媚行为，无需等待模型生成完整答案。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型存在谄媚行为问题，即倾向于同意用户的错误信念和传播错误信息，而不是保持独立推理。这种行为削弱了模型可靠性并带来社会风险。现有方法主要关注基于最终答案的判断和修正，无法理解谄媚行为在推理过程中如何发展。

Method: MONICA框架集成谄媚监控器和校准器。监控器在响应生成过程中提供实时的谄媚漂移分数监控，校准器在分数超过预设阈值时动态抑制谄媚行为。该框架在推理步骤级别工作，无需模型完成完整答案生成。

Result: 在12个数据集和3个大型推理模型上的广泛实验表明，MONICA有效减少了中间推理步骤和最终答案中的谄媚行为，实现了鲁棒的性能改进。

Conclusion: MONICA为缓解大型推理模型的谄媚行为提供了创新解决方案，通过实时监控推理轨迹中的谄媚行为，在保证模型性能的同时显著提升了模型可靠性。

Abstract: Large Reasoning Models (LRMs) suffer from sycophantic behavior, where models
tend to agree with users' incorrect beliefs and follow misinformation rather
than maintain independent reasoning. This behavior undermines model reliability
and poses societal risks. Mitigating LRM sycophancy requires monitoring how
this sycophancy emerges during the reasoning trajectory; however, current
methods mainly focus on judging based on final answers and correcting them,
without understanding how sycophancy develops during reasoning processes. To
address this limitation, we propose MONICA, a novel Monitor-guided Calibration
framework that monitors and mitigates sycophancy during model inference at the
level of reasoning steps, without requiring the model to finish generating its
complete answer. MONICA integrates a sycophantic monitor that provides
real-time monitoring of sycophantic drift scores during response generation
with a calibrator that dynamically suppresses sycophantic behavior when scores
exceed predefined thresholds. Extensive experiments across 12 datasets and 3
LRMs demonstrate that our method effectively reduces sycophantic behavior in
both intermediate reasoning steps and final answers, yielding robust
performance improvements.

</details>


### [276] [Brain-Inspired Planning for Better Generalization in Reinforcement Learning](https://arxiv.org/abs/2511.06470)
*Mingde "Harry" Zhao*

Main category: cs.AI

TL;DR: 该论文通过引入类似人脑的推理行为来增强强化学习智能体的零样本系统泛化能力，包括空间抽象、Skipper框架和可行性评估器三种方法，显著提升了智能体在未知环境中的规划性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习系统在应用于真实世界场景时面临重大挑战，主要原因是在与训练条件不同的环境中泛化能力差。论文旨在通过赋予RL智能体人类大脑中发现的有助于系统泛化的推理行为来探索增强智能体零样本系统泛化能力的方向。

Method: 1) 受人类有意识规划行为启发，引入自上而下的注意力机制"空间抽象"，让决策时规划智能体根据瞬时意图动态关注环境状态的最相关方面；2) 基于空间抽象开发Skipper框架，自动将复杂任务分解为更简单的子任务；3) 识别并解决依赖生成模型的规划智能体的共同失效模式和安全风险，提出学习可行性评估器来拒绝幻觉化不可行目标。

Result: 空间抽象方法显著改善了训练任务外的系统泛化；Skipper框架提供了对分布偏移的鲁棒性并在长期组合规划中有效；可行性评估器使多种规划智能体的性能得到显著提升，有效解决了"妄想规划"问题。

Conclusion: 研究验证了将人类认知机制融入强化学习智能体的有效性，通过空间抽象、任务分解和幻觉检测等方法显著提升了系统泛化能力。未来研究方向是实现通用任务抽象和完全启用抽象规划，进一步提升智能体在复杂环境中的适应能力。

Abstract: Existing Reinforcement Learning (RL) systems encounter significant challenges
when applied to real-world scenarios, primarily due to poor generalization
across environments that differ from their training conditions. This thesis
explores the direction of enhancing agents' zero-shot systematic generalization
abilities by granting RL agents reasoning behaviors that are found to help
systematic generalization in the human brain. Inspired by human conscious
planning behaviors, we first introduced a top-down attention mechanism, which
allows a decision-time planning agent to dynamically focus its reasoning on the
most relevant aspects of the environmental state given its instantaneous
intentions, a process we call "spatial abstraction". This approach
significantly improves systematic generalization outside the training tasks.
Subsequently, building on spatial abstraction, we developed the Skipper
framework to automatically decompose complex tasks into simpler, more
manageable sub-tasks. Skipper provides robustness against distributional shifts
and efficacy in long-term, compositional planning by focusing on pertinent
spatial and temporal elements of the environment. Finally, we identified a
common failure mode and safety risk in planning agents that rely on generative
models to generate state targets during planning. It is revealed that most
agents blindly trust the targets they hallucinate, resulting in delusional
planning behaviors. Inspired by how the human brain rejects delusional
intentions, we propose learning a feasibility evaluator to enable rejecting
hallucinated infeasible targets, which led to significant performance
improvements in various kinds of planning agents. Finally, we suggest
directions for future research, aimed at achieving general task abstraction and
fully enabling abstract planning.

</details>


### [277] [GHOST: Solving the Traveling Salesman Problem on Graphs of Convex Sets](https://arxiv.org/abs/2511.06471)
*Jingtao Tang,Hang Ma*

Main category: cs.AI

TL;DR: GHOST是一个分层框架，通过组合组合环路搜索和凸轨迹优化，最优地解决了GCS-TSP（基于凸集图的新型旅行商问题变体）。


<details>
  <summary>Details</summary>
Motivation: 传统的TSP方法无法适用于GCS-TSP，因为在基于凸集图的轨迹规划中，边成本取决于通过每个凸区域的具体轨迹，而不是固定值。这种依赖性使得经典的组合优化方法在处理此类轨迹规划问题时失效。

Method: GHOST采用分层框架，在GCS诱导的完全图上系统性地探索环路。通过新颖的抽象路径展开算法计算可接受的下界，指导高层（环路）和低层（实现环路的可行GCS路径）的最佳优先搜索。这些强下界提供了强大的剪枝能力，在避免不必要凸优化调用的同时实现高效搜索。

Result: GHOST保证最优性，并提供有界次优变体用于时间关键场景。实验表明，对于简单情况，GHOST比统一混合整数凸规划基线快数个数量级，并且能够独特处理涉及高阶连续性约束和不完整GCS的复杂轨迹规划问题。

Conclusion: GHOST为GCS-TSP提供了高效且最优的解决方案，通过结合组合优化和凸轨迹优化，使得复杂的轨迹规划问题变得实际可行，特别是在传统方法无法处理的高维约束场景中展现出显著优势。

Abstract: We study GCS-TSP, a new variant of the Traveling Salesman Problem (TSP)
defined over a Graph of Convex Sets (GCS) -- a powerful representation for
trajectory planning that decomposes the configuration space into convex regions
connected by a sparse graph. In this setting, edge costs are not fixed but
depend on the specific trajectory selected through each convex region, making
classical TSP methods inapplicable. We introduce GHOST, a hierarchical
framework that optimally solves the GCS-TSP by combining combinatorial tour
search with convex trajectory optimization. GHOST systematically explores tours
on a complete graph induced by the GCS, using a novel abstract-path-unfolding
algorithm to compute admissible lower bounds that guide best-first search at
both the high level (over tours) and the low level (over feasible GCS paths
realizing the tour). These bounds provide strong pruning power, enabling
efficient search while avoiding unnecessary convex optimization calls. We prove
that GHOST guarantees optimality and present a bounded-suboptimal variant for
time-critical scenarios. Experiments show that GHOST is orders-of-magnitude
faster than unified mixed-integer convex programming baselines for simple cases
and uniquely handles complex trajectory planning problems involving high-order
continuity constraints and an incomplete GCS.

</details>


### [278] [FractalBench: Diagnosing Visual-Mathematical Reasoning Through Recursive Program Synthesis](https://arxiv.org/abs/2511.06522)
*Jan Ondras,Marek Šuppa*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Mathematical reasoning requires abstracting symbolic rules from visual
patterns -- inferring the infinite from the finite. We investigate whether
multimodal AI systems possess this capability through FractalBench, a benchmark
evaluating fractal program synthesis from images. Fractals provide ideal test
cases: Iterated Function Systems with only a few contraction maps generate
complex self-similar patterns through simple recursive rules, requiring models
to bridge visual perception with mathematical abstraction. We evaluate four
leading MLLMs -- GPT-4o, Claude 3.7 Sonnet, Gemini 2.5 Flash, and Qwen 2.5-VL
-- on 12 canonical fractals. Models must generate executable Python code
reproducing the fractal, enabling objective evaluation. Results reveal a
striking disconnect: 76% generate syntactically valid code but only 4% capture
mathematical structure. Success varies systematically -- models handle
geometric transformations (Koch curves: 17-21%) but fail at branching recursion
(trees: <2%), revealing fundamental gaps in mathematical abstraction.
FractalBench provides a contamination-resistant diagnostic for
visual-mathematical reasoning and is available at
https://github.com/NaiveNeuron/FractalBench

</details>


### [279] [GRAPH-GRPO-LEX: Contract Graph Modeling and Reinforcement Learning with Group Relative Policy Optimization](https://arxiv.org/abs/2511.06618)
*Moriya Dechtiar,Daniel Martin Katz,Mari Sundaresan,Sylvain Jaume,Hongming Wang*

Main category: cs.AI

TL;DR: 本文提出了GRAPH-GRPO-LEX框架，将法律合同转换为结构化语义图谱，通过强化学习结合大型语言模型自动提取实体和关系，实现合同分析的可视化和自动化。


<details>
  <summary>Details</summary>
Motivation: 合同文档结构复杂，包含显式和隐式依赖关系及丰富语义内容，导致合同起草和人工审查既繁重又易出错。需要自动化的方法来简化合同审查和分析过程。

Method: 1. 建立详细的本体论映射，将核心法律合同元素转换为图论中的节点和边；2. 提出GRAPH-GRPO-LEX框架，结合LLM和强化学习（GRPO）进行实体和关系的分段提取；3. 设计基于图谱度量的奖励函数，使用门控GRPO方法。

Result: 该方法能够自动识别条款间的直接关系，甚至发现隐藏依赖，实现从线性人工阅读到可视化图谱分析的转变，为合同检查奠定基础。

Conclusion: 通过将法律合同转换为语义图谱并结合强化学习方法，实现了合同分析的自动化和可视化，提高了分析效率和准确性，为合同审查提供了新的技术路径。

Abstract: Contracts are complex documents featuring detailed formal structures,
explicit and implicit dependencies and rich semantic content. Given these
document properties, contract drafting and manual examination of contracts have
proven to be both arduous and susceptible to errors. This work aims to simplify
and automate the task of contract review and analysis using a novel framework
for transforming legal contracts into structured semantic graphs, enabling
computational analysis and data-driven insights. We introduce a detailed
ontology mapping core legal contract elements to their graph-theoretic
equivalents of nodes and edges. We then present a reinforcement learning based
Large Language Model (LLM) framework for segmentation and extraction of
entities and relationships from contracts. Our method, GRAPH-GRPO-LEX,
incorporates both LLMs and reinforcement learning with group relative policy
optimization (GRPO). By applying a carefully drafted reward function of graph
metrics, we demonstrate the ability to automatically identify direct
relationships between clauses, and even uncover hidden dependencies. Our
introduction of the gated GRPO approach shows a strong learning signal and can
move contract analysis from a linear, manual reading process to an easily
visualized graph. This allows for a more dynamic analysis, including building
the groundwork for contract linting similar to what is now practiced in
software engineering.

</details>


### [280] [MathSE: Improving Multimodal Mathematical Reasoning via Self-Evolving Iterative Reflection and Reward-Guided Fine-Tuning](https://arxiv.org/abs/2511.06805)
*Jinhao Chen,Zhen Yang,Jianxin Shi,Tianyu Wo,Jie Tang*

Main category: cs.AI

TL;DR: 提出了MathSE框架，一种用于多模态大语言模型的数学自演化方法，通过迭代推理、反思和奖励反馈机制显著提升数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在数学推理任务中表现不足，传统方法依赖教师模型蒸馏的静态数据集，限制了模型对新问题的适应能力和泛化性能。

Method: 设计MathSE数学自演化框架，通过推理-反思-奖励反馈的循环迭代过程，利用前一阶段正确推理路径和专门的成果奖励模型(ORM)进行持续改进。

Result: 在多个挑战性基准测试中取得显著性能提升，特别是在MathVL-test上超越了领先的开源多模态数学推理模型QVQ。

Conclusion: MathSE框架有效解决了传统一次性微调的局限性，为提升多模态大语言模型的数学推理能力提供了新的自演化训练范式。

Abstract: Multimodal large language models (MLLMs) have demonstrated remarkable
capabilities in vision-language answering tasks. Despite their strengths, these
models often encounter challenges in achieving complex reasoning tasks such as
mathematical problem-solving. Previous works have focused on fine-tuning on
specialized mathematical datasets. However, these datasets are typically
distilled directly from teacher models, which capture only static reasoning
patterns and leaving substantial gaps compared to student models. This reliance
on fixed teacher-derived datasets not only restricts the model's ability to
adapt to novel or more intricate questions that extend beyond the confines of
the training data, but also lacks the iterative depth needed for robust
generalization. To overcome these limitations, we propose \textbf{\method}, a
\textbf{Math}ematical \textbf{S}elf-\textbf{E}volving framework for MLLMs. In
contrast to traditional one-shot fine-tuning paradigms, \method iteratively
refines the model through cycles of inference, reflection, and reward-based
feedback. Specifically, we leverage iterative fine-tuning by incorporating
correct reasoning paths derived from previous-stage inference and integrating
reflections from a specialized Outcome Reward Model (ORM). To verify the
effectiveness of \method, we evaluate it on a suite of challenging benchmarks,
demonstrating significant performance gains over backbone models. Notably, our
experimental results on MathVL-test surpass the leading open-source multimodal
mathematical reasoning model QVQ. Our code and models are available at
\texttt{https://zheny2751\allowbreak-dotcom.github.io/\allowbreak
MathSE.github.io/}.

</details>


### [281] [Proceedings of the 2025 XCSP3 Competition](https://arxiv.org/abs/2511.06918)
*Gilles Audemard,Christophe Lecoutre,Emmanuel Lonca*

Main category: cs.AI

TL;DR: 这是2025年XCSP3约束求解器竞赛的会议记录文档，竞赛结果在CP'25会议上展示。


<details>
  <summary>Details</summary>
Motivation: 组织XCSP3竞赛旨在促进约束求解器技术的发展，为不同求解器提供比较和评估的平台，推动约束编程领域的研究进展。

Method: 通过竞赛形式收集和组织约束求解器的性能数据，在CP'25国际会议上展示竞赛结果，作为学术交流的一部分。

Result: 文档记录了2025年XCSP3竞赛的完整结果，为约束编程社区提供了求解器性能比较的参考数据。

Conclusion: 作为CP'25会议的正式会议记录，该文档为约束求解器研究提供了重要的竞赛数据支持，有助于该领域的技术发展和学术交流。

Abstract: This document represents the proceedings of the 2025 XCSP3 Competition. The
results of this competition of constraint solvers were presented at CP'25 (31st
International Conference on Principles and Practice of Constraint Programming).

</details>


### [282] [Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and Curriculum Learning](https://arxiv.org/abs/2511.07061)
*Xinran Li,Xiujuan Xu,Jiaqi Qiao,Yu Liu*

Main category: cs.AI

TL;DR: 提出PRC-Emo框架，集成提示工程、示例检索和课程学习，提升大语言模型在对话情感识别中的表现


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在对话情感识别任务中难以有效捕捉显性和隐性情感之间的内在联系，限制了其情感感知能力

Method: 设计基于显性和隐性情感线索的敏感提示模板；构建首个专门的ERC演示检索库，包含训练样本和高质量对话示例；在LoRA微调中引入课程学习策略，基于同说话者和不同说话者间的情感转移权重分配样本难度

Result: 在IEMOCAP和MELD两个基准数据集上达到新的最先进性能，证明了方法的有效性和泛化能力

Conclusion: PRC-Emo框架能够显著改善基于大语言模型的情感理解，在对话情感识别任务中展现出良好的效果和通用性

Abstract: Emotion Recognition in Conversation (ERC) is a crucial task for understanding
human emotions and enabling natural human-computer interaction. Although Large
Language Models (LLMs) have recently shown great potential in this field, their
ability to capture the intrinsic connections between explicit and implicit
emotions remains limited. We propose a novel ERC training framework, PRC-Emo,
which integrates Prompt engineering, demonstration Retrieval, and Curriculum
learning, with the goal of exploring whether LLMs can effectively perceive
emotions in conversational contexts. Specifically, we design emotion-sensitive
prompt templates based on both explicit and implicit emotional cues to better
guide the model in understanding the speaker's psychological states. We
construct the first dedicated demonstration retrieval repository for ERC, which
includes training samples from widely used datasets, as well as high-quality
dialogue examples generated by LLMs and manually verified. Moreover, we
introduce a curriculum learning strategy into the LoRA fine-tuning process,
incorporating weighted emotional shifts between same-speaker and
different-speaker utterances to assign difficulty levels to dialogue samples,
which are then organized in an easy-to-hard training sequence. Experimental
results on two benchmark datasets-- IEMOCAP and MELD --show that our method
achieves new state-of-the-art (SOTA) performance, demonstrating the
effectiveness and generalizability of our approach in improving LLM-based
emotional understanding.

</details>


### [283] [Improving Region Representation Learning from Urban Imagery with Noisy Long-Caption Supervision](https://arxiv.org/abs/2511.07062)
*Yimei Zhang,Guojiang Shen,Kaili Ning,Tongwei Ren,Xuebo Qiu,Mengmeng Wang,Xiangjie Kong*

Main category: cs.AI

TL;DR: UrbanLN是一个通过长文本感知和噪声抑制改进城市区域表示学习的预训练框架，通过信息保留的拉伸插值策略和双层优化策略解决了视觉特征与长文本对齐和噪声问题，在多个真实城市和下游任务中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 城市区域表示学习在从无标签城市数据中提取有意义的特征方面发挥关键作用。城市的外观如同其"肖像"，包含潜在的社会经济和环境特征。尽管已有研究利用大语言模型将文本知识融入基于图像的城市区域表示学习，但仍存在两个主要挑战：i)难以将细粒度视觉特征与长文本描述对齐；ii)大语言模型生成的描述中存在噪声，导致知识融入效果不佳。

Method: 提出UrbanLN预训练框架，核心包括：1)信息保留的拉伸插值策略，用于在复杂城市场景中将长文本描述与细粒度视觉语义对齐；2)双层优化策略：数据层面采用多模型协作流程自动生成多样化可靠的文本描述，无需人工干预；模型层面使用基于动量的自蒸馏机制生成稳定的伪目标，在噪声条件下实现鲁棒的跨模态学习。

Result: 在四个真实城市的广泛实验和各种下游任务中，UrbanLN框架展现了优越的性能，验证了其在城市区域表示学习方面的有效性。

Conclusion: UrbanLN成功解决了城市区域表示学习中视觉特征与长文本对齐困难以及噪声干扰的问题，通过创新的长文本感知和噪声抑制策略，为城市计算领域提供了一种更有效的城市特征提取方法。

Abstract: Region representation learning plays a pivotal role in urban computing by
extracting meaningful features from unlabeled urban data. Analogous to how
perceived facial age reflects an individual's health, the visual appearance of
a city serves as its ``portrait", encapsulating latent socio-economic and
environmental characteristics. Recent studies have explored leveraging Large
Language Models (LLMs) to incorporate textual knowledge into imagery-based
urban region representation learning. However, two major challenges remain:
i)~difficulty in aligning fine-grained visual features with long captions, and
ii) suboptimal knowledge incorporation due to noise in LLM-generated captions.
To address these issues, we propose a novel pre-training framework called
UrbanLN that improves Urban region representation learning through Long-text
awareness and Noise suppression. Specifically, we introduce an
information-preserved stretching interpolation strategy that aligns long
captions with fine-grained visual semantics in complex urban scenes. To
effectively mine knowledge from LLM-generated captions and filter out noise, we
propose a dual-level optimization strategy. At the data level, a multi-model
collaboration pipeline automatically generates diverse and reliable captions
without human intervention. At the model level, we employ a momentum-based
self-distillation mechanism to generate stable pseudo-targets, facilitating
robust cross-modal learning under noisy conditions. Extensive experiments
across four real-world cities and various downstream tasks demonstrate the
superior performance of our UrbanLN.

</details>


### [284] [Increasing AI Explainability by LLM Driven Standard Processes](https://arxiv.org/abs/2511.07083)
*Marc Jansen,Marcel Pehlke*

Main category: cs.AI

TL;DR: 本文提出了一种通过将大语言模型嵌入标准化分析流程来提高人工智能系统可解释性的方法。


<details>
  <summary>Details</summary>
Motivation: 传统可解释AI方法主要关注特征归因或事后解释，但存在推理过程不透明、难以审计的局限性，需要一种能将不透明推理转化为透明可审计决策轨迹的新方法。

Method: 提出一种分层架构框架，将LLM集成到问题-选项-标准(QOC)、敏感性分析、博弈论和风险管理等标准化决策模型中，通过分离LLM的推理空间与上层的可解释流程空间，实现推理过程的透明化。

Result: 实证评估表明，该系统能够在去中心化治理、系统分析和战略推理等场景中重现人类级别的决策逻辑，为可靠、可解释、可验证的AI辅助决策提供了基础。

Conclusion: LLM驱动的标准化流程为构建可靠、可解释、可验证的AI辅助决策系统提供了新范式，通过将不透明的推理转化为透明的决策轨迹，显著提升了AI系统的可解释性和可信度。

Abstract: This paper introduces an approach to increasing the explainability of
artificial intelligence (AI) systems by embedding Large Language Models (LLMs)
within standardized analytical processes. While traditional explainable AI
(XAI) methods focus on feature attribution or post-hoc interpretation, the
proposed framework integrates LLMs into defined decision models such as
Question-Option-Criteria (QOC), Sensitivity Analysis, Game Theory, and Risk
Management. By situating LLM reasoning within these formal structures, the
approach transforms opaque inference into transparent and auditable decision
traces. A layered architecture is presented that separates the reasoning space
of the LLM from the explainable process space above it. Empirical evaluations
show that the system can reproduce human-level decision logic in decentralized
governance, systems analysis, and strategic reasoning contexts. The results
suggest that LLM-driven standard processes provide a foundation for reliable,
interpretable, and verifiable AI-supported decision making.

</details>


### [285] [LLM Driven Processes to Foster Explainable AI](https://arxiv.org/abs/2511.07086)
*Marcel Pehlke,Marc Jansen*

Main category: cs.AI

TL;DR: 本文提出了一个模块化、可解释的LLM智能体决策支持管道，将推理过程外化为可审计的产物，在物流案例中实现了55.5%的因子对齐和57%的角色匹配度。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统LLM系统输出不透明、难以审计的问题，需要开发一个能够模拟专家工作流程且具有透明、可检查步骤的决策支持系统。

Method: 系统实现了三个框架：Vester敏感度模型（因子集、符号影响矩阵、系统角色、反馈循环）、正规博弈（策略、收益矩阵、均衡）和序贯博弈（角色条件智能体、树构建、反向归纳），每个步骤都有可交换模块，将LLM组件（默认GPT-5）与确定性分析器配对用于均衡和基于矩阵的角色分类。

Result: 在100次运行的物流案例中，26个因子的平均因子对齐度为55.5%，运输核心子集为62.9%，匹配角色一致率为57%，使用八标准评分的LLM评委得分与重建的人类基准相当。

Conclusion: 可配置的LLM管道能够通过透明、可检查的步骤模拟专家工作流程，为决策支持提供了可审计和可解释的解决方案。

Abstract: We present a modular, explainable LLM-agent pipeline for decision support
that externalizes reasoning into auditable artifacts. The system instantiates
three frameworks: Vester's Sensitivity Model (factor set, signed impact matrix,
systemic roles, feedback loops); normal-form games (strategies, payoff matrix,
equilibria); and sequential games (role-conditioned agents, tree construction,
backward induction), with swappable modules at every step. LLM components
(default: GPT-5) are paired with deterministic analyzers for equilibria and
matrix-based role classification, yielding traceable intermediates rather than
opaque outputs. In a real-world logistics case (100 runs), mean factor
alignment with a human baseline was 55.5\% over 26 factors and 62.9\% on the
transport-core subset; role agreement over matches was 57\%. An LLM judge using
an eight-criterion rubric (max 100) scored runs on par with a reconstructed
human baseline. Configurable LLM pipelines can thus mimic expert workflows with
transparent, inspectable steps.

</details>


### [286] [Green AI: A systematic review and meta-analysis of its definitions, lifecycle models, hardware and measurement attempts](https://arxiv.org/abs/2511.07090)
*Marcel Rojahn,Marcus Grum*

Main category: cs.AI

TL;DR: 本文提出了一个统一的绿色AI框架，包括明确定义、五阶段生命周期评估、PDCA治理机制、硬件系统策略和校准测量体系，用于全面管理AI的多维度环境负担。


<details>
  <summary>Details</summary>
Motivation: 当前AI生命周期的环境负担(能源、碳、水、隐性影响)日益严重，现有云工具缺乏标准化且常忽略水足迹和供应链影响，限制了可比性和可重现性。

Method: 建立绿色AI统一操作定义；构建与LCA对应的五阶段生命周期；通过PDCA循环实现治理；系统化边缘云连续体的硬件策略；开发结合估算模型和直接计量的校准测量框架。

Result: 形成了可操作的、基于证据的指导框架，为研究人员、从业者、决策者提供标准化的AI环境影响管理方案，支持跨供应商的可重现比较。

Conclusion: 通过统一定义、生命周期流程、硬件策略和校准测量的结合，为多维度环境负担管理提供了实用框架，推动了绿色AI的标准化和可实施性。

Abstract: Across the Artificial Intelligence (AI) lifecycle - from hardware to
development, deployment, and reuse - burdens span energy, carbon, water, and
embodied impacts. Cloud provider tools improve transparency but remain
heterogeneous and often omit water and value chain effects, limiting
comparability and reproducibility. Addressing these multi dimensional burdens
requires a lifecycle approach linking phase explicit mapping with system levers
(hardware, placement, energy mix, cooling, scheduling) and calibrated
measurement across facility, system, device, and workload levels. This article
(i) establishes a unified, operational definition of Green AI distinct from
Sustainable AI; (ii) formalizes a five phase lifecycle mapped to Life Cycle
Assessment (LCA) stages, making energy, carbon, water, and embodied impacts
first class; (iii) specifies governance via Plan Do Check Act (PDCA) cycles
with decision gateways; (iv) systematizes hardware and system level strategies
across the edge cloud continuum to reduce embodied burdens; and (v) defines a
calibrated measurement framework combining estimator models with direct
metering to enable reproducible, provider agnostic comparisons. Combining
definition, lifecycle processes, hardware strategies, and calibrated
measurement, this article offers actionable, evidence based guidance for
researchers, practitioners, and policymakers.

</details>


### [287] [Data Complexity of Querying Description Logic Knowledge Bases under Cost-Based Semantics](https://arxiv.org/abs/2511.07095)
*Meghyn Bienvenu,Quentin Manière*

Main category: cs.AI

TL;DR: 本文研究了基于成本语义的不一致加权描述逻辑知识库查询的数据复杂性问题，在包含逆向角色和角色包含的DL-Lite方言中取得了重要突破。


<details>
  <summary>Details</summary>
Motivation: 研究不一致加权描述逻辑知识库在基于成本语义下的查询数据复杂性，扩展现有研究范围到包含逆向角色和角色包含的DL-Lite方言。

Method: 为每个解释分配基于违反公理权重和断言的成本，通过考虑最优或有界成本解释来确定确定性和可能性查询答案，分析DL-Lite方言的数据复杂性。

Result: 精化了多个下界，确定了最优成本确定性答案语义的精确复杂度；最令人意外的发现是当考虑固定成本界限时，DL-Lite^H_bool本体的一阶实例查询和合取查询可通过一阶重写计算，达到TC^0的最低数据复杂度。

Conclusion: 尽管基于成本的语义通常难以计算，但在特定条件下（DL-Lite^H_bool本体和固定成本界限）可实现非常高效的查询计算，为不一致知识库的实际应用提供了理论基础。

Abstract: In this paper, we study the data complexity of querying inconsistent weighted
description logic (DL) knowledge bases under recently-introduced cost-based
semantics. In a nutshell, the idea is to assign each interpretation a cost
based upon the weights of the violated axioms and assertions, and certain and
possible query answers are determined by considering all (resp. some)
interpretations having optimal or bounded cost. Whereas the initial study of
cost-based semantics focused on DLs between $\mathcal{EL}_\bot$ and
$\mathcal{ALCO}$, we consider DLs that may contain inverse roles and role
inclusions, thus covering prominent DL-Lite dialects. Our data complexity
analysis goes significantly beyond existing results by sharpening several lower
bounds and pinpointing the precise complexity of optimal-cost certain answer
semantics (no non-trivial upper bound was known). Moreover, while all existing
results show the intractability of cost-based semantics, our most challenging
and surprising result establishes that if we consider
$\text{DL-Lite}^\mathcal{H}_\mathsf{bool}$ ontologies and a fixed cost bound,
certain answers for instance queries and possible answers for conjunctive
queries can be computed using first-order rewriting and thus enjoy the lowest
possible data complexity ($\mathsf{TC}_0$).

</details>


### [288] [Boosting Fine-Grained Urban Flow Inference via Lightweight Architecture and Focalized Optimization](https://arxiv.org/abs/2511.07098)
*Yuanshao Zhu,Xiangyu Zhao,Zijian Zhang,Xuetao Wei,James Jianqiao Yu*

Main category: cs.AI

TL;DR: 本文提出了PLGF架构和DualFocal Loss的统一解决方案，用于细粒度城市流量推断，在大幅减少模型大小（97%）的同时实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有城市流量推断方法面临两个关键挑战：过参数化模型的巨大计算成本阻碍实际部署；传统损失函数在城市流量高度偏斜分布上性能不佳。

Method: 提出两个核心组件：1) PLGF轻量级架构，采用渐进式局部-全局融合策略捕获细粒度细节和全局上下文依赖；2) DualFocal Loss函数，集成双空间监督和难度感知聚焦机制，自适应关注难预测区域。

Result: 在4个真实场景实验中验证了方法的效性和可扩展性。在达到SOTA性能的同时，模型大小比当前高性能方法减少97%；在可比参数预算下，准确率比强基线提升超过10%。

Conclusion: 通过协同架构效率与自适应优化，为细粒度城市流量推断提供了统一的轻量级高效解决方案，显著降低了计算成本并提升了预测精度。

Abstract: Fine-grained urban flow inference is crucial for urban planning and
intelligent transportation systems, enabling precise traffic management and
resource allocation. However, the practical deployment of existing methods is
hindered by two key challenges: the prohibitive computational cost of
over-parameterized models and the suboptimal performance of conventional loss
functions on the highly skewed distribution of urban flows. To address these
challenges, we propose a unified solution that synergizes architectural
efficiency with adaptive optimization. Specifically, we first introduce PLGF, a
lightweight yet powerful architecture that employs a Progressive Local-Global
Fusion strategy to effectively capture both fine-grained details and global
contextual dependencies. Second, we propose DualFocal Loss, a novel function
that integrates dual-space supervision with a difficulty-aware focusing
mechanism, enabling the model to adaptively concentrate on hard-to-predict
regions. Extensive experiments on 4 real-world scenarios validate the
effectiveness and scalability of our method. Notably, while achieving
state-of-the-art performance, PLGF reduces the model size by up to 97% compared
to current high-performing methods. Furthermore, under comparable parameter
budgets, our model yields an accuracy improvement of over 10% against strong
baselines. The implementation is included in the https://github.com/Yasoz/PLGF.

</details>


### [289] [A Theoretical Analysis of Detecting Large Model-Generated Time Series](https://arxiv.org/abs/2511.07104)
*Junji Hou,Junzhou Zhao,Shuo Zhang,Pinghui Wang*

Main category: cs.AI

TL;DR: 本文提出了一种检测时间序列大模型(TSLMs)生成数据的新方法UCE，基于"收缩假设"——模型生成的时间序列在递归预测下不确定性会逐渐降低。UCE通过聚合连续前缀的不确定性指标，在32个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着数据滥用和伪造风险增加，需要识别TSLM生成的合成时间序列。但现有文本检测方法不适用于时间序列，因为时间序列信息密度更低、概率分布更平滑，限制了基于token的检测器的判别能力。

Method: 提出"收缩假设"：模型生成的时间序列在递归预测下表现出逐渐减少的不确定性。在理论假设下正式证明该假设，并基于此开发不确定性收缩估计器(UCE)，这是一个白盒检测器，通过聚合连续前缀的不确定性指标来识别TSLM生成的时间序列。

Result: 在32个数据集上的广泛实验表明，UCE一致地优于最先进的基线方法，为检测模型生成的时间序列提供了可靠且可泛化的解决方案。

Conclusion: UCE基于理论证明的收缩假设，为检测TSLM生成的时间序列提供了一个可靠、通用且性能优越的解决方案，有效解决了现有文本检测方法在时间序列模态上的不适用性问题。

Abstract: Motivated by the increasing risks of data misuse and fabrication, we
investigate the problem of identifying synthetic time series generated by
Time-Series Large Models (TSLMs) in this work. While there are extensive
researches on detecting model generated text, we find that these existing
methods are not applicable to time series data due to the fundamental modality
difference, as time series usually have lower information density and smoother
probability distributions than text data, which limit the discriminative power
of token-based detectors. To address this issue, we examine the subtle
distributional differences between real and model-generated time series and
propose the contraction hypothesis, which states that model-generated time
series, unlike real ones, exhibit progressively decreasing uncertainty under
recursive forecasting. We formally prove this hypothesis under theoretical
assumptions on model behavior and time series structure. Model-generated time
series exhibit progressively concentrated distributions under recursive
forecasting, leading to uncertainty contraction. We provide empirical
validation of the hypothesis across diverse datasets. Building on this insight,
we introduce the Uncertainty Contraction Estimator (UCE), a white-box detector
that aggregates uncertainty metrics over successive prefixes to identify
TSLM-generated time series. Extensive experiments on 32 datasets show that UCE
consistently outperforms state-of-the-art baselines, offering a reliable and
generalizable solution for detecting model-generated time series.

</details>


### [290] [MENTOR: A Metacognition-Driven Self-Evolution Framework for Uncovering and Mitigating Implicit Risks in LLMs on Domain Tasks](https://arxiv.org/abs/2511.07107)
*Liang Shan,Kaicheng Shen,Wen Wu,Zhenyu Ying,Chaochao Lu,Guangze Ye,Liang He*

Main category: cs.AI

TL;DR: MENTOR是一个基于元认知的自我演进框架，用于发现和缓解LLMs在特定领域任务中的隐性风险，通过元认知自我评估、动态规则生成和激活引导技术实现持续改进。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs对齐方法主要针对偏见、仇恨言论等显性风险，忽略了特定领域更深层隐性风险，且缺乏灵活可泛化的框架，无法有效适应多样化的专业领域需求。

Method: 提出MENTOR框架：1）引入元认知自我评估工具，通过换位思考和结果分析让LLMs反思价值对齐问题；2）构建包含9000个风险查询的跨领域数据集；3）基于反思结果动态生成规则知识图谱扩展静态规则树；4）使用激活引导技术在推理时强化规则遵循。

Result: 在教育、金融、管理三个垂直领域的防御测试中，框架显著降低语义攻击成功率；元认知评估不仅与人类评估高度一致，还能提供更全面深入的LLMs价值对齐分析。

Conclusion: MENTOR建立了持续自我演进周期，通过动态规则生成有效提高了泛化能力，降低了静态系统的维护成本和僵化性，为LLMs的隐性风险缓解提供了新水平的解决方案。

Abstract: Ensuring the safety and value alignment of large language models (LLMs) is
critical for their deployment. Current alignment efforts primarily target
explicit risks such as bias, hate speech, and violence. However, they often
fail to address deeper, domain-specific implicit risks and lack a flexible,
generalizable framework applicable across diverse specialized fields. Hence, we
proposed MENTOR: A MEtacognition-driveN self-evoluTion framework for uncOvering
and mitigating implicit Risks in LLMs on Domain Tasks. To address the
limitations of labor-intensive human evaluation, we introduce a novel
metacognitive self-assessment tool. This enables LLMs to reflect on potential
value misalignments in their responses using strategies like perspective-taking
and consequential thinking. We also release a supporting dataset of 9,000 risk
queries spanning education, finance, and management to enhance domain-specific
risk identification. Subsequently, based on the outcomes of metacognitive
reflection, the framework dynamically generates supplementary rule knowledge
graphs that extend predefined static rule trees. This enables models to
actively apply validated rules to future similar challenges, establishing a
continuous self-evolution cycle that enhances generalization by reducing
maintenance costs and inflexibility of static systems. Finally, we employ
activation steering during inference to guide LLMs in following the rules, a
cost-effective method to robustly enhance enforcement across diverse contexts.
Experimental results show MENTOR's effectiveness: In defensive testing across
three vertical domains, the framework substantially reduces semantic attack
success rates, enabling a new level of implicit risk mitigation for LLMs.
Furthermore, metacognitive assessment not only aligns closely with baseline
human evaluators but also delivers more thorough and insightful analysis of
LLMs value alignment.

</details>


### [291] [Two Heads are Better than One: Distilling Large Language Model Features Into Small Models with Feature Decomposition and Mixture](https://arxiv.org/abs/2511.07110)
*Tianhao Fu,Xinxin Xu,Weichen Xu,Jue Chen,Ruilong Ren,Bowen Deng,Xinyu Zhao,Jian Cao,Xixin Cao*

Main category: cs.AI

TL;DR: CMM框架通过协同知识蒸馏将LLM特征解耦到层、任务、数据三个正交维度，使用Hájek-MoE集成多个学生模型输出，在真实市场数据上显著优于现有蒸馏方法和RL做市策略。


<details>
  <summary>Details</summary>
Motivation: 基于强化学习的做市策略在金融交易中备受关注，而大语言模型在金融领域应用表现出色但推理速度慢，目前缺乏针对做市任务的LLM蒸馏研究，需要解决效率问题。

Method: 首先提出归一化荧光探针研究LLM特征机制，然后提出CMM框架将LLM特征在层、任务、数据三个维度解耦，多个学生模型协作学习不同维度的简单特征，并通过Hájek-MoE在核函数生成的公共特征空间中集成各模型输出。

Result: 在四个真实世界市场数据集上的广泛实验表明，CMM方法优于当前蒸馏方法和基于强化学习的做市策略，证明了其优越性能。

Conclusion: CMM框架成功实现了LLM在金融做市任务中的高效知识蒸馏，通过特征解耦和协同学习的创新设计，为解决LLM应用中的效率瓶颈提供了有效解决方案。

Abstract: Market making (MM) through Reinforcement Learning (RL) has attracted
significant attention in financial trading. With the development of Large
Language Models (LLMs), more and more attempts are being made to apply LLMs to
financial areas. A simple, direct application of LLM as an agent shows
significant performance. Such methods are hindered by their slow inference
speed, while most of the current research has not studied LLM distillation for
this specific task. To address this, we first propose the normalized
fluorescent probe to study the mechanism of the LLM's feature. Based on the
observation found by our investigation, we propose Cooperative Market Making
(CMM), a novel framework that decouples LLM features across three orthogonal
dimensions: layer, task, and data. Various student models collaboratively learn
simple LLM features along with different dimensions, with each model
responsible for a distinct feature to achieve knowledge distillation.
Furthermore, CMM introduces an H\'{a}jek-MoE to integrate the output of the
student models by investigating the contribution of different models in a
kernel function-generated common feature space. Extensive experimental results
on four real-world market datasets demonstrate the superiority of CMM over the
current distillation method and RL-based market-making strategies.

</details>


### [292] [PADiff: Predictive and Adaptive Diffusion Policies for Ad Hoc Teamwork](https://arxiv.org/abs/2511.07260)
*Hohei Chan,Xinzhi Zhang,Antao Xiang,Weinan Zhang,Mengchen Zhao*

Main category: cs.AI

TL;DR: PADiff是一种基于扩散模型的临时团队合作方法，通过捕获多模态行为并集成队友预测信息，使智能体能够与未知队友有效协作。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法优化单一期望回报，导致策略坍缩为单一主导行为，无法捕获临时团队中固有的多模态合作模式，因此需要开发能够预测并适应未知队友的新方法。

Method: 提出PADiff方法，结合扩散模型捕获智能体的多模态行为，并创新性地将队友的关键预测信息整合到去噪过程中，以应对高度非平稳的临时团队合作环境。

Result: 在三个协作环境中的大量实验证明，PADiff显著优于现有的临时团队合作方法，展现了更好的协作适应性和多样性。

Conclusion: PADiff成功解决了传统RL方法在临时团队合作中的策略坍缩问题，通过扩散模型和预测信息整合，实现了与未知队友的有效多模态协作，为实际应用提供了新的解决方案。

Abstract: Ad hoc teamwork (AHT) requires agents to collaborate with previously unseen
teammates, which is crucial for many real-world applications. The core
challenge of AHT is to develop an ego agent that can predict and adapt to
unknown teammates on the fly. Conventional RL-based approaches optimize a
single expected return, which often causes policies to collapse into a single
dominant behavior, thus failing to capture the multimodal cooperation patterns
inherent in AHT. In this work, we introduce PADiff, a diffusion-based approach
that captures agent's multimodal behaviors, unlocking its diverse cooperation
modes with teammates. However, standard diffusion models lack the ability to
predict and adapt in highly non-stationary AHT scenarios. To address this
limitation, we propose a novel diffusion-based policy that integrates critical
predictive information about teammates into the denoising process. Extensive
experiments across three cooperation environments demonstrate that PADiff
outperforms existing AHT methods significantly.

</details>


### [293] [AgenticSciML: Collaborative Multi-Agent Systems for Emergent Discovery in Scientific Machine Learning](https://arxiv.org/abs/2511.07262)
*Qile Jiang,George Karniadakis*

Main category: cs.AI

TL;DR: AgenticSciML是一个多智能体协作系统，通过10多个专业化AI智能体的合作，自动发现和优化科学机器学习方法，在物理信息学习和算子学习任务中将误差降低4个数量级。


<details>
  <summary>Details</summary>
Motivation: 现有科学机器学习的架构设计、损失函数制定和训练策略仍依赖专家驱动的研究过程，需要大量实验和特定领域洞察，限制了方法的发现效率和可扩展性。

Method: 构建包含10多个专业化AI智能体的协作系统，集成结构化辩论、检索增强的方法记忆和集成引导的进化搜索，让智能体通过结构化推理和迭代进化来提出、批评和完善SciML解决方案。

Result: 在物理信息学习和算子学习任务中，框架发现的解决方案比单智能体和人工设计的基线方法在误差降低上表现优异最多达4个数量级，并生成了自适应专家混合架构、分解式PINN和物理信息算子学习模型等新颖策略。

Conclusion: AI智能体间的协作推理能够产生涌现性的方法论创新，为科学计算中可扩展、透明和自主发现指明了道路，有望推动科学机器学习领域的自动化发展。

Abstract: Scientific Machine Learning (SciML) integrates data-driven inference with
physical modeling to solve complex problems in science and engineering.
However, the design of SciML architectures, loss formulations, and training
strategies remains an expert-driven research process, requiring extensive
experimentation and problem-specific insights. Here we introduce AgenticSciML,
a collaborative multi-agent system in which over 10 specialized AI agents
collaborate to propose, critique, and refine SciML solutions through structured
reasoning and iterative evolution. The framework integrates structured debate,
retrieval-augmented method memory, and ensemble-guided evolutionary search,
enabling the agents to generate and assess new hypotheses about architectures
and optimization procedures. Across physics-informed learning and operator
learning tasks, the framework discovers solution methods that outperform
single-agent and human-designed baselines by up to four orders of magnitude in
error reduction. The agents produce novel strategies -- including adaptive
mixture-of-expert architectures, decomposition-based PINNs, and
physics-informed operator learning models -- that do not appear explicitly in
the curated knowledge base. These results show that collaborative reasoning
among AI agents can yield emergent methodological innovation, suggesting a path
toward scalable, transparent, and autonomous discovery in scientific computing.

</details>


### [294] [Beyond Detection: Exploring Evidence-based Multi-Agent Debate for Misinformation Intervention and Persuasion](https://arxiv.org/abs/2511.07267)
*Chen Han,Yijia Ma,Jin Tan,Wenzhen Zheng,Xijin Tang*

Main category: cs.AI

TL;DR: ED2D是一种基于证据的多智能体辩论框架，不仅能检测错误信息，还能通过生成的反驳文本有效纠正用户信念，但在错误分类时可能产生负面效果。


<details>
  <summary>Details</summary>
Motivation: 现有MAD框架关注检测准确率而忽视帮助用户理解推理过程和培养未来抗御能力，辩论记录作为推理透明化的丰富资源未被充分利用。

Method: 开发ED2D框架，在MAD基础上融入事实证据检索，设计为多智能体劝服系统，生成反驳文本以纠正用户信念并阻止错误信息传播，同时建立公共社区网站促进透明思考和协作事实核查。

Result: ED2D在三个错误信息检测基准上优于现有基线，正确预测时反驳文本的劝服效果与人类专家相当；但错误分类时其解释可能强化用户误解，即使同时呈现准确的人类解释。

Conclusion: 研究表明MAD系统用于错误信息干预既有前景也存在风险，需要谨慎部署以避免错误分类带来的负面影响，通过透明化和社区协作可提升用户批判思维能力。

Abstract: Multi-agent debate (MAD) frameworks have emerged as promising approaches for
misinformation detection by simulating adversarial reasoning. While prior work
has focused on detection accuracy, it overlooks the importance of helping users
understand the reasoning behind factual judgments and develop future
resilience. The debate transcripts generated during MAD offer a rich but
underutilized resource for transparent reasoning. In this study, we introduce
ED2D, an evidence-based MAD framework that extends previous approach by
incorporating factual evidence retrieval. More importantly, ED2D is designed
not only as a detection framework but also as a persuasive multi-agent system
aimed at correcting user beliefs and discouraging misinformation sharing. We
compare the persuasive effects of ED2D-generated debunking transcripts with
those authored by human experts. Results demonstrate that ED2D outperforms
existing baselines across three misinformation detection benchmarks. When ED2D
generates correct predictions, its debunking transcripts exhibit persuasive
effects comparable to those of human experts; However, when ED2D misclassifies,
its accompanying explanations may inadvertently reinforce users'misconceptions,
even when presented alongside accurate human explanations. Our findings
highlight both the promise and the potential risks of deploying MAD systems for
misinformation intervention. We further develop a public community website to
help users explore ED2D, fostering transparency, critical thinking, and
collaborative fact-checking.

</details>


### [295] [DigiData: Training and Evaluating General-Purpose Mobile Control Agents](https://arxiv.org/abs/2511.07413)
*Yuxuan Sun,Manchen Wang,Shengyi Qian,William R. Wong,Eric Gan,Pierluca D'Oro,Alejandro Castillejo Munoz,Sneha Silwal,Pedro Matias,Nitin Kamra,Satwik Kottur,Nick Raines,Xuanyi Zhao,Joy Chen,Joseph Greer,Andrea Madotto,Allen Bolourchi,James Valori,Kevin Carlberg,Karl Ridgeway,Joseph Tighe*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: AI agents capable of controlling user interfaces have the potential to
transform human interaction with digital devices. To accelerate this
transformation, two fundamental building blocks are essential: high-quality
datasets that enable agents to achieve complex and human-relevant goals, and
robust evaluation methods that allow researchers and practitioners to rapidly
enhance agent performance. In this paper, we introduce DigiData, a large-scale,
high-quality, diverse, multi-modal dataset designed for training mobile control
agents. Unlike existing datasets, which derive goals from unstructured
interactions, DigiData is meticulously constructed through comprehensive
exploration of app features, resulting in greater diversity and higher goal
complexity. Additionally, we present DigiData-Bench, a benchmark for evaluating
mobile control agents on real-world complex tasks. We demonstrate that the
commonly used step-accuracy metric falls short in reliably assessing mobile
control agents and, to address this, we propose dynamic evaluation protocols
and AI-powered evaluations as rigorous alternatives for agent assessment. Our
contributions aim to significantly advance the development of mobile control
agents, paving the way for more intuitive and effective human-device
interactions.

</details>
